This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/__pycache__/, **/*.pyc, **/*.pyo, **/.git/, **/node_modules/, **/.venv/, **/venv/, **/dist/, **/build/, **/*.egg-info/, **/.pytest_cache/, **/.mypy_cache/, **/.ruff_cache/, **/.tox/, **/.github/, **/docs/, **/experiments/, **/checkpoints/, **/logs/, **/mlruns/, **/.ipynb_checkpoints/, **/.DS_Store, **/finetuning_ckpts/, **/profiler/, **/.devcontainer/, **/uv.lock, **/*.pt, **/*.pkl, **/*.tar.gz, **/*.log, **/*.db
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
dspy/
  adapters/
    types/
      __init__.py
      audio.py
      base_type.py
      citation.py
      code.py
      document.py
      file.py
      history.py
      image.py
      reasoning.py
      tool.py
    __init__.py
    baml_adapter.py
    base.py
    chat_adapter.py
    json_adapter.py
    two_step_adapter.py
    utils.py
    xml_adapter.py
  clients/
    __init__.py
    base_lm.py
    cache.py
    databricks.py
    embedding.py
    lm_local.py
    lm.py
    openai.py
    provider.py
    utils_finetune.py
  datasets/
    alfworld/
      __init__.py
      alfworld.py
      base_config.yml
    __init__.py
    colors.py
    dataloader.py
    dataset.py
    gsm8k.py
    hotpotqa.py
    math.py
  dsp/
    utils/
      __init__.py
      dpr.py
      settings.py
      utils.py
    __init__.py
    colbertv2.py
  evaluate/
    __init__.py
    auto_evaluation.py
    evaluate.py
    metrics.py
  experimental/
    __init__.py
  predict/
    avatar/
      __init__.py
      avatar.py
      models.py
      signatures.py
    __init__.py
    aggregation.py
    best_of_n.py
    chain_of_thought.py
    code_act.py
    knn.py
    multi_chain_comparison.py
    parallel.py
    parameter.py
    predict.py
    program_of_thought.py
    react.py
    refine.py
    retry.py
    rlm.py
  primitives/
    __init__.py
    base_module.py
    code_interpreter.py
    example.py
    module.py
    prediction.py
    python_interpreter.py
    repl_types.py
    runner.js
  propose/
    __init__.py
    dataset_summary_generator.py
    grounded_proposer.py
    propose_base.py
    utils.py
  retrievers/
    __init__.py
    databricks_rm.py
    embeddings.py
    retrieve.py
    weaviate_rm.py
  signatures/
    __init__.py
    field.py
    signature.py
    utils.py
  streaming/
    __init__.py
    messages.py
    streamify.py
    streaming_listener.py
  teleprompt/
    gepa/
      __init__.py
      gepa_utils.py
      gepa.py
      instruction_proposal.py
    __init__.py
    avatar_optimizer.py
    bettertogether.py
    bootstrap_finetune.py
    bootstrap_trace.py
    bootstrap.py
    copro_optimizer.py
    ensemble.py
    grpo.py
    infer_rules.py
    knn_fewshot.py
    mipro_optimizer_v2.py
    random_search.py
    signature_opt.py
    simba_utils.py
    simba.py
    teleprompt_optuna.py
    teleprompt.py
    utils.py
    vanilla.py
  utils/
    __init__.py
    annotation.py
    asyncify.py
    caching.py
    callback.py
    dummies.py
    exceptions.py
    hasher.py
    inspect_history.py
    langchain_tool.py
    logging_utils.py
    magicattr.py
    mcp.py
    parallelizer.py
    saving.py
    syncify.py
    unbatchify.py
    usage_tracker.py
  __init__.py
  __metadata__.py
tests/
  adapters/
    test_adapter_utils.py
    test_audio.py
    test_baml_adapter.py
    test_base_type.py
    test_chat_adapter.py
    test_citation.py
    test_code.py
    test_document.py
    test_json_adapter.py
    test_reasoning.py
    test_tool.py
    test_two_step_adapter.py
    test_xml_adapter.py
  callback/
    test_callback.py
  clients/
    test_cache.py
    test_databricks.py
    test_embedding.py
    test_inspect_global_history.py
    test_lm_local.py
    test_lm.py
  datasets/
    test_dataset.py
  evaluate/
    test_evaluate.py
    test_metrics.py
  examples/
    test_baleen.py
  metadata/
    test_metadata.py
  predict/
    test_aggregation.py
    test_best_of_n.py
    test_chain_of_thought.py
    test_code_act.py
    test_knn.py
    test_multi_chain_comparison.py
    test_parallel.py
    test_predict.py
    test_program_of_thought.py
    test_react.py
    test_refine.py
    test_retry.py
    test_rlm.py
  primitives/
    resources/
      saved_program.json
    test_base_module.py
    test_example.py
    test_module.py
    test_python_interpreter.py
  propose/
    test_grounded_proposer.py
  reliability/
    complex_types/
      generated/
        test_many_types_1/
          inputs/
            input1.json
            input2.json
          program.py
          schema.json
        test_nesting_1/
          inputs/
            input1.json
            input2.json
          program.py
          schema.json
        test_nesting_2/
          inputs/
            input1.json
          program.py
          schema.json
    generate/
      __init__.py
      __main__.py
      utils.py
    input_formats/
      generated/
        test_markdown_1/
          inputs/
            input1.json
            input2.json
          program.py
          schema.json
    __init__.py
    conftest.py
    README.md
    reliability_conf.yaml
    test_generated.py
    test_pydantic_models.py
    utils.py
  retrievers/
    test_colbertv2.py
    test_embeddings.py
  signatures/
    test_adapter_file.py
    test_adapter_image.py
    test_custom_types.py
    test_signature.py
  streaming/
    test_streaming.py
  teleprompt/
    gepa_dummy_lm_custom_component_selector_custom_instruction_proposer.json
    gepa_dummy_lm.json
    test_bootstrap_finetune.py
    test_bootstrap_trace.py
    test_bootstrap.py
    test_copro_optimizer.py
    test_ensemble.py
    test_finetune.py
    test_gepa_instruction_proposer.py
    test_gepa.py
    test_grpo.py
    test_knn_fewshot.py
    test_random_search.py
    test_teleprompt.py
    test_utils.py
  test_utils/
    server/
      __init__.py
      litellm_server_config.yaml
      litellm_server.py
    __init__.py
  utils/
    resources/
      mcp_server.py
    __init__.py
    test_annotation.py
    test_asyncify.py
    test_exceptions.py
    test_langchain_tool.py
    test_magicattr.py
    test_mcp.py
    test_parallelizer.py
    test_saving.py
    test_settings.py
    test_syncify.py
    test_unbatchify.py
    test_usage_tracker.py
  __init__.py
  conftest.py
  mock_interpreter.py
  README.md
.gitignore
.pre-commit-config.yaml
CONTRIBUTING.md
LICENSE
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="dspy/adapters/types/__init__.py">
__all__ = ["History", "Image", "Audio", "File", "Type", "Tool", "ToolCalls", "Code", "Reasoning"]
</file>

<file path="dspy/adapters/types/audio.py">
SF_AVAILABLE = True
⋮----
SF_AVAILABLE = False
⋮----
def _normalize_audio_format(audio_format: str) -> str
⋮----
"""Removes 'x-' prefixes from audio format strings."""
⋮----
class Audio(Type)
⋮----
data: str
audio_format: str
⋮----
model_config = pydantic.ConfigDict(
⋮----
def format(self) -> list[dict[str, Any]]
⋮----
data = self.data
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, values: Any) -> Any
⋮----
"""
        Validate input for Audio, expecting 'data' and 'audio_format' keys in dictionary.
        """
⋮----
@classmethod
    def from_url(cls, url: str) -> "Audio"
⋮----
"""
        Download an audio file from URL and encode it as base64.
        """
response = requests.get(url)
⋮----
mime_type = response.headers.get("Content-Type", "audio/wav")
⋮----
audio_format = mime_type.split("/")[1]
⋮----
audio_format = _normalize_audio_format(audio_format)
⋮----
encoded_data = base64.b64encode(response.content).decode("utf-8")
⋮----
@classmethod
    def from_file(cls, file_path: str) -> "Audio"
⋮----
"""
        Read local audio file and encode it as base64.
        """
⋮----
file_data = file.read()
⋮----
encoded_data = base64.b64encode(file_data).decode("utf-8")
⋮----
"""
        Process numpy-like array and encode it as base64. Uses sampling rate and audio format for encoding.
        """
⋮----
byte_buffer = io.BytesIO()
⋮----
encoded_data = base64.b64encode(byte_buffer.getvalue()).decode("utf-8")
⋮----
def __str__(self) -> str
⋮----
def __repr__(self) -> str
⋮----
length = len(self.data)
⋮----
def encode_audio(audio: Union[str, bytes, dict, "Audio", Any], sampling_rate: int = 16000, format: str = "wav") -> dict
⋮----
"""
    Encode audio to a dict with 'data' and 'audio_format'.
    
    Accepts: local file path, URL, data URI, dict, Audio instance, numpy array, or bytes (with known format).
    """
⋮----
mime = header.split(";")[0].split(":")[1]
audio_format = mime.split("/")[1]
⋮----
a = Audio.from_file(audio)
⋮----
a = Audio.from_url(audio)
⋮----
a = Audio.from_array(audio, sampling_rate=sampling_rate, format=format)
⋮----
encoded = base64.b64encode(audio).decode("utf-8")
</file>

<file path="dspy/adapters/types/base_type.py">
CUSTOM_TYPE_START_IDENTIFIER = "<<CUSTOM-TYPE-START-IDENTIFIER>>"
CUSTOM_TYPE_END_IDENTIFIER = "<<CUSTOM-TYPE-END-IDENTIFIER>>"
⋮----
class Type(pydantic.BaseModel)
⋮----
"""Base class to support creating custom types for DSPy signatures.

    This is the parent class of DSPy custom types, e.g, dspy.Image. Subclasses must implement the `format` method to
    return a list of dictionaries (same as the Array of content parts in the OpenAI API user message's content field).

    Example:

        ```python
        class Image(Type):
            url: str

            def format(self) -> list[dict[str, Any]]:
                return [{"type": "image_url", "image_url": {"url": self.url}}]
        ```
    """
⋮----
def format(self) -> list[dict[str, Any]] | str
⋮----
@classmethod
    def description(cls) -> str
⋮----
"""Description of the custom type"""
⋮----
@classmethod
    def extract_custom_type_from_annotation(cls, annotation)
⋮----
"""Extract all custom types from the annotation.

        This is used to extract all custom types from the annotation of a field, while the annotation can
        have arbitrary level of nesting. For example, we detect `Tool` is in `list[dict[str, Tool]]`.
        """
# Direct match. Nested type like `list[dict[str, Event]]` passes `isinstance(annotation, type)` in python 3.10
# while fails in python 3.11. To accommodate users using python 3.10, we need to capture the error and ignore it.
⋮----
origin = get_origin(annotation)
⋮----
result = []
# Recurse into all type args
⋮----
@pydantic.model_serializer()
    def serialize_model(self)
⋮----
formatted = self.format()
⋮----
"""Adapt the custom type to the native LM feature if possible.

        When the LM and configuration supports the related native LM feature, e.g., native tool calling, native
        reasoning, etc., we adapt the signature and `lm_kwargs` to enable the native LM feature.

        Args:
            signature: The DSPy signature for the LM call.
            field_name: The name of the field in the signature to adapt to the native LM feature.
            lm: The LM instance.
            lm_kwargs: The keyword arguments for the LM call, subject to in-place updates if adaptation if required.

        Returns:
            The adapted signature. If the custom type is not natively supported by the LM, return the original
            signature.
        """
⋮----
@classmethod
    def is_streamable(cls) -> bool
⋮----
"""Whether the custom type is streamable."""
⋮----
@classmethod
    def parse_stream_chunk(cls, chunk: ModelResponseStream) -> Optional["Type"]
⋮----
"""
        Parse a stream chunk into the custom type.

        Args:
            chunk: A stream chunk.

        Returns:
            A custom type object or None if the chunk is not for this custom type.
        """
⋮----
@classmethod
    def parse_lm_response(cls, response: str | dict[str, Any]) -> Optional["Type"]
⋮----
"""Parse a LM response into the custom type.

        Args:
            response: A LM response.

        Returns:
            A custom type object.
        """
⋮----
def split_message_content_for_custom_types(messages: list[dict[str, Any]]) -> list[dict[str, Any]]
⋮----
"""Split user message content into a list of content blocks.

    This method splits each user message's content in the `messages` list to be a list of content block, so that
    the custom types like `dspy.Image` can be properly formatted for better quality. For example, the split content
    may look like below if the user message has a `dspy.Image` object:

    ```
    [
        {"type": "text", "text": "{text_before_image}"},
        {"type": "image_url", "image_url": {"url": "{image_url}"}},
        {"type": "text", "text": "{text_after_image}"},
    ]
    ```

    This is implemented by finding the `<<CUSTOM-TYPE-START-IDENTIFIER>>` and `<<CUSTOM-TYPE-END-IDENTIFIER>>`
    in the user message content and splitting the content around them. The `<<CUSTOM-TYPE-START-IDENTIFIER>>`
    and `<<CUSTOM-TYPE-END-IDENTIFIER>>` are the reserved identifiers for the custom types as in `dspy.Type`.

    Args:
        messages: a list of messages sent to the LM. The format is the same as [OpenAI API's messages
            format](https://platform.openai.com/docs/guides/chat-completions/response-format).

    Returns:
        A list of messages with the content split into a list of content blocks around custom types content.
    """
⋮----
# Custom type messages are only in user messages
⋮----
pattern = rf"{CUSTOM_TYPE_START_IDENTIFIER}(.*?){CUSTOM_TYPE_END_IDENTIFIER}"
⋮----
last_end = 0
# DSPy adapter always formats user input into a string content before custom type splitting
content: str = message["content"]
⋮----
# Add text before the current block
⋮----
# Parse the JSON inside the block
custom_type_content = match.group(1).strip()
parsed = None
⋮----
parsed = parse_fn(custom_type_content)
⋮----
# fallback to raw string if it's not valid JSON
⋮----
last_end = end
⋮----
# No custom type found, return the original message
⋮----
# Add any remaining text after the last match
⋮----
def _parse_doubly_quoted_json(json_str: str) -> Any
⋮----
"""
    Parse a doubly quoted JSON string into a Python dict.
    `dspy.Type` can be json-encoded twice if included in either list or dict, e.g., `list[dspy.experimental.Document]`
    """
</file>

<file path="dspy/adapters/types/citation.py">
@experimental(version="3.0.4")
class Citations(Type)
⋮----
"""Citations extracted from an LM response with source references.

    This type represents citations returned by language models that support
    citation extraction, particularly Anthropic's Citations API through LiteLLM.
    Citations include the quoted text and source information.

    Example:
        ```python
        import os
        import dspy
        from dspy.signatures import Signature
        from dspy.experimental import Citations, Document
        os.environ["ANTHROPIC_API_KEY"] = "YOUR_ANTHROPIC_API_KEY"

        class AnswerWithSources(Signature):
            '''Answer questions using provided documents with citations.'''
            documents: list[Document] = dspy.InputField()
            question: str = dspy.InputField()
            answer: str = dspy.OutputField()
            citations: Citations = dspy.OutputField()

        # Create documents to provide as sources
        docs = [
            Document(
                data="The Earth orbits the Sun in an elliptical path.",
                title="Basic Astronomy Facts"
            ),
            Document(
                data="Water boils at 100°C at standard atmospheric pressure.",
                title="Physics Fundamentals",
                metadata={"author": "Dr. Smith", "year": 2023}
            )
        ]

        # Use with a model that supports citations like Claude
        lm = dspy.LM("anthropic/claude-opus-4-1-20250805")
        predictor = dspy.Predict(AnswerWithSources)
        result = predictor(documents=docs, question="What temperature does water boil?", lm=lm)

        for citation in result.citations.citations:
            print(citation.format())
        ```
    """
⋮----
class Citation(Type)
⋮----
"""Individual citation with character location information."""
⋮----
type: str = "char_location"
cited_text: str
document_index: int
document_title: str | None = None
start_char_index: int
end_char_index: int
supported_text: str | None = None
⋮----
def format(self) -> dict[str, Any]
⋮----
"""Format citation as dictionary for LM consumption.

            Returns:
                A dictionary in the format expected by citation APIs.
            """
citation_dict = {
⋮----
citations: list[Citation]
⋮----
@classmethod
    def from_dict_list(cls, citations_dicts: list[dict[str, Any]]) -> "Citations"
⋮----
"""Convert a list of dictionaries to a Citations instance.

        Args:
            citations_dicts: A list of dictionaries, where each dictionary should have 'cited_text' key
                and 'document_index', 'start_char_index', 'end_char_index' keys.

        Returns:
            A Citations instance.

        Example:
            ```python
            citations_dict = [
                {
                    "cited_text": "The sky is blue",
                    "document_index": 0,
                    "document_title": "Weather Guide",
                    "start_char_index": 0,
                    "end_char_index": 15,
                    "supported_text": "The sky was blue yesterday."
                }
            ]
            citations = Citations.from_dict_list(citations_dict)
            ```
        """
citations = [cls.Citation(**item) for item in citations_dicts]
⋮----
@classmethod
    def description(cls) -> str
⋮----
"""Description of the citations type for use in prompts."""
⋮----
def format(self) -> list[dict[str, Any]]
⋮----
"""Format citations as a list of dictionaries."""
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, data: Any)
⋮----
# Handle case where data is a list of dicts with citation info
⋮----
# Handle case where data is a dict
⋮----
# Handle case where data is a dict with "citations" key
citations_data = data["citations"]
⋮----
# Handle case where data is a single citation dict
⋮----
def __iter__(self)
⋮----
"""Allow iteration over citations."""
⋮----
def __len__(self)
⋮----
"""Return the number of citations."""
⋮----
def __getitem__(self, index)
⋮----
"""Allow indexing into citations."""
⋮----
@classmethod
    def adapt_to_native_lm_feature(cls, signature, field_name, lm, lm_kwargs) -> bool
⋮----
@classmethod
    def is_streamable(cls) -> bool
⋮----
"""Whether the Citations type is streamable."""
⋮----
@classmethod
    def parse_stream_chunk(cls, chunk) -> Optional["Citations"]
⋮----
"""
        Parse a stream chunk into Citations.

        Args:
            chunk: A stream chunk from the LM.

        Returns:
            A Citations object if the chunk contains citation data, None otherwise.
        """
⋮----
# Check if the chunk has citation data in provider_specific_fields
⋮----
delta = chunk.choices[0].delta
⋮----
citation_data = delta.provider_specific_fields.get("citation")
⋮----
@classmethod
    def parse_lm_response(cls, response: str | dict[str, Any]) -> Optional["Citations"]
⋮----
"""Parse a LM response into Citations.

        Args:
            response: A LM response that may contain citation data.

        Returns:
            A Citations object if citation data is found, None otherwise.
        """
⋮----
# Check if the response contains citations in the expected format
⋮----
citations_data = response["citations"]
</file>

<file path="dspy/adapters/types/code.py">
class Code(Type)
⋮----
"""Code type in DSPy.

    This type is useful for code generation and code analysis.

    Example 1: dspy.Code as output type in code generation:

    ```python
    import dspy

    dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))


    class CodeGeneration(dspy.Signature):
        '''Generate python code to answer the question.'''

        question: str = dspy.InputField(description="The question to answer")
        code: dspy.Code["java"] = dspy.OutputField(description="The code to execute")


    predict = dspy.Predict(CodeGeneration)

    result = predict(question="Given an array, find if any of the two numbers sum up to 10")
    print(result.code)
    ```

    Example 2: dspy.Code as input type in code analysis:

    ```python
    import dspy
    import inspect

    dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

    class CodeAnalysis(dspy.Signature):
        '''Analyze the time complexity of the function.'''

        code: dspy.Code["python"] = dspy.InputField(description="The function to analyze")
        result: str = dspy.OutputField(description="The time complexity of the function")


    predict = dspy.Predict(CodeAnalysis)


    def sleepsort(x):
        import time

        for i in x:
            time.sleep(i)
            print(i)

    result = predict(code=inspect.getsource(sleepsort))
    print(result.result)
    ```
    """
⋮----
code: str
⋮----
language: ClassVar[str] = "python"
⋮----
def format(self)
⋮----
@pydantic.model_serializer()
    def serialize_model(self)
⋮----
"""Override to bypass the <<CUSTOM-TYPE-START-IDENTIFIER>> and <<CUSTOM-TYPE-END-IDENTIFIER>> tags."""
⋮----
@classmethod
    def description(cls) -> str
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, data: Any)
⋮----
def _filter_code(code: str) -> str
⋮----
"""Extract code from markdown code blocks, stripping any language identifier."""
# Case 1: format like:
# ```python
# {code_block}
# ```
regex_pattern = r"```(?:[^\n]*)\n(.*?)```"
match = re.search(regex_pattern, code, re.DOTALL)
⋮----
# Case 2: ```<code>``` (no language, single-line)
regex_pattern_simple = r"```(.*?)```"
match = re.search(regex_pattern_simple, code, re.DOTALL)
⋮----
# Fallback case
⋮----
# Patch __class_getitem__ directly on the class to support dspy.Code["python"] syntax
def _code_class_getitem(cls, language)
⋮----
code_with_language_cls = create_model(f"{cls.__name__}_{language}", __base__=cls)
</file>

<file path="dspy/adapters/types/document.py">
@experimental(version="3.0.4")
class Document(Type)
⋮----
"""A document type for providing content that can be cited by language models.

    This type represents documents that can be passed to language models for citation-enabled
    responses, particularly useful with Anthropic's Citations API. Documents include the content
    and metadata that helps the LM understand and reference the source material.

    Attributes:
        data: The text content of the document
        title: Optional title for the document (used in citations)
        media_type: MIME type of the document content (defaults to "text/plain")
        context: Optional context information about the document

    Example:
        ```python
        import dspy
        from dspy.signatures import Signature
        from dspy.experimental import Document, Citations

        class AnswerWithSources(Signature):
            '''Answer questions using provided documents with citations.'''
            documents: list[Document] = dspy.InputField()
            question: str = dspy.InputField()
            answer: str = dspy.OutputField()
            citations: Citations = dspy.OutputField()

        # Create documents
        docs = [
            Document(
                data="The Earth orbits the Sun in an elliptical path.",
                title="Basic Astronomy Facts"
            ),
            Document(
                data="Water boils at 100°C at standard atmospheric pressure.",
                title="Physics Fundamentals",
            )
        ]

        # Use with a citation-supporting model
        lm = dspy.LM("anthropic/claude-opus-4-1-20250805")
        predictor = dspy.Predict(AnswerWithSources)
        result = predictor(documents=docs, question="What temperature does water boil?", lm=lm)
        print(result.citations)
        ```
    """
⋮----
data: str
title: str | None = None
media_type: Literal["text/plain", "application/pdf"] = "text/plain"
context: str | None = None
⋮----
def format(self) -> list[dict[str, Any]]
⋮----
"""Format document for LM consumption.

        Returns:
            A list containing the document block in the format expected by citation-enabled language models.
        """
document_block = {
⋮----
@classmethod
    def description(cls) -> str
⋮----
"""Description of the document type for use in prompts."""
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, data: Any)
⋮----
# Handle case where data is just a string (data only)
⋮----
# Handle case where data is a dict
⋮----
def __str__(self) -> str
⋮----
"""String representation showing title and content length."""
title_part = f"'{self.title}': " if self.title else ""
</file>

<file path="dspy/adapters/types/file.py">
class File(Type)
⋮----
"""A file input type for DSPy.
    See https://platform.openai.com/docs/api-reference/chat/create#chat_create-messages-user_message-content-array_of_content_parts-file_content_part-file for specification.

    The file_data field should be a data URI with the format:
        data:<mime_type>;base64,<base64_encoded_data>

    Example:
        ```python
        import dspy

        class QA(dspy.Signature):
            file: dspy.File = dspy.InputField()
            summary = dspy.OutputField()
        program = dspy.Predict(QA)
        result = program(file=dspy.File.from_path("./research.pdf"))
        print(result.summary)
        ```
    """
⋮----
file_data: str | None = None
file_id: str | None = None
filename: str | None = None
⋮----
model_config = pydantic.ConfigDict(
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, values: Any) -> Any
⋮----
def format(self) -> list[dict[str, Any]]
⋮----
file_dict = {}
⋮----
def __str__(self)
⋮----
def __repr__(self)
⋮----
parts = []
⋮----
# file data has "data:text/plain;base64,..." format
mime_type = self.file_data.split(";")[0].split(":")[1]
len_data = len(self.file_data.split("base64,")[1]) if "base64," in self.file_data else len(self.file_data)
⋮----
len_data = len(self.file_data)
⋮----
@classmethod
    def from_path(cls, file_path: str, filename: str | None = None, mime_type: str | None = None) -> "File"
⋮----
"""Create a File from a local file path.

        Args:
            file_path: Path to the file to read
            filename: Optional filename to use (defaults to basename of path)
            mime_type: Optional MIME type (defaults to auto-detection from file extension)
        """
⋮----
file_bytes = f.read()
⋮----
filename = os.path.basename(file_path)
⋮----
mime_type = "application/octet-stream"
⋮----
encoded_data = base64.b64encode(file_bytes).decode("utf-8")
file_data = f"data:{mime_type};base64,{encoded_data}"
⋮----
"""Create a File from raw bytes.

        Args:
            file_bytes: Raw bytes of the file
            filename: Optional filename
            mime_type: MIME type (defaults to 'application/octet-stream')
        """
⋮----
@classmethod
    def from_file_id(cls, file_id: str, filename: str | None = None) -> "File"
⋮----
"""Create a File from an uploaded file ID."""
⋮----
def encode_file_to_dict(file_input: Any) -> dict
⋮----
"""
    Encode various file inputs to a dict with file_data, file_id, and/or filename.

    Args:
        file_input: Can be a file path (str), bytes, or File instance.

    Returns:
        dict: A dictionary with file_data, file_id, and/or filename keys.
    """
⋮----
result = {}
⋮----
file_obj = File.from_path(file_input)
⋮----
file_obj = File.from_bytes(file_input)
</file>

<file path="dspy/adapters/types/history.py">
class History(pydantic.BaseModel)
⋮----
"""Class representing the conversation history.

    The conversation history is a list of messages, each message entity should have keys from the associated signature.
    For example, if you have the following signature:

    ```
    class MySignature(dspy.Signature):
        question: str = dspy.InputField()
        history: dspy.History = dspy.InputField()
        answer: str = dspy.OutputField()
    ```

    Then the history should be a list of dictionaries with keys "question" and "answer".

    Example:
        ```
        import dspy

        dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

        class MySignature(dspy.Signature):
            question: str = dspy.InputField()
            history: dspy.History = dspy.InputField()
            answer: str = dspy.OutputField()

        history = dspy.History(
            messages=[
                {"question": "What is the capital of France?", "answer": "Paris"},
                {"question": "What is the capital of Germany?", "answer": "Berlin"},
            ]
        )

        predict = dspy.Predict(MySignature)
        outputs = predict(question="What is the capital of France?", history=history)
        ```

    Example of capturing the conversation history:
        ```
        import dspy

        dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

        class MySignature(dspy.Signature):
            question: str = dspy.InputField()
            history: dspy.History = dspy.InputField()
            answer: str = dspy.OutputField()

        predict = dspy.Predict(MySignature)
        outputs = predict(question="What is the capital of France?")
        history = dspy.History(messages=[{"question": "What is the capital of France?", **outputs}])
        outputs_with_history = predict(question="Are you sure?", history=history)
        ```
    """
⋮----
messages: list[dict[str, Any]]
⋮----
model_config = pydantic.ConfigDict(
</file>

<file path="dspy/adapters/types/image.py">
PIL_AVAILABLE = True
⋮----
PIL_AVAILABLE = False
⋮----
class Image(Type)
⋮----
url: str
⋮----
model_config = pydantic.ConfigDict(
⋮----
def __init__(self, url: Any = None, *, download: bool = False, **data)
⋮----
"""Create an Image.

        Parameters
        ----------
        url:
            The image source. Supported values include

            - ``str``: HTTP(S)/GS URL or local file path
            - ``bytes``: raw image bytes
            - ``PIL.Image.Image``: a PIL image instance
            - ``dict`` with a single ``{"url": value}`` entry (legacy form)
            - already encoded data URI

        download:
            Whether remote URLs should be downloaded to infer their MIME type.

        Any additional keyword arguments are passed to :class:`pydantic.BaseModel`.
        """
⋮----
# Support a positional argument while allowing ``url=`` in **data.
⋮----
# Legacy dict form from previous model validator.
⋮----
# ``url`` may be a string, bytes, or a PIL image.
⋮----
# Normalize any accepted input into a base64 data URI or plain URL.
⋮----
# Delegate the rest of initialization to pydantic's BaseModel.
⋮----
@lru_cache(maxsize=32)
    def format(self) -> list[dict[str, Any]] | str
⋮----
image_url = encode_image(self.url)
⋮----
@classmethod
    def from_url(cls, url: str, download: bool = False)
⋮----
@classmethod
    def from_file(cls, file_path: str)
⋮----
@classmethod
    def from_PIL(cls, pil_image):  # noqa: N802
⋮----
def __str__(self)
⋮----
def __repr__(self)
⋮----
len_base64 = len(self.url.split("base64,")[1])
image_type = self.url.split(";")[0].split("/")[-1]
⋮----
def is_url(string: str) -> bool
⋮----
"""Check if a string is a valid URL."""
⋮----
result = urlparse(string)
⋮----
def encode_image(image: Union[str, bytes, "PILImage.Image", dict], download_images: bool = False) -> str
⋮----
"""
    Encode an image or file to a base64 data URI.

    Args:
        image: The image or file to encode. Can be a PIL Image, file path, URL, or data URI.
        download_images: Whether to download images from URLs.

    Returns:
        str: The data URI of the file or the URL if download_images is False.

    Raises:
        ValueError: If the file type is not supported.
    """
⋮----
# NOTE: Not doing other validation for now
⋮----
# Already a data URI
⋮----
# File path
⋮----
# URL
⋮----
# Return the URL as is
⋮----
# Unsupported string format
⋮----
# PIL Image
⋮----
# Raw bytes
⋮----
img = PILImage.open(io.BytesIO(image))
⋮----
def _encode_image_from_file(file_path: str) -> str
⋮----
"""Encode a file from a file path to a base64 data URI."""
⋮----
file_data = file.read()
⋮----
# Use mimetypes to guess directly from the file path
⋮----
encoded_data = base64.b64encode(file_data).decode("utf-8")
⋮----
def _encode_image_from_url(image_url: str) -> str
⋮----
"""Encode a file from a URL to a base64 data URI."""
response = requests.get(image_url)
⋮----
content_type = response.headers.get("Content-Type", "")
⋮----
# Use the content type from the response headers if available
⋮----
mime_type = content_type
⋮----
# Try to guess MIME type from URL
⋮----
encoded_data = base64.b64encode(response.content).decode("utf-8")
⋮----
def _encode_pil_image(image: "PILImage") -> str
⋮----
"""Encode a PIL Image object to a base64 data URI."""
buffered = io.BytesIO()
file_format = image.format or "PNG"
⋮----
# Get the correct MIME type using the image format
file_extension = file_format.lower()
⋮----
encoded_data = base64.b64encode(buffered.getvalue()).decode("utf-8")
⋮----
def _get_file_extension(path_or_url: str) -> str
⋮----
"""Extract the file extension from a file path or URL."""
extension = os.path.splitext(urlparse(path_or_url).path)[1].lstrip(".").lower()
return extension or "png"  # Default to 'png' if no extension found
⋮----
def is_image(obj) -> bool
⋮----
"""Check if the object is an image or a valid media file reference."""
</file>

<file path="dspy/adapters/types/reasoning.py">
class Reasoning(Type)
⋮----
"""Reasoning type in DSPy.

    This type is useful when you want the DSPy output to include the reasoning of the LM. We build this type so that
    DSPy can support the reasoning model and non-reasoning model with the same code.

    This is a str-like type, you can convert a string directly to a Reasoning object, and from DSPy adapters'
    perspective, `Reasoning` is treated as a string.
    """
⋮----
content: str
⋮----
def format(self)
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, data: Any)
⋮----
# `lm_kwargs` overrides `lm.kwargs`.
reasoning_effort = lm_kwargs["reasoning_effort"]
⋮----
reasoning_effort = lm.kwargs["reasoning_effort"]
⋮----
# Turn on the native reasoning explicitly if Reasoning field is present in the signature and no explicit
# reasoning effort is set in `lm_kwargs` or `lm.kwargs`.
reasoning_effort = "low"
⋮----
# If users explicitly set `reasoning_effort` to None or the LM doesn't support reasoning, we don't enable
# native reasoning.
⋮----
# There is a caveat of Litellm as 1.79.0 that when using the chat completion API on GPT-5 family models,
# the reasoning content is not available in the response. As a workaround, we don't enable the native
# reasoning feature for GPT-5 family models when using the chat completion API.
# Litellm issue: https://github.com/BerriAI/litellm/issues/14748
⋮----
# Delete the reasoning field from the signature to use the native reasoning feature.
⋮----
@classmethod
    def parse_lm_response(cls, response: str | dict[str, Any]) -> Optional["Reasoning"]
⋮----
"""Parse the LM response into a Reasoning object."""
⋮----
@classmethod
    def parse_stream_chunk(cls, chunk) -> str | None
⋮----
"""
        Parse a stream chunk into reasoning content if available.

        Args:
            chunk: A stream chunk from the LM.

        Returns:
            The reasoning content (str) if available, None otherwise.
        """
⋮----
@classmethod
    def is_streamable(cls) -> bool
⋮----
def __repr__(self) -> str
⋮----
def __str__(self) -> str
⋮----
def __eq__(self, other: object) -> bool
⋮----
def __ne__(self, other: object) -> bool
⋮----
def __len__(self) -> int
⋮----
def __getitem__(self, key)
⋮----
def __contains__(self, item) -> bool
⋮----
def __iter__(self)
⋮----
def __add__(self, other)
⋮----
def __radd__(self, other)
⋮----
def __getattr__(self, name)
⋮----
"""
        Delegate string methods to the underlying content.

        This makes Reasoning fully str-like by forwarding any string method calls
        (like .strip(), .lower(), .split(), etc.) to the content string.

        Note: This is called only when the attribute is not found on the Reasoning instance,
        so it won't interfere with Pydantic fields or existing methods.
        """
# Check if this is a valid string method/attribute
⋮----
# Delegate to the content string
⋮----
# If it's not a string method, provide a helpful error
</file>

<file path="dspy/adapters/types/tool.py">
_TYPE_MAPPING = {"string": str, "integer": int, "number": float, "boolean": bool, "array": list, "object": dict}
⋮----
class Tool(Type)
⋮----
"""Tool class.

    This class is used to simplify the creation of tools for tool calling (function calling) in LLMs. Only supports
    functions for now.
    """
⋮----
func: Callable
name: str | None = None
desc: str | None = None
args: dict[str, Any] | None = None
arg_types: dict[str, Any] | None = None
arg_desc: dict[str, str] | None = None
has_kwargs: bool = False
⋮----
"""Initialize the Tool class.

        Users can choose to specify the `name`, `desc`, `args`, and `arg_types`, or let the `dspy.Tool`
        automatically infer the values from the function. For values that are specified by the user, automatic inference
        will not be performed on them.

        Args:
            func (Callable): The actual function that is being wrapped by the tool.
            name (Optional[str], optional): The name of the tool. Defaults to None.
            desc (Optional[str], optional): The description of the tool. Defaults to None.
            args (Optional[dict[str, Any]], optional): The args and their schema of the tool, represented as a
                dictionary from arg name to arg's json schema. Defaults to None.
            arg_types (Optional[dict[str, Any]], optional): The argument types of the tool, represented as a dictionary
                from arg name to the type of the argument. Defaults to None.
            arg_desc (Optional[dict[str, str]], optional): Descriptions for each arg, represented as a
                dictionary from arg name to description string. Defaults to None.

        Example:

        ```python
        def foo(x: int, y: str = "hello"):
            return str(x) + y

        tool = Tool(foo)
        print(tool.args)
        # Expected output: {'x': {'type': 'integer'}, 'y': {'type': 'string', 'default': 'hello'}}
        ```
        """
⋮----
def _parse_function(self, func: Callable, arg_desc: dict[str, str] | None = None)
⋮----
"""Helper method that parses a function to extract the name, description, and args.

        This is a helper function that automatically infers the name, description, and args of the tool from the
        provided function. In order to make the inference work, the function must have valid type hints.
        """
annotations_func = func if inspect.isfunction(func) or inspect.ismethod(func) else func.__call__
name = getattr(func, "__name__", type(func).__name__)
desc = getattr(func, "__doc__", None) or getattr(annotations_func, "__doc__", "")
args = {}
arg_types = {}
⋮----
# Use inspect.signature to get all arg names
sig = inspect.signature(annotations_func)
# Get available type hints
available_hints = get_type_hints(annotations_func)
# Build a dictionary of arg name -> type (defaulting to Any when missing)
hints = {param_name: available_hints.get(param_name, Any) for param_name in sig.parameters.keys()}
default_values = {param_name: sig.parameters[param_name].default for param_name in sig.parameters.keys()}
⋮----
# Process each argument's type to generate its JSON schema.
⋮----
# Check if the type (or its origin) is a subclass of Pydantic's BaseModel
origin = get_origin(v) or v
⋮----
# Get json schema, and replace $ref with the actual schema
v_json_schema = _resolve_json_schema_reference(v.model_json_schema())
⋮----
def _validate_and_parse_args(self, **kwargs)
⋮----
# Validate the args value comply to the json schema.
⋮----
instance = v.model_dump() if hasattr(v, "model_dump") else v
type_str = self.args[k].get("type")
⋮----
# Parse the args to the correct type.
parsed_kwargs = {}
⋮----
# Create a pydantic model wrapper with a dummy field `value` to parse the arg to the correct type.
# This is specifically useful for handling nested Pydantic models like `list[list[MyPydanticModel]]`
pydantic_wrapper = create_model("Wrapper", value=(self.arg_types[k], ...))
parsed = pydantic_wrapper.model_validate({"value": v})
⋮----
def format(self)
⋮----
def format_as_litellm_function_call(self)
⋮----
def _run_async_in_sync(self, coroutine)
⋮----
loop = asyncio.get_running_loop()
⋮----
# Run the coroutine outside of "except" block to avoid propagation
loop = None
⋮----
@with_callbacks
    def __call__(self, **kwargs)
⋮----
parsed_kwargs = self._validate_and_parse_args(**kwargs)
result = self.func(**parsed_kwargs)
⋮----
@with_callbacks
    async def acall(self, **kwargs)
⋮----
# We should allow calling a sync tool in the async path.
⋮----
@classmethod
    def from_mcp_tool(cls, session: "mcp.ClientSession", tool: "mcp.types.Tool") -> "Tool"
⋮----
"""
        Build a DSPy tool from an MCP tool and a ClientSession.

        Args:
            session: The MCP session to use.
            tool: The MCP tool to convert.

        Returns:
            A Tool object.
        """
⋮----
@classmethod
    def from_langchain(cls, tool: "BaseTool") -> "Tool"
⋮----
"""
        Build a DSPy tool from a LangChain tool.

        Args:
            tool: The LangChain tool to convert.

        Returns:
            A Tool object.

        Example:

        ```python
        import asyncio
        import dspy
        from langchain.tools import tool as lc_tool

        @lc_tool
        def add(x: int, y: int):
            "Add two numbers together."
            return x + y

        dspy_tool = dspy.Tool.from_langchain(add)

        async def run_tool():
            return await dspy_tool.acall(x=1, y=2)

        print(asyncio.run(run_tool()))
        # 3
        ```
        """
⋮----
def __repr__(self)
⋮----
def __str__(self)
⋮----
desc = f", whose description is <desc>{self.desc}</desc>.".replace("\n", "  ") if self.desc else "."
arg_desc = f"It takes arguments {self.args}."
⋮----
class ToolCalls(Type)
⋮----
class ToolCall(Type)
⋮----
name: str
args: dict[str, Any]
⋮----
def execute(self, functions: dict[str, Any] | list[Tool] | None = None) -> Any
⋮----
"""Execute this individual tool call and return its result.

            Args:
                functions: Functions to search for the tool. Can be:
                          - Dict mapping tool names to functions: {"tool_name": function}
                          - List of Tool objects: [Tool(function), ...]
                          - None: Will search in caller's locals and globals (automatic lookup)

            Returns:
                The result from executing this tool call.

            Raises:
                ValueError: If the tool function cannot be found.
                Exception: Any exception raised by the tool function.
            """
func = None
⋮----
# Automatic lookup in caller's globals and locals
frame = inspect.currentframe().f_back
⋮----
caller_globals = frame.f_globals
caller_locals = frame.f_locals
func = caller_locals.get(self.name) or caller_globals.get(self.name)
⋮----
func = functions.get(self.name)
⋮----
func = tool.func
⋮----
args = self.args or {}
⋮----
tool_calls: list[ToolCall]
⋮----
@classmethod
    def from_dict_list(cls, tool_calls_dicts: list[dict[str, Any]]) -> "ToolCalls"
⋮----
"""Convert a list of dictionaries to a ToolCalls instance.

        Args:
            dict_list: A list of dictionaries, where each dictionary should have 'name' and 'args' keys.

        Returns:
            A ToolCalls instance.

        Example:

            ```python
            tool_calls_dict = [
                {"name": "search", "args": {"query": "hello"}},
                {"name": "translate", "args": {"text": "world"}}
            ]
            tool_calls = ToolCalls.from_dict_list(tool_calls_dict)
            ```
        """
tool_calls = [cls.ToolCall(**item) for item in tool_calls_dicts]
⋮----
@classmethod
    def description(cls) -> str
⋮----
def format(self) -> list[dict[str, Any]]
⋮----
# The tool_call field is compatible with OpenAI's tool calls schema.
⋮----
@pydantic.model_validator(mode="before")
@classmethod
    def validate_input(cls, data: Any)
⋮----
# Handle case where data is a list of dicts with "name" and "args" keys
⋮----
# Handle case where data is a dict
⋮----
# Handle case where data is a dict with "tool_calls" key
tool_calls_data = data["tool_calls"]
⋮----
# Handle case where data is a dict with "name" and "args" keys
⋮----
def _resolve_json_schema_reference(schema: dict) -> dict
⋮----
"""Recursively resolve json model schema, expanding all references."""
⋮----
# If there are no definitions to resolve, return the main schema
⋮----
def resolve_refs(obj: Any) -> Any
⋮----
ref_path = obj["$ref"].split("/")[-1]
⋮----
# Must be a list
⋮----
# Resolve all references in the main schema
resolved_schema = resolve_refs(schema)
# Remove the $defs key as it's no longer needed
⋮----
"""Convert an input json schema to tool arguments compatible with DSPy Tool.

    Args:
        schema: An input json schema describing the tool's input parameters

    Returns:
        A tuple of (args, arg_types, arg_desc) for DSPy Tool definition.
    """
⋮----
properties = schema.get("properties", None)
⋮----
required = schema.get("required", [])
⋮----
defs = schema.get("$defs", {})
⋮----
prop = _resolve_json_schema_reference({"$defs": defs, **prop})
</file>

<file path="dspy/adapters/__init__.py">
__all__ = [
</file>

<file path="dspy/adapters/baml_adapter.py">
"""
Custom adapter for improving structured outputs using the information from Pydantic models.
Based on the format used by BAML: https://github.com/BoundaryML/baml
"""
⋮----
# Changing the comment symbol to Python's # rather than other languages' // seems to help
COMMENT_SYMBOL = "#"
INDENTATION = "  "
⋮----
"""Recursively renders a type annotation into a simplified string.

    Args:
        annotation: The type annotation to render
        depth: Current recursion depth (prevents infinite recursion)
        indent: Current indentation level for nested structures
    """
# Non-nested types
⋮----
origin = get_origin(annotation)
args = get_args(annotation)
⋮----
# Optional[T] or T | None
⋮----
non_none_args = [arg for arg in args if arg is not type(None)]
# Render the non-None part of the union
type_render = " or ".join([_render_type_str(arg, depth + 1, indent, seen_models) for arg in non_none_args])
# Add "or null" if None was part of the union
⋮----
# Literal[T1, T2, ...]
⋮----
# list[T]
⋮----
# For Pydantic models in lists, use bracket notation
inner_type = args[0]
⋮----
# Build inner schema - the Pydantic model inside should use indent level for array contents
inner_schema = _build_simplified_schema(inner_type, indent + 1, seen_models)
# Format with proper bracket notation and indentation
current_indent = INDENTATION * indent
⋮----
# dict[T1, T2]
⋮----
# fallback
⋮----
"""Builds a simplified, human-readable schema from a Pydantic model.

    Args:
        pydantic_model: The Pydantic model to build schema for
        indent: Current indentation level
        seen_models: Set to track visited pydantic models (prevents infinite recursion)
    """
seen_models = seen_models or set()
⋮----
# Add `pydantic_model` to `seen_models` with a placeholder value to avoid infinite recursion.
⋮----
lines = []
⋮----
next_indent = INDENTATION * (indent + 1)
⋮----
# Add model docstring as a comment above the object if it exists
# Only do this for top-level schemas (indent=0), since nested field docstrings
# are already added before the field name in the parent schema
⋮----
docstring = pydantic_model.__doc__.strip()
# Handle multiline docstrings by prefixing each line with the comment symbol
⋮----
line = line.strip()
⋮----
fields = pydantic_model.model_fields
⋮----
# If there's an alias but no description, show the alias as a comment
⋮----
# If the field type is a BaseModel, add its docstring as a comment before the field
field_annotation = field.annotation
# Handle Optional types
origin = get_origin(field_annotation)
⋮----
args = get_args(field_annotation)
⋮----
field_annotation = non_none_args[0]
⋮----
docstring = field_annotation.__doc__.strip()
⋮----
rendered_type = _render_type_str(field.annotation, indent=indent + 1, seen_models=seen_models)
line = f"{next_indent}{name}: {rendered_type},"
⋮----
class BAMLAdapter(JSONAdapter)
⋮----
"""
    A DSPy adapter that improves the rendering of complex/nested Pydantic models to help LMs.

    This adapter generates a compact, human-readable schema representation for nested Pydantic output
    fields, inspired by the BAML project's JSON formatter (https://github.com/BoundaryML/baml).
    The resulting rendered schema is more token-efficient and easier for smaller LMs to follow than a
    raw JSON schema. It also includes Pydantic field descriptions as comments in the schema, which
    provide valuable additional context for the LM to understand the expected output.

    Example Usage:
    ```python
    import dspy
    from pydantic import BaseModel, Field
    from typing import Literal
    from baml_adapter import BAMLAdapter  # Import from your module

    # 1. Define your Pydantic models
    class PatientAddress(BaseModel):
        street: str
        city: str
        country: Literal["US", "CA"]

    class PatientDetails(BaseModel):
        name: str = Field(description="Full name of the patient.")
        age: int
        address: PatientAddress | None

    # 2. Define a signature using the Pydantic model as an output field
    class ExtractPatientInfo(dspy.Signature):
        '''Extract patient information from the clinical note.'''
        clinical_note: str = dspy.InputField()
        patient_info: PatientDetails = dspy.OutputField()

    # 3. Configure dspy to use the new adapter
    llm = dspy.OpenAI(model="gpt-4.1-mini")
    dspy.configure(lm=llm, adapter=BAMLAdapter())

    # 4. Run your program
    extractor = dspy.Predict(ExtractPatientInfo)
    note = "John Doe, 45 years old, lives at 123 Main St, Anytown. Resident of the US."
    result = extractor(clinical_note=note)
    print(result.patient_info)

    # Expected output:
    # PatientDetails(name='John Doe', age=45, address=PatientAddress(street='123 Main St', city='Anytown', country='US'))
    ```
    """
⋮----
def format_field_structure(self, signature: type[Signature]) -> str
⋮----
"""Overrides the base method to generate a simplified schema for Pydantic models."""
⋮----
sections = []
⋮----
# Add structural explanation
⋮----
# Add input structure section
⋮----
sections.append("")  # Empty line after each input
⋮----
# Add output structure section
⋮----
field_type = field.annotation
⋮----
# Add completed section
⋮----
"""Overrides the base method to render Pydantic input instances as clean JSON."""
messages = [prefix]
⋮----
value = inputs.get(key)
formatted_value = ""
⋮----
# Use clean, indented JSON for Pydantic instances
formatted_value = value.model_dump_json(indent=2, by_alias=True)
⋮----
# Fallback to the original dspy formatter for other types
formatted_value = original_format_field_value(field_info=field_info, value=value)
⋮----
output_requirements = self.user_message_output_requirements(signature)
</file>

<file path="dspy/adapters/base.py">
logger = logging.getLogger(__name__)
⋮----
_DEFAULT_NATIVE_RESPONSE_TYPES = [Citations, Reasoning]
⋮----
class Adapter
⋮----
"""Base Adapter class.

    The Adapter serves as the interface layer between DSPy module/signature and Language Models (LMs). It handles the
    complete transformation pipeline from DSPy inputs to LM calls and back to structured outputs.

    Key responsibilities:
        - Transform user inputs and signatures into properly formatted LM prompts, which also instructs the LM to format
            the response in a specific format.
        - Parse LM outputs into dictionaries matching the signature's output fields.
        - Enable/disable native LM features (function calling, citations, etc.) based on configuration.
        - Handle conversation history, few-shot examples, and custom type processing.

    The adapter pattern allows DSPy to work with different LM interfaces while maintaining a consistent programming
    model for users.
    """
⋮----
"""
        Args:
            callbacks: List of callback functions to execute during `format()` and `parse()` methods. Callbacks can be
                used for logging, monitoring, or custom processing. Defaults to None (empty list).
            use_native_function_calling: Whether to enable native function calling capabilities when the LM supports it.
                If True, the adapter will automatically configure function calling when input fields contain `dspy.Tool`
                or `list[dspy.Tool]` types. Defaults to False.
            native_response_types: List of output field types that should be handled by native LM features rather than
                adapter parsing. For example, `dspy.Citations` can be populated directly by citation APIs
                (e.g., Anthropic's citation feature). Defaults to `[Citations]`.
        """
⋮----
def __init_subclass__(cls, **kwargs) -> None
⋮----
# Decorate format() and parse() method with with_callbacks
⋮----
tool_call_input_field_name = self._get_tool_call_input_field_name(signature)
tool_call_output_field_name = self._get_tool_call_output_field_name(signature)
⋮----
tools = inputs[tool_call_input_field_name]
tools = tools if isinstance(tools, list) else [tools]
⋮----
litellm_tools = []
⋮----
signature_for_native_function_calling = signature.delete(tool_call_output_field_name)
signature_for_native_function_calling = signature_for_native_function_calling.delete(
⋮----
# Handle custom types that use native LM features, e.g., reasoning, citations, etc.
⋮----
signature = field.annotation.adapt_to_native_lm_feature(signature, name, lm, lm_kwargs)
⋮----
values = []
⋮----
tool_call_output_field_name = self._get_tool_call_output_field_name(original_signature)
⋮----
output_logprobs = None
tool_calls = None
text = output
⋮----
text = output["text"]
output_logprobs = output.get("logprobs")
tool_calls = output.get("tool_calls")
⋮----
value = self.parse(processed_signature, text)
⋮----
# We need to set the field not present in the processed signature to None for consistency.
⋮----
value = {}
⋮----
tool_calls = [
⋮----
# Parse custom types that does not rely on the `Adapter.parse()` method
⋮----
parsed_value = field.annotation.parse_lm_response(output)
⋮----
"""
        Execute the adapter pipeline: format inputs, call LM, and parse outputs.

        Args:
            lm: The Language Model instance to use for generation. Must be an instance of `dspy.BaseLM`.
            lm_kwargs: Additional keyword arguments to pass to the LM call (e.g., temperature, max_tokens). These are
                passed directly to the LM.
            signature: The DSPy signature associated with this LM call.
            demos: List of few-shot examples to include in the prompt. Each dictionary should contain keys matching the
                signature's input and output field names. Examples are formatted as user/assistant message pairs.
            inputs: The current input values for this call. Keys must match the signature's input field names.

        Returns:
            List of dictionaries representing parsed LM responses. Each dictionary contains keys matching the
            signature's output field names. For multiple generations (n > 1), returns multiple dictionaries.
        """
processed_signature = self._call_preprocess(lm, lm_kwargs, signature, inputs)
inputs = self.format(processed_signature, demos, inputs)
⋮----
outputs = lm(messages=inputs, **lm_kwargs)
⋮----
outputs = await lm.acall(messages=inputs, **lm_kwargs)
⋮----
"""Format the input messages for the LM call.

        This method converts the DSPy structured input along with few-shot examples and conversation history into
        multiturn messages as expected by the LM. For custom adapters, this method can be overridden to customize
        the formatting of the input messages.

        In general we recommend the messages to have the following structure:
        ```
        [
            {"role": "system", "content": system_message},
            # Begin few-shot examples
            {"role": "user", "content": few_shot_example_1_input},
            {"role": "assistant", "content": few_shot_example_1_output},
            {"role": "user", "content": few_shot_example_2_input},
            {"role": "assistant", "content": few_shot_example_2_output},
            ...
            # End few-shot examples
            # Begin conversation history
            {"role": "user", "content": conversation_history_1_input},
            {"role": "assistant", "content": conversation_history_1_output},
            {"role": "user", "content": conversation_history_2_input},
            {"role": "assistant", "content": conversation_history_2_output},
            ...
            # End conversation history
            {"role": "user", "content": current_input},
        ]

        And system message should contain the field description, field structure, and task description.
        ```


        Args:
            signature: The DSPy signature for which to format the input messages.
            demos: A list of few-shot examples.
            inputs: The input arguments to the DSPy module.

        Returns:
            A list of multiturn messages as expected by the LM.
        """
inputs_copy = dict(inputs)
⋮----
# If the signature and inputs have conversation history, we need to format the conversation history and
# remove the history field from the signature.
history_field_name = self._get_history_field_name(signature)
⋮----
# In order to format the conversation history, we need to remove the history field from the signature.
signature_without_history = signature.delete(history_field_name)
conversation_history = self.format_conversation_history(
⋮----
messages = []
system_message = self.format_system_message(signature)
⋮----
# Conversation history and current input
content = self.format_user_message_content(signature_without_history, inputs_copy, main_request=True)
⋮----
# Only current input
content = self.format_user_message_content(signature, inputs_copy, main_request=True)
⋮----
messages = split_message_content_for_custom_types(messages)
⋮----
def format_system_message(self, signature: type[Signature]) -> str
⋮----
"""Format the system message for the LM call.


        Args:
            signature: The DSPy signature for which to format the system message.
        """
⋮----
def format_field_description(self, signature: type[Signature]) -> str
⋮----
"""Format the field description for the system message.

        This method formats the field description for the system message. It should return a string that contains
        the field description for the input fields and the output fields.

        Args:
            signature: The DSPy signature for which to format the field description.

        Returns:
            A string that contains the field description for the input fields and the output fields.
        """
⋮----
def format_field_structure(self, signature: type[Signature]) -> str
⋮----
"""Format the field structure for the system message.

        This method formats the field structure for the system message. It should return a string that dictates the
        format the input fields should be provided to the LM, and the format the output fields will be in the response.
        Refer to the ChatAdapter and JsonAdapter for an example.

        Args:
            signature: The DSPy signature for which to format the field structure.
        """
⋮----
def format_task_description(self, signature: type[Signature]) -> str
⋮----
"""Format the task description for the system message.

        This method formats the task description for the system message. In most cases this is just a thin wrapper
        over `signature.instructions`.

        Args:
            signature: The DSPy signature of the DSpy module.

        Returns:
            A string that describes the task.
        """
⋮----
"""Format the user message content.

        This method formats the user message content, which can be used in formatting few-shot examples, conversation
        history, and the current input.

        Args:
            signature: The DSPy signature for which to format the user message content.
            inputs: The input arguments to the DSPy module.
            prefix: A prefix to the user message content.
            suffix: A suffix to the user message content.

        Returns:
            A string that contains the user message content.
        """
⋮----
"""Format the assistant message content.

        This method formats the assistant message content, which can be used in formatting few-shot examples,
        conversation history.

        Args:
            signature: The DSPy signature for which to format the assistant message content.
            outputs: The output fields to be formatted.
            missing_field_message: A message to be used when a field is missing.

        Returns:
            A string that contains the assistant message content.
        """
⋮----
def format_demos(self, signature: type[Signature], demos: list[dict[str, Any]]) -> list[dict[str, Any]]
⋮----
"""Format the few-shot examples.

        This method formats the few-shot examples as multiturn messages.

        Args:
            signature: The DSPy signature for which to format the few-shot examples.
            demos: A list of few-shot examples, each element is a dictionary with keys of the input and output fields of
                the signature.

        Returns:
            A list of multiturn messages.
        """
complete_demos = []
incomplete_demos = []
⋮----
# Check if all fields are present and not None
is_complete = all(k in demo and demo[k] is not None for k in signature.fields)
⋮----
# Check if demo has at least one input and one output field
has_input = any(k in demo for k in signature.input_fields)
has_output = any(k in demo for k in signature.output_fields)
⋮----
# We only keep incomplete demos that have at least one input and one output field
⋮----
incomplete_demo_prefix = "This is an example of the task, though some input or output fields are not supplied."
⋮----
def _get_history_field_name(self, signature: type[Signature]) -> bool
⋮----
def _get_tool_call_input_field_name(self, signature: type[Signature]) -> bool
⋮----
# Look for annotation `list[dspy.Tool]` or `dspy.Tool`
origin = get_origin(field.annotation)
⋮----
def _get_tool_call_output_field_name(self, signature: type[Signature]) -> bool
⋮----
"""Format the conversation history.

        This method formats the conversation history and the current input as multiturn messages.

        Args:
            signature: The DSPy signature for which to format the conversation history.
            history_field_name: The name of the history field in the signature.
            inputs: The input arguments to the DSPy module.

        Returns:
            A list of multiturn messages.
        """
conversation_history = inputs[history_field_name].messages if history_field_name in inputs else None
⋮----
# Remove the history field from the inputs
⋮----
def parse(self, signature: type[Signature], completion: str) -> dict[str, Any]
⋮----
"""Parse the LM output into a dictionary of the output fields.

        This method parses the LM output into a dictionary of the output fields.

        Args:
            signature: The DSPy signature for which to parse the LM output.
            completion: The LM output to be parsed.

        Returns:
            A dictionary of the output fields.
        """
</file>

<file path="dspy/adapters/chat_adapter.py">
field_header_pattern = re.compile(r"\[\[ ## (\w+) ## \]\]")
⋮----
class FieldInfoWithName(NamedTuple)
⋮----
name: str
info: FieldInfo
⋮----
class ChatAdapter(Adapter)
⋮----
"""Default Adapter for most language models.

    The ChatAdapter formats DSPy signatures into a format compatible with most language models.
    It uses delimiter patterns like `[[ ## field_name ## ]]` to clearly separate input and output fields in
    the message content.

    Key features:
        - Structures inputs and outputs using field header markers for clear field delineation.
        - Provides automatic fallback to JSONAdapter if the chat format fails.
    """
⋮----
"""
        Args:
            callbacks: List of callback functions to execute during adapter methods.
            use_native_function_calling: Whether to enable native function calling capabilities.
            native_response_types: List of output field types handled by native LM features.
            use_json_adapter_fallback: Whether to automatically fallback to JSONAdapter if the ChatAdapter fails.
                If True, when an error occurs (except ContextWindowExceededError), the adapter will retry using
                JSONAdapter. Defaults to True.
        """
⋮----
# fallback to JSONAdapter
⋮----
# On context window exceeded error, already using JSONAdapter, or use_json_adapter_fallback is False
# we don't want to retry with a different adapter. Raise the original error instead of the fallback error.
⋮----
def format_field_description(self, signature: type[Signature]) -> str
⋮----
def format_field_structure(self, signature: type[Signature]) -> str
⋮----
"""
        `ChatAdapter` requires input and output fields to be in their own sections, with section header using markers
        `[[ ## field_name ## ]]`. An arbitrary field `completed` ([[ ## completed ## ]]) is added to the end of the
        output fields section to indicate the end of the output fields.
        """
parts = []
⋮----
def format_signature_fields_for_instructions(fields: dict[str, FieldInfo])
⋮----
def format_task_description(self, signature: type[Signature]) -> str
⋮----
instructions = textwrap.dedent(signature.instructions)
objective = ("\n" + " " * 8).join([""] + instructions.splitlines())
⋮----
messages = [prefix]
⋮----
value = inputs.get(k)
formatted_field_value = format_field_value(field_info=v, value=value)
⋮----
output_requirements = self.user_message_output_requirements(signature)
⋮----
def user_message_output_requirements(self, signature: type[Signature]) -> str
⋮----
"""Returns a simplified format reminder for the language model.

        In chat-based interactions, language models may lose track of the required output format
        as the conversation context grows longer. This method generates a concise reminder of
        the expected output structure that can be included in user messages.

        Args:
            signature (Type[Signature]): The DSPy signature defining the expected input/output fields.

        Returns:
            str: A simplified description of the required output format.

        Note:
            This is a more lightweight version of `format_field_structure` specifically designed
            for inline reminders within chat messages.
        """
⋮----
def type_info(v)
⋮----
message = "Respond with the corresponding output fields, starting with the field "
⋮----
assistant_message_content = self.format_field_with_value(
⋮----
def parse(self, signature: type[Signature], completion: str) -> dict[str, Any]
⋮----
sections = [(None, [])]
⋮----
match = field_header_pattern.match(line.strip())
⋮----
# If the header pattern is found, split the rest of the line as content
header = match.group(1)
remaining_content = line[match.end() :].strip()
⋮----
sections = [(k, "\n".join(v).strip()) for k, v in sections]
⋮----
fields = {}
⋮----
def format_field_with_value(self, fields_with_values: dict[FieldInfoWithName, Any]) -> str
⋮----
"""
        Formats the values of the specified fields according to the field's DSPy type (input or output),
        annotation (e.g. str, int, etc.), and the type of the value itself. Joins the formatted values
        into a single string, which is is a multiline string if there are multiple fields.

        Args:
            fields_with_values: A dictionary mapping information about a field to its corresponding
                value.

        Returns:
            The joined formatted values of the fields, represented as a string
        """
output = []
⋮----
formatted_field_value = format_field_value(field_info=field.info, value=field_value)
⋮----
"""
        Format the call data into finetuning data according to the OpenAI API specifications.

        For the chat adapter, this means formatting the data as a list of messages, where each message is a dictionary
        with a "role" and "content" key. The role can be "system", "user", or "assistant". Then, the messages are
        wrapped in a dictionary with a "messages" key.
        """
system_user_messages = self.format(  # returns a list of dicts with the keys "role" and "content"
assistant_message_content = self.format_assistant_message_content(  # returns a string, without the role
assistant_message = {"role": "assistant", "content": assistant_message_content}
messages = system_user_messages + [assistant_message]
</file>

<file path="dspy/adapters/json_adapter.py">
logger = logging.getLogger(__name__)
⋮----
def _has_open_ended_mapping(signature: SignatureMeta) -> bool
⋮----
"""
    Check whether any output field in the signature has an open-ended mapping type,
    such as dict[str, Any]. Structured Outputs require explicit properties, so such fields
    are incompatible.
    """
⋮----
annotation = field.annotation
⋮----
class JSONAdapter(ChatAdapter)
⋮----
def __init__(self, callbacks: list[BaseCallback] | None = None, use_native_function_calling: bool = True)
⋮----
# JSONAdapter uses native function calling by default.
⋮----
def _json_adapter_call_common(self, lm, lm_kwargs, signature, demos, inputs, call_fn)
⋮----
"""Common call logic to be used for both sync and async calls."""
provider = lm.model.split("/", 1)[0] or "openai"
params = litellm.get_supported_openai_params(model=lm.model, custom_llm_provider=provider)
⋮----
has_tool_calls = any(field.annotation == ToolCalls for field in signature.output_fields.values())
# Some models support json mode but not structured outputs
# Follows guidance from: https://docs.litellm.ai/docs/completion/json_mode#check-model-support
supports_structured_outputs = litellm.supports_response_schema(model=lm.model, custom_llm_provider=provider)
⋮----
# We found that structured output mode doesn't work well with dspy.ToolCalls as output field.
# So we fall back to json mode if native function calling is disabled and ToolCalls is present.
⋮----
result = self._json_adapter_call_common(lm, lm_kwargs, signature, demos, inputs, super().__call__)
⋮----
structured_output_model = _get_structured_outputs_response_format(
⋮----
result = self._json_adapter_call_common(lm, lm_kwargs, signature, demos, inputs, super().acall)
⋮----
structured_output_model = _get_structured_outputs_response_format(signature)
⋮----
def format_field_structure(self, signature: type[Signature]) -> str
⋮----
parts = []
⋮----
def format_signature_fields_for_instructions(fields: dict[str, FieldInfo], role: str)
⋮----
def user_message_output_requirements(self, signature: type[Signature]) -> str
⋮----
def type_info(v)
⋮----
message = "Respond with a JSON object in the following order of fields: "
⋮----
fields_with_values = {
⋮----
def parse(self, signature: type[Signature], completion: str) -> dict[str, Any]
⋮----
fields = json_repair.loads(completion)
⋮----
pattern = r"\{(?:[^{}]|(?R))*\}"
match = regex.search(pattern, completion, regex.DOTALL)
⋮----
completion = match.group(0)
⋮----
fields = {k: v for k, v in fields.items() if k in signature.output_fields}
⋮----
# Attempt to cast each value to type signature.output_fields[k].annotation.
⋮----
def format_field_with_value(self, fields_with_values: dict[FieldInfoWithName, Any], role: str = "user") -> str
⋮----
"""
        Formats the values of the specified fields according to the field's DSPy type (input or output),
        annotation (e.g. str, int, etc.), and the type of the value itself. Joins the formatted values
        into a single string, which is a multiline string if there are multiple fields.

        Args:
            fields_with_values: A dictionary mapping information about a field to its corresponding value.
        Returns:
            The joined formatted values of the fields, represented as a string.
        """
⋮----
output = []
⋮----
formatted_field_value = format_field_value(field_info=field.info, value=field_value)
⋮----
d = fields_with_values.items()
d = {k.name: v for k, v in d}
⋮----
# TODO: implement format_finetune_data method in JSONAdapter
⋮----
"""
    Builds a Pydantic model from a DSPy signature's output_fields and ensures the generated JSON schema
    is compatible with OpenAI Structured Outputs (all objects have a "required" key listing every property,
    and additionalProperties is always false).

    IMPORTANT: If any field's annotation is an open-ended mapping (e.g. dict[str, Any]), then a structured
    schema cannot be generated since all properties must be explicitly declared. In that case, an exception
    is raised so that the caller can fall back to using a plain "json_object" response_format.
    """
# Although we've already performed an early check, we keep this here as a final guard.
⋮----
fields = {}
⋮----
# Skip ToolCalls field if native function calling is enabled.
⋮----
default = field.default if hasattr(field, "default") else ...
⋮----
# Build the model with extra fields forbidden.
pydantic_model = pydantic.create_model(
⋮----
# Generate the initial schema.
schema = pydantic_model.model_json_schema()
⋮----
# Remove any DSPy-specific metadata.
⋮----
def enforce_required(schema_part: dict)
⋮----
"""
        Recursively ensure that:
            - for any object schema, a "required" key is added with all property names (or [] if no properties)
            - additionalProperties is set to False regardless of the previous value.
            - the same enforcement is run for nested arrays and definitions.
        """
⋮----
props = schema_part.get("properties")
⋮----
# For objects with explicitly declared properties:
⋮----
# For objects with no properties (should not happen normally but a fallback).
⋮----
# Also enforce in any nested definitions / $defs.
⋮----
# Override the model's JSON schema generation to return our precomputed schema.
</file>

<file path="dspy/adapters/two_step_adapter.py">
"""
NOTE/TODO/FIXME:

The main issue below is that the second step's signature is entirely created on the fly and is invoked with a chat
adapter explicitly constructed with no demonstrations. This means that it cannot "learn" or get optimized.
"""
⋮----
class TwoStepAdapter(Adapter)
⋮----
"""
    A two-stage adapter that:
        1. Uses a simpler, more natural prompt for the main LM
        2. Uses a smaller LM with chat adapter to extract structured data from the response of main LM
    This adapter uses a common __call__ logic defined in base Adapter class.
    This class is particularly useful when interacting with reasoning models as the main LM since reasoning models
    are known to struggle with structured outputs.

    Example:
    ```
    import dspy
    lm = dspy.LM(model="openai/o3-mini", max_tokens=16000, temperature = 1.0)
    adapter = dspy.TwoStepAdapter(dspy.LM("openai/gpt-4o-mini"))
    dspy.configure(lm=lm, adapter=adapter)
    program = dspy.ChainOfThought("question->answer")
    result = program("What is the capital of France?")
    print(result)
    ```
    """
⋮----
def __init__(self, extraction_model: LM, **kwargs)
⋮----
"""
        Format a prompt for the first stage with the main LM.
        This no specific structure is required for the main LM, we customize the format method
        instead of format_field_description or format_field_structure.

        Args:
            signature: The signature of the original task
            demos: A list of demo examples
            inputs: The current input

        Returns:
            A list of messages to be passed to the main LM.
        """
messages = []
⋮----
# Create a task description for the main LM
task_description = self.format_task_description(signature)
⋮----
# Format the current input
⋮----
def parse(self, signature: Signature, completion: str) -> dict[str, Any]
⋮----
"""
        Use a smaller LM (extraction_model) with chat adapter to extract structured data
        from the raw completion text of the main LM.

        Args:
            signature: The signature of the original task
            completion: The completion from the main LM

        Returns:
            A dictionary containing the extracted structured data.
        """
# The signature is supposed to be "text -> {original output fields}"
extractor_signature = self._create_extractor_signature(signature)
⋮----
# Call the smaller LM to extract structured data from the raw completion text with ChatAdapter
parsed_result = ChatAdapter()(
⋮----
inputs = self.format(signature, demos, inputs)
⋮----
outputs = await lm.acall(messages=inputs, **lm_kwargs)
⋮----
values = []
⋮----
tool_call_output_field_name = self._get_tool_call_output_field_name(signature)
⋮----
output_logprobs = None
tool_calls = None
text = output
⋮----
text = output["text"]
output_logprobs = output.get("logprobs")
tool_calls = output.get("tool_calls")
⋮----
value = await ChatAdapter().acall(
value = value[0]
⋮----
tool_calls = [
⋮----
def format_task_description(self, signature: Signature) -> str
⋮----
"""Create a description of the task based on the signature"""
parts = []
⋮----
parts = [prefix]
⋮----
"""Create a new signature containing a new 'text' input field and all output fields.

        Args:
            original_signature: The original signature to extract output fields from

        Returns:
            A new Signature type with a text input field and all output fields
        """
# Create new fields dict with 'text' input field and all output fields
new_fields = {
⋮----
outputs_str = ", ".join([f"`{field}`" for field in original_signature.output_fields])
instructions = f"The input is a text that should contain all the necessary information to produce the fields {outputs_str}. \
</file>

<file path="dspy/adapters/utils.py">
def serialize_for_json(value: Any) -> Any
⋮----
"""
    Formats the specified value so that it can be serialized as a JSON string.

    Args:
        value: The value to format as a JSON string.
    Returns:
        The formatted value, which is serializable as a JSON string.
    """
# Attempt to format the value as a JSON-compatible object using pydantic, falling back to
# a string representation of the value if that fails (e.g. if the value contains an object
# that pydantic doesn't recognize or can't serialize)
⋮----
def format_field_value(field_info: FieldInfo, value: Any, assume_text=True) -> str | dict
⋮----
"""
    Formats the value of the specified field according to the field's DSPy type (input or output),
    annotation (e.g. str, int, etc.), and the type of the value itself.

    Args:
      field_info: Information about the field, including its DSPy field type and annotation.
      value: The value of the field.
    Returns:
      The formatted value of the field, represented as a string.
    """
string_value = None
⋮----
# If the field has no special type requirements, format it as a nice numbered list for the LM.
string_value = _format_input_list_field_value(value)
⋮----
jsonable_value = serialize_for_json(value)
⋮----
string_value = json.dumps(jsonable_value, ensure_ascii=False)
⋮----
# If the value is not a Python representation of a JSON object or Array
# (e.g. the value is a JSON string), just use the string representation of the value
# to avoid double-quoting the JSON string (which would hurt accuracy for certain
# tasks, e.g. tasks that rely on computing string length)
string_value = str(jsonable_value)
⋮----
def _get_json_schema(field_type)
⋮----
def move_type_to_front(d)
⋮----
# Move the 'type' key to the front of the dictionary, recursively, for LLM readability/adherence.
⋮----
schema = pydantic.TypeAdapter(field_type).json_schema()
schema = move_type_to_front(schema)
⋮----
def translate_field_type(field_name, field_info)
⋮----
field_type = field_info.annotation
⋮----
desc = ""
⋮----
desc = "must be True or False"
⋮----
desc = f"must be a single {field_type.__name__} value"
⋮----
enum_vals = "; ".join(str(member.value) for member in field_type)
desc = f"must be one of: {enum_vals}"
⋮----
desc = (
⋮----
# Strongly encourage the LM to avoid choosing values that don't appear in the
# literal or returning a value of the form 'Literal[<selected_value>]'
⋮----
desc = f"must adhere to the JSON schema: {json.dumps(_get_json_schema(field_type), ensure_ascii=False)}"
⋮----
desc = (" " * 8) + f"# note: the value you produce {desc}" if desc else ""
⋮----
def find_enum_member(enum, identifier)
⋮----
"""
    Finds the enum member corresponding to the specified identifier, which may be the
    enum member's name or value.

    Args:
        enum: The enum to search for the member.
        identifier: If the enum is explicitly-valued, this is the value of the enum member to find.
                    If the enum is auto-valued, this is the name of the enum member to find.
    Returns:
        The enum member corresponding to the specified identifier.
    """
# Check if the identifier is a valid enum member value *before* checking if it's a valid enum
# member name, since the identifier will be a value for explicitly-valued enums. This handles
# the (rare) case where an enum member value is the same as another enum member's name in
# an explicitly-valued enum
⋮----
# If the identifier is not a valid enum member value, check if it's a valid enum member name,
# since the identifier will be a member name for auto-valued enums
⋮----
def parse_value(value, annotation)
⋮----
origin = get_origin(annotation)
⋮----
allowed = get_args(annotation)
⋮----
v = value.strip()
⋮----
v = v[v.find("[") + 1 : -1]
⋮----
v = v[1:-1]
⋮----
# Handle union annotations, e.g., `str | None`, `Optional[str]`, `Union[str, int, None]`, etc.
⋮----
candidate = json_repair.loads(value)  # json_repair.loads returns "" on failure.
⋮----
candidate = ast.literal_eval(value)
⋮----
candidate = value
⋮----
# For dspy.Type, try parsing from the original value in case it has a custom parser
⋮----
def get_annotation_name(annotation)
⋮----
args = get_args(annotation)
⋮----
# Keep backward compatibility with the old behavior in `dspy.ChainOfThought`, where reasoning
# field type is treated as a string.
⋮----
args_str = ", ".join(
⋮----
args_str = ", ".join(get_annotation_name(a) for a in args)
⋮----
def get_field_description_string(fields: dict) -> str
⋮----
field_descriptions = []
⋮----
field_message = f"{idx + 1}. `{k}`"
⋮----
desc = v.json_schema_extra["desc"] if v.json_schema_extra["desc"] != f"${{{k}}}" else ""
⋮----
custom_types = DspyType.extract_custom_type_from_annotation(v.annotation)
⋮----
def _format_input_list_field_value(value: list[Any]) -> str
⋮----
"""
    Formats the value of an input field of type list[Any].

    Args:
      value: The value of the list-type input field.
    Returns:
      A string representation of the input field's list value.
    """
⋮----
def _format_blob(blob: str) -> str
⋮----
"""
    Formats the specified text blobs so that an LM can parse it correctly within a list
    of multiple text blobs.

    Args:
        blob: The text blob to format.
    Returns:
        The formatted text blob.
    """
⋮----
modified_blob = blob.replace("\n", "\n    ")
⋮----
def _quoted_string_for_literal_type_annotation(s: str) -> str
⋮----
"""
    Return the specified string quoted for inclusion in a literal type annotation.
    """
has_single = "'" in s
has_double = '"' in s
⋮----
# Only single quotes => enclose in double quotes
⋮----
# Only double quotes => enclose in single quotes
⋮----
# Both => enclose in single quotes; escape each single quote with \'
escaped = s.replace("'", "\\'")
⋮----
# Neither => enclose in single quotes
</file>

<file path="dspy/adapters/xml_adapter.py">
class XMLAdapter(ChatAdapter)
⋮----
def __init__(self, callbacks: list[BaseCallback] | None = None)
⋮----
def format_field_with_value(self, fields_with_values: dict[FieldInfoWithName, Any]) -> str
⋮----
output = []
⋮----
formatted = format_field_value(field_info=field.info, value=field_value)
⋮----
def format_field_structure(self, signature: type[Signature]) -> str
⋮----
"""
        XMLAdapter requires input and output fields to be wrapped in XML tags like `<field_name>`.
        """
⋮----
parts = []
⋮----
def format_signature_fields_for_instructions(fields: dict[str, FieldInfo])
⋮----
messages = [prefix]
⋮----
output_requirements = self.user_message_output_requirements(signature)
⋮----
def user_message_output_requirements(self, signature: type[Signature]) -> str
⋮----
message = "Respond with the corresponding output fields wrapped in XML tags "
⋮----
def parse(self, signature: type[Signature], completion: str) -> dict[str, Any]
⋮----
fields = {}
⋮----
name = match.group("name")
content = match.group("content").strip()
⋮----
# Cast values using base class parse_value helper
⋮----
def _parse_field_value(self, field_info, raw, completion, signature)
</file>

<file path="dspy/clients/__init__.py">
logger = logging.getLogger(__name__)
⋮----
DISK_CACHE_DIR = os.environ.get("DSPY_CACHEDIR") or os.path.join(Path.home(), ".dspy_cache")
DISK_CACHE_LIMIT = int(os.environ.get("DSPY_CACHE_LIMIT", 3e10))  # 30 GB default
⋮----
"""Configure the cache for DSPy.

    Args:
        enable_disk_cache: Whether to enable on-disk cache.
        enable_memory_cache: Whether to enable in-memory cache.
        disk_cache_dir: The directory to store the on-disk cache.
        disk_size_limit_bytes: The size limit of the on-disk cache.
        memory_max_entries: The maximum number of entries in the in-memory cache. To allow the cache to grow without
                            bounds, set this parameter to `math.inf` or a similar value.
    """
⋮----
DSPY_CACHE = Cache(
⋮----
# Update the reference to point to the new cache
⋮----
litellm.cache = None  # By default we disable LiteLLM cache and use DSPy on-disk cache.
⋮----
def _get_dspy_cache()
⋮----
disk_cache_dir = os.environ.get("DSPY_CACHEDIR") or os.path.join(Path.home(), ".dspy_cache")
disk_cache_limit = int(os.environ.get("DSPY_CACHE_LIMIT", 3e10))
⋮----
_dspy_cache = Cache(
⋮----
# If cache creation fails (e.g., in AWS Lambda), create a memory-only cache
⋮----
DSPY_CACHE = _get_dspy_cache()
⋮----
# Accessed at run time by litellm; i.e., fine to keep after import
⋮----
def configure_litellm_logging(level: str = "ERROR")
⋮----
"""Configure LiteLLM logging to the specified level."""
# Litellm uses a global logger called `verbose_logger` to control all loggings.
⋮----
numeric_logging_level = getattr(logging, level)
⋮----
def enable_litellm_logging()
⋮----
def disable_litellm_logging()
⋮----
# By default, we disable LiteLLM logging for clean logging
⋮----
__all__ = [
</file>

<file path="dspy/clients/base_lm.py">
MAX_HISTORY_SIZE = 10_000
GLOBAL_HISTORY = []
⋮----
class BaseLM
⋮----
"""Base class for handling LLM calls.

    Most users can directly use the `dspy.LM` class, which is a subclass of `BaseLM`. Users can also implement their
    own subclasses of `BaseLM` to support custom LLM providers and inject custom logic. To do so, simply override the
    `forward` method and make sure the return format is identical to the
    [OpenAI response format](https://platform.openai.com/docs/api-reference/responses/object).

    Example:

    ```python
    from openai import OpenAI

    import dspy


    class MyLM(dspy.BaseLM):
        def forward(self, prompt, messages=None, **kwargs):
            client = OpenAI()
            return client.chat.completions.create(
                model=self.model,
                messages=messages or [{"role": "user", "content": prompt}],
                **self.kwargs,
            )


    lm = MyLM(model="gpt-4o-mini")
    dspy.configure(lm=lm)
    print(dspy.Predict("q->a")(q="Why did the chicken cross the kitchen?"))
    ```
    """
⋮----
def __init__(self, model, model_type="chat", temperature=0.0, max_tokens=1000, cache=True, **kwargs)
⋮----
def _process_lm_response(self, response, prompt, messages, **kwargs)
⋮----
merged_kwargs = {**self.kwargs, **kwargs}
⋮----
outputs = self._process_response(response)
⋮----
outputs = self._process_completion(response, merged_kwargs)
⋮----
# Logging, with removed api key & where `cost` is None on cache hit.
kwargs = {k: v for k, v in kwargs.items() if not k.startswith("api_")}
entry = {
⋮----
response = self.forward(prompt=prompt, messages=messages, **kwargs)
outputs = self._process_lm_response(response, prompt, messages, **kwargs)
⋮----
response = await self.aforward(prompt=prompt, messages=messages, **kwargs)
⋮----
"""Forward pass for the language model.

        Subclasses must implement this method, and the response should be identical to either of the following formats:
        - [OpenAI response format](https://platform.openai.com/docs/api-reference/responses/object)
        - [OpenAI chat completion format](https://platform.openai.com/docs/api-reference/chat/object)
        - [OpenAI text completion format](https://platform.openai.com/docs/api-reference/completions/object)
        """
⋮----
"""Async forward pass for the language model.

        Subclasses must implement this method, and the response should be identical to either of the following formats:
        - [OpenAI response format](https://platform.openai.com/docs/api-reference/responses/object)
        - [OpenAI chat completion format](https://platform.openai.com/docs/api-reference/chat/object)
        - [OpenAI text completion format](https://platform.openai.com/docs/api-reference/completions/object)
        """
⋮----
def copy(self, **kwargs)
⋮----
"""Returns a copy of the language model with possibly updated parameters.

        Any provided keyword arguments update the corresponding attributes or LM kwargs of
        the copy. For example, ``lm.copy(rollout_id=1, temperature=1.0)`` returns an LM whose
        requests use a different rollout ID at non-zero temperature to bypass cache collisions.
        """
⋮----
new_instance = copy.deepcopy(self)
⋮----
def inspect_history(self, n: int = 1)
⋮----
def update_history(self, entry)
⋮----
# Global LM history
⋮----
# dspy.LM.history
⋮----
# Per-module history
caller_modules = settings.caller_modules or []
⋮----
def _process_completion(self, response, merged_kwargs)
⋮----
"""Process the response of OpenAI chat completion API and extract outputs.

        Args:
            response: The OpenAI chat completion response
                https://platform.openai.com/docs/api-reference/chat/object
            merged_kwargs: Merged kwargs from self.kwargs and method kwargs

        Returns:
            List of processed outputs
        """
outputs = []
⋮----
output = {}
⋮----
# Extract citations from LiteLLM response if available
citations = self._extract_citations_from_response(c)
⋮----
# Return a list if every output only has "text" key
outputs = [output["text"] for output in outputs]
⋮----
def _extract_citations_from_response(self, choice)
⋮----
"""Extract citations from LiteLLM response if available.
        Reference: https://docs.litellm.ai/docs/providers/anthropic#beta-citations-api

        Args:
            choice: The choice object from response.choices

        Returns:
            A list of citation dictionaries or None if no citations found
        """
⋮----
# Check for citations in LiteLLM provider_specific_fields
citations_data = choice.message.provider_specific_fields.get("citations")
⋮----
def _process_response(self, response)
⋮----
"""Process the response of OpenAI Response API and extract outputs.

        Args:
            response: OpenAI Response API response
                https://platform.openai.com/docs/api-reference/responses/object

        Returns:
            List of processed outputs, which is always of size 1 because the Response API only supports one output.
        """
text_outputs = []
tool_calls = []
reasoning_contents = []
⋮----
output_item_type = output_item.type
⋮----
result = {}
⋮----
# All `response.output` items map to one answer, so we return a list of size 1.
⋮----
def inspect_history(n: int = 1)
⋮----
"""The global history shared across all LMs."""
</file>

<file path="dspy/clients/cache.py">
logger = logging.getLogger(__name__)
⋮----
class Cache
⋮----
"""DSPy Cache

    `Cache` provides 2 levels of caching (in the given order):
        1. In-memory cache - implemented with cachetools.LRUCache
        2. On-disk cache - implemented with diskcache.FanoutCache
    """
⋮----
"""
        Args:
            enable_disk_cache: Whether to enable on-disk cache.
            enable_memory_cache: Whether to enable in-memory cache.
            disk_cache_dir: The directory where the disk cache is stored.
            disk_size_limit_bytes: The maximum size of the disk cache (in bytes).
            memory_max_entries: The maximum size of the in-memory cache (in number of items).
        """
⋮----
def __contains__(self, key: str) -> bool
⋮----
"""Check if a key is in the cache."""
⋮----
def cache_key(self, request: dict[str, Any], ignored_args_for_cache_key: list[str] | None = None) -> str
⋮----
"""
        Obtain a unique cache key for the given request dictionary by hashing its JSON
        representation. For request fields having types that are known to be JSON-incompatible,
        convert them to a JSON-serializable format before hashing.
        """
⋮----
ignored_args_for_cache_key = ignored_args_for_cache_key or []
⋮----
def transform_value(value)
⋮----
# Try to get the source code of the callable if available
⋮----
# For regular functions, we can get the source code
⋮----
# For lambda functions or other callables where source isn't available,
# use a string representation
⋮----
params = {k: transform_value(v) for k, v in request.items() if k not in ignored_args_for_cache_key}
⋮----
def get(self, request: dict[str, Any], ignored_args_for_cache_key: list[str] | None = None) -> Any
⋮----
key = self.cache_key(request, ignored_args_for_cache_key)
⋮----
response = self.memory_cache[key]
⋮----
# Found on disk but not in memory cache, add to memory cache
response = self.disk_cache[key]
⋮----
response = copy.deepcopy(response)
⋮----
# Clear the usage data when cache is hit, because no LM call is made
⋮----
enable_memory_cache = self.enable_memory_cache and enable_memory_cache
⋮----
# Early return to avoid computing cache key if both memory and disk cache are disabled
⋮----
# Disk cache writing can fail for different reasons, e.g. disk full or the `value` is not picklable.
⋮----
def reset_memory_cache(self) -> None
⋮----
def save_memory_cache(self, filepath: str) -> None
⋮----
def load_memory_cache(self, filepath: str, allow_pickle: bool = False) -> None
⋮----
*,  # everything after this is keyword-only
maxsize: int | None = None,  # legacy / no-op
⋮----
"""
    Decorator for applying caching to a function based on the request argument.

    Args:
        cache_arg_name: The name of the argument that contains the request. If not provided, the entire kwargs is used
            as the request.
        ignored_args_for_cache_key: A list of arguments to ignore when computing the cache key from the request.
        enable_memory_cache: Whether to enable in-memory cache at call time. If False, the memory cache will not be
            written to on new data.
    """
ignored_args_for_cache_key = ignored_args_for_cache_key or ["api_key", "api_base", "base_url"]
# Deprecation notice
⋮----
def decorator(fn)
⋮----
@wraps(fn)
        def process_request(args, kwargs)
⋮----
# Use fully qualified function name for uniqueness
fn_identifier = f"{fn.__module__}.{fn.__qualname__}"
⋮----
# Create a modified request that includes the function identifier so that it's incorporated into the cache
# key. Deep copy is required because litellm sometimes modifies the kwargs in place.
⋮----
# When `cache_arg_name` is provided, use the value of the argument with this name as the request for
# caching.
modified_request = copy.deepcopy(kwargs[cache_arg_name])
⋮----
# When `cache_arg_name` is not provided, use the entire kwargs as the request for caching.
modified_request = copy.deepcopy(kwargs)
⋮----
@wraps(fn)
        def sync_wrapper(*args, **kwargs)
⋮----
cache = dspy.cache
modified_request = process_request(args, kwargs)
⋮----
# Retrieve from cache if available
cached_result = cache.get(modified_request, ignored_args_for_cache_key)
⋮----
# Otherwise, compute and store the result
# Make a copy of the original request in case it's modified in place, e.g., deleting some fields
original_request = copy.deepcopy(modified_request)
result = fn(*args, **kwargs)
# `enable_memory_cache` can be provided at call time to avoid indefinite growth.
⋮----
@wraps(fn)
        async def async_wrapper(*args, **kwargs)
⋮----
result = await fn(*args, **kwargs)
</file>

<file path="dspy/clients/databricks.py">
logger = logging.getLogger(__name__)
⋮----
class TrainingJobDatabricks(TrainingJob)
⋮----
def __init__(self, finetuning_run=None, *args, **kwargs)
⋮----
def status(self)
⋮----
run = fm.get(self.finetuning_run)
⋮----
class DatabricksProvider(Provider)
⋮----
finetunable = True
TrainingJob = TrainingJobDatabricks
⋮----
@staticmethod
    def is_provider_model(model: str) -> bool
⋮----
# We don't automatically infer Databricks models because Databricks is not a proprietary model provider.
⋮----
workspace_client = _get_workspace_client()
model_version = next(workspace_client.model_versions.list(model)).version
⋮----
# Allow users to override the host and token. This is useful on Databricks hosted runtime.
databricks_host = databricks_host or workspace_client.config.host
databricks_token = databricks_token or workspace_client.config.token
⋮----
headers = {"Context-Type": "text/json", "Authorization": f"Bearer {databricks_token}"}
⋮----
optimizable_info = requests.get(
⋮----
chunk_size = optimizable_info["throughput_chunk_size"]
⋮----
# Minimum desired provisioned throughput
min_provisioned_throughput = 0
⋮----
# Maximum desired provisioned throughput
max_provisioned_throughput = chunk_size
⋮----
# Databricks serving endpoint names cannot contain ".".
model_name = model.replace(".", "_")
⋮----
get_endpoint_response = requests.get(
⋮----
# The serving endpoint already exists, we will update it instead of creating a new one.
data = {
⋮----
response = requests.put(
⋮----
# Send the POST request to create the serving endpoint.
⋮----
response = requests.post(url=f"{databricks_host}/api/2.0/serving-endpoints", json=data, headers=headers)
⋮----
client = OpenAI(
# Wait for the deployment to be ready.
num_retries = deploy_timeout // 60
⋮----
train_data_format = TrainDataFormat.CHAT
⋮----
train_data_format = TrainDataFormat.COMPLETION
⋮----
# Add the file name to the directory path.
⋮----
databricks_host = train_kwargs.pop("databricks_host", None)
databricks_token = train_kwargs.pop("databricks_token", None)
⋮----
skip_deploy = train_kwargs.pop("skip_deploy", False)
deploy_timeout = train_kwargs.pop("deploy_timeout", 900)
⋮----
finetuning_run = fm.create(
⋮----
# Wait for the finetuning run to be ready.
⋮----
model_to_deploy = train_kwargs.get("register_to")
⋮----
# The finetuned model name should be in the format: "databricks/<endpoint_name>".
⋮----
@staticmethod
    def upload_data(train_data: list[dict[str, Any]], databricks_unity_catalog_path: str, data_format: TrainDataFormat)
⋮----
file_path = _save_data_to_local_file(train_data, data_format)
⋮----
w = _get_workspace_client()
⋮----
target_path = os.path.join(databricks_unity_catalog_path, os.path.basename(file_path))
⋮----
def _get_workspace_client() -> "WorkspaceClient"
⋮----
def _create_directory_in_databricks_unity_catalog(w: "WorkspaceClient", databricks_unity_catalog_path: str)
⋮----
pattern = r"^/Volumes/(?P<catalog>[^/]+)/(?P<schema>[^/]+)/(?P<volume>[^/]+)(/[^/]+)+$"
match = re.match(pattern, databricks_unity_catalog_path)
⋮----
catalog = match.group("catalog")
schema = match.group("schema")
volume = match.group("volume")
⋮----
volume_path = f"{catalog}.{schema}.{volume}"
⋮----
# Create the directory if it doesn't exist, we don't raise an error because this is a common case.
⋮----
def _save_data_to_local_file(train_data: list[dict[str, Any]], data_format: TrainDataFormat)
⋮----
file_name = f"finetuning_{uuid.uuid4()}.jsonl"
⋮----
finetune_dir = get_finetune_directory()
file_path = os.path.join(finetune_dir, file_name)
file_path = os.path.abspath(file_path)
⋮----
def _validate_chat_data(data: dict[str, Any])
⋮----
def _validate_completion_data(data: dict[str, Any])
</file>

<file path="dspy/clients/embedding.py">
class Embedder
⋮----
"""DSPy embedding class.

    The class for computing embeddings for text inputs. This class provides a unified interface for both:

    1. Hosted embedding models (e.g. OpenAI's text-embedding-3-small) via litellm integration
    2. Custom embedding functions that you provide

    For hosted models, simply pass the model name as a string (e.g., "openai/text-embedding-3-small"). The class will use
    litellm to handle the API calls and caching.

    For custom embedding models, pass a callable function that:
    - Takes a list of strings as input.
    - Returns embeddings as either:
        - A 2D numpy array of float32 values
        - A 2D list of float32 values
    - Each row should represent one embedding vector

    Args:
        model: The embedding model to use. This can be either a string (representing the name of the hosted embedding
            model, must be an embedding model supported by litellm) or a callable that represents a custom embedding
            model.
        batch_size (int, optional): The default batch size for processing inputs in batches. Defaults to 200.
        caching (bool, optional): Whether to cache the embedding response when using a hosted model. Defaults to True.
        **kwargs: Additional default keyword arguments to pass to the embedding model.

    Examples:
        Example 1: Using a hosted model.

        ```python
        import dspy

        embedder = dspy.Embedder("openai/text-embedding-3-small", batch_size=100)
        embeddings = embedder(["hello", "world"])

        assert embeddings.shape == (2, 1536)
        ```

        Example 2: Using any local embedding model, e.g. from https://huggingface.co/models?library=sentence-transformers.

        ```python
        # pip install sentence_transformers
        import dspy
        from sentence_transformers import SentenceTransformer

        # Load an extremely efficient local model for retrieval
        model = SentenceTransformer("sentence-transformers/static-retrieval-mrl-en-v1", device="cpu")

        embedder = dspy.Embedder(model.encode)
        embeddings = embedder(["hello", "world"], batch_size=1)

        assert embeddings.shape == (2, 1024)
        ```

        Example 3: Using a custom function.

        ```python
        import dspy
        import numpy as np

        def my_embedder(texts):
            return np.random.rand(len(texts), 10)

        embedder = dspy.Embedder(my_embedder)
        embeddings = embedder(["hello", "world"], batch_size=1)

        assert embeddings.shape == (2, 10)
        ```
    """
⋮----
def __init__(self, model: str | Callable, batch_size: int = 200, caching: bool = True, **kwargs: dict[str, Any])
⋮----
def _preprocess(self, inputs, batch_size=None, caching=None, **kwargs)
⋮----
is_single_input = True
inputs = [inputs]
⋮----
is_single_input = False
⋮----
batch_size = batch_size or self.batch_size
caching = caching or self.caching
merged_kwargs = self.default_kwargs.copy()
⋮----
input_batches = []
⋮----
def _postprocess(self, embeddings_list, is_single_input)
⋮----
embeddings = np.array(embeddings_list, dtype=np.float32)
⋮----
def __call__(self, inputs: str | list[str], batch_size: int | None = None, caching: bool | None = None, **kwargs: dict[str, Any]) -> np.ndarray
⋮----
"""Compute embeddings for the given inputs.

        Args:
            inputs: The inputs to compute embeddings for, can be a single string or a list of strings.
            batch_size (int, optional): The batch size for processing inputs. If None, defaults to the batch_size set
                during initialization.
            caching (bool, optional): Whether to cache the embedding response when using a hosted model. If None,
                defaults to the caching setting from initialization.
            kwargs: Additional keyword arguments to pass to the embedding model. These will override the default
                kwargs provided during initialization.

        Returns:
            numpy.ndarray: If the input is a single string, returns a 1D numpy array representing the embedding.
            If the input is a list of strings, returns a 2D numpy array of embeddings, one embedding per row.
        """
⋮----
compute_embeddings = _cached_compute_embeddings if caching else _compute_embeddings
⋮----
embeddings_list = []
⋮----
async def acall(self, inputs, batch_size=None, caching=None, **kwargs)
⋮----
acompute_embeddings = _cached_acompute_embeddings if caching else _acompute_embeddings
⋮----
def _compute_embeddings(model, batch_inputs, caching=False, **kwargs)
⋮----
caching = caching and litellm.cache is not None
embedding_response = litellm.embedding(model=model, input=batch_inputs, caching=caching, **kwargs)
⋮----
@request_cache(ignored_args_for_cache_key=["api_key", "api_base", "base_url"])
def _cached_compute_embeddings(model, batch_inputs, caching=True, **kwargs)
⋮----
async def _acompute_embeddings(model, batch_inputs, caching=False, **kwargs)
⋮----
embedding_response = await litellm.aembedding(model=model, input=batch_inputs, caching=caching, **kwargs)
⋮----
@request_cache(ignored_args_for_cache_key=["api_key", "api_base", "base_url"])
async def _cached_acompute_embeddings(model, batch_inputs, caching=True, **kwargs)
</file>

<file path="dspy/clients/lm_local.py">
logger = logging.getLogger(__name__)
⋮----
class LocalProvider(Provider)
⋮----
def __init__(self)
⋮----
@staticmethod
    def launch(lm: "LM", launch_kwargs: dict[str, Any] | None = None)
⋮----
import sglang  # noqa: F401
⋮----
launch_kwargs = launch_kwargs or lm.launch_kwargs
⋮----
model = lm.model
⋮----
model = model[7:]
⋮----
model = model[6:]
⋮----
model = model[len("huggingface/") :]
⋮----
port = get_free_port()
timeout = launch_kwargs.get("timeout", 1800)
command = [
⋮----
# We will manually stream & capture logs.
process = subprocess.Popen(
⋮----
stdout=subprocess.PIPE,  # We'll read from pipe
stderr=subprocess.STDOUT,  # Merge stderr into stdout
⋮----
# A threading.Event to control printing after the server is ready.
# This will store *all* lines (both before and after readiness).
⋮----
stop_printing_event = threading.Event()
logs_buffer = []
⋮----
def _tail_process(proc, buffer, stop_event)
⋮----
line = proc.stdout.readline()
⋮----
# Process ended and no new line
⋮----
# Print only if stop_event is not set
⋮----
# Start a background thread to read from the process continuously
thread = threading.Thread(
⋮----
# Wait until the server is ready (or times out)
base_url = f"http://localhost:{port}"
⋮----
# If the server doesn't come up, we might want to kill it:
⋮----
# Once server is ready, we tell the thread to stop printing further lines.
⋮----
# A convenience getter so the caller can see all logs so far (and future).
def get_logs() -> str
⋮----
# Join them all into a single string, or you might return a list
⋮----
# Let the user know server is up
⋮----
@staticmethod
    def kill(lm: "LM", launch_kwargs: dict[str, Any] | None = None)
⋮----
# Ideally, the following happens atomically
⋮----
data_path = save_data(train_data)
⋮----
output_dir = create_output_dir(model, data_path)
⋮----
default_train_kwargs = {
train_kwargs = {**default_train_kwargs, **(train_kwargs or {})}
output_dir = train_kwargs["output_dir"]  # user might have changed the output_dir
⋮----
def create_output_dir(model_name, data_path)
⋮----
model_str = model_name.replace("/", "-")
time_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
rnd_str = "".join(random.choices(string.ascii_lowercase + string.digits, k=6))
model_identifier = f"{rnd_str}_{model_str}_{time_str}"
output_dir = data_path.replace(".jsonl", "_" + model_identifier)
⋮----
def train_sft_locally(model_name, train_data, train_kwargs)
⋮----
device = train_kwargs.get("device", None)
⋮----
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
⋮----
model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(device)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)
⋮----
# Set up the chat format; generally only for non-chat model variants, hence the try-except.
⋮----
hf_dataset = Dataset.from_list(train_data)
⋮----
def tokenize_function(example)
⋮----
return encode_sft_example(example, tokenizer, train_kwargs["max_seq_length"])  # noqa: F821
⋮----
tokenized_dataset = hf_dataset.map(tokenize_function, batched=False)
⋮----
tokenized_dataset = tokenized_dataset.filter(lambda example: (example["labels"] != -100).any())
⋮----
use_peft = train_kwargs.get("use_peft", False)
peft_config = None
⋮----
rank_dimension = 32
lora_alpha = 64
lora_dropout = 0.05
⋮----
peft_config = LoraConfig(
⋮----
sft_config = SFTConfig(
⋮----
max_grad_norm=2.0,  # note that the current SFTConfig default is 1.0
⋮----
dataset_kwargs={  # We need to pass dataset_kwargs because we are processing the dataset ourselves
"add_special_tokens": False,  # Special tokens handled by template
"append_concat_token": False,  # No additional separator needed
⋮----
trainer = SFTTrainer(
⋮----
# Train!
⋮----
# Save the model!
⋮----
merge = True
⋮----
# Load PEFT model on CPU
model_ = AutoPeftModelForCausalLM.from_pretrained(
⋮----
merged_model = model_.merge_and_unload()
⋮----
# Clean up!
⋮----
def get_free_port() -> int
⋮----
"""
    Return a free TCP port on localhost.
    """
⋮----
def wait_for_server(base_url: str, timeout: int | None = None) -> None
⋮----
"""
    Wait for the server to be ready by polling the /v1/models endpoint.

    Args:
        base_url: The base URL of the server (e.g. http://localhost:1234)
        timeout: Maximum time to wait in seconds. None means wait forever.
    """
start_time = time.time()
⋮----
response = requests.get(
⋮----
# A small extra sleep to ensure server is fully up.
⋮----
# Server not up yet, wait and retry
⋮----
def encode_sft_example(example, tokenizer, max_seq_length)
⋮----
"""
    This function encodes a single example into a format that can be used for sft training.
    Here, we assume each example has a 'messages' field. Each message in it is a dict with 'role' and 'content' fields.
    We use the `apply_chat_template` function from the tokenizer to tokenize the messages and prepare the input and label tensors.

    Code obtained from the allenai/open-instruct repository: https://github.com/allenai/open-instruct/blob/4365dea3d1a6111e8b2712af06b22a4512a0df88/open_instruct/finetune.py
    """
⋮----
messages = example["messages"]
⋮----
input_ids = tokenizer.apply_chat_template(
labels = input_ids.clone()
# mask the non-assistant part for avoiding loss
⋮----
# we calculate the start index of this non-assistant message
⋮----
message_start_idx = 0
⋮----
message_start_idx = tokenizer.apply_chat_template(
⋮----
conversation=messages[:message_idx],  # here marks the end of the previous messages
⋮----
# next, we calculate the end index of this non-assistant message
⋮----
# for intermediate messages that follow with an assistant message, we need to
# set `add_generation_prompt=True` to avoid the assistant generation prefix being included in the loss
# (e.g., `<|assistant|>`)
message_end_idx = tokenizer.apply_chat_template(
⋮----
# for the last message or the message that doesn't follow with an assistant message,
# we don't need to add the assistant generation prefix
⋮----
# set the label to -100 for the non-assistant part
⋮----
attention_mask = torch.ones_like(input_ids)
</file>

<file path="dspy/clients/lm.py">
logger = logging.getLogger(__name__)
⋮----
class LM(BaseLM)
⋮----
"""
    A language model supporting chat or text completion requests for use with DSPy modules.
    """
⋮----
"""
        Create a new language model instance for use with DSPy modules and programs.

        Args:
            model: The model to use. This should be a string of the form ``"llm_provider/llm_name"``
                   supported by LiteLLM. For example, ``"openai/gpt-4o"``.
            model_type: The type of the model, either ``"chat"`` or ``"text"``.
            temperature: The sampling temperature to use when generating responses.
            max_tokens: The maximum number of tokens to generate per response.
            cache: Whether to cache the model responses for reuse to improve performance
                   and reduce costs.
            callbacks: A list of callback functions to run before and after each request.
            num_retries: The number of times to retry a request if it fails transiently due to
                         network error, rate limiting, etc. Requests are retried with exponential
                         backoff.
            provider: The provider to use. If not specified, the provider will be inferred from the model.
            finetuning_model: The model to finetune. In some providers, the models available for finetuning is different
                from the models available for inference.
            rollout_id: Optional integer used to differentiate cache entries for otherwise
                identical requests. Different values bypass DSPy's caches while still caching
                future calls with the same inputs and rollout ID. Note that `rollout_id`
                only affects generation when `temperature` is non-zero. This argument is
                stripped before sending requests to the provider.
        """
# Remember to update LM.copy() if you modify the constructor!
⋮----
# Handle model-specific configuration for different model families
model_family = model.split("/")[-1].lower() if "/" in model else model.lower()
⋮----
# Recognize OpenAI reasoning models (o1, o3, o4, gpt-5 family)
# Exclude non-reasoning variants like gpt-5-chat this is in azure ai foundry
# Allow date suffixes like -2023-01-01 after model name or mini/nano/pro
# For gpt-5, use negative lookahead to exclude -chat and allow other suffixes
model_pattern = re.match(
⋮----
def _warn_zero_temp_rollout(self, temperature: float | None, rollout_id)
⋮----
def _get_cached_completion_fn(self, completion_fn, cache)
⋮----
ignored_args_for_cache_key = ["api_key", "api_base", "base_url"]
⋮----
completion_fn = request_cache(
⋮----
litellm_cache_args = {"no-cache": True, "no-store": True}
⋮----
# Build the request.
kwargs = dict(kwargs)
cache = kwargs.pop("cache", self.cache)
⋮----
messages = messages or [{"role": "user", "content": prompt}]
⋮----
messages = [{**m, "role": "developer"} if m.get("role") == "system" else m for m in messages]
kwargs = {**self.kwargs, **kwargs}
⋮----
completion = litellm_completion
⋮----
completion = litellm_text_completion
⋮----
completion = litellm_responses_completion
⋮----
results = completion(
⋮----
completion = alitellm_completion
⋮----
completion = alitellm_text_completion
⋮----
completion = alitellm_responses_completion
⋮----
results = await completion(
⋮----
def launch(self, launch_kwargs: dict[str, Any] | None = None)
⋮----
def kill(self, launch_kwargs: dict[str, Any] | None = None)
⋮----
def thread_function_wrapper()
⋮----
thread = threading.Thread(target=thread_function_wrapper)
train_kwargs = train_kwargs or self.train_kwargs
model_to_finetune = self.finetuning_model or self.model
job = self.provider.TrainingJob(
⋮----
def reinforce(self, train_kwargs) -> ReinforceJob
⋮----
# TODO(GRPO Team): Should we return an initialized job here?
⋮----
err = f"Provider {self.provider} does not implement the reinforcement learning interface."
⋮----
job = self.provider.ReinforceJob(lm=self, train_kwargs=train_kwargs)
⋮----
def _run_finetune_job(self, job: TrainingJob)
⋮----
# TODO(enhance): We should listen for keyboard interrupts somewhere.
# Requires TrainingJob.cancel() to be implemented for each provider.
⋮----
model = self.provider.finetune(
lm = self.copy(model=model)
⋮----
def infer_provider(self) -> Provider
⋮----
def dump_state(self)
⋮----
state_keys = [
# Exclude api_key from kwargs to prevent API keys from being saved in plain text
filtered_kwargs = {k: v for k, v in self.kwargs.items() if k != "api_key"}
⋮----
def _check_truncation(self, results)
⋮----
stream = dspy.settings.send_stream
caller_predict = dspy.settings.caller_predict
⋮----
# The stream is already opened, and will be closed by the caller.
stream = cast(MemoryObjectSendStream, stream)
caller_predict_id = id(caller_predict) if caller_predict else None
⋮----
async def stream_completion(request: dict[str, Any], cache_kwargs: dict[str, Any])
⋮----
response = await litellm.acompletion(
chunks = []
⋮----
# Add the predict id to the chunk so that the stream listener can identify which predict produces it.
⋮----
def sync_stream_completion()
⋮----
syncified_stream_completion = syncify(stream_completion)
⋮----
async def async_stream_completion()
⋮----
def litellm_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None)
⋮----
cache = cache or {"no-cache": True, "no-store": True}
request = dict(request)
⋮----
headers = _add_dspy_identifier_to_headers(request.pop("headers", None))
stream_completion = _get_stream_completion_fn(request, cache, sync=True, headers=headers)
⋮----
def litellm_text_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None)
⋮----
headers = request.pop("headers", None)
# Extract the provider and model from the model string.
# TODO: Not all the models are in the format of "provider/model"
model = request.pop("model").split("/", 1)
⋮----
# Use the API key and base from the request, or from the environment.
api_key = request.pop("api_key", None) or os.getenv(f"{provider}_API_KEY")
api_base = request.pop("api_base", None) or os.getenv(f"{provider}_API_BASE")
⋮----
# Build the prompt from the messages.
prompt = "\n\n".join([x["content"] for x in request.pop("messages")] + ["BEGIN RESPONSE:"])
⋮----
async def alitellm_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None)
⋮----
stream_completion = _get_stream_completion_fn(request, cache, sync=False)
⋮----
async def alitellm_text_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None)
⋮----
def litellm_responses_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None)
⋮----
request = _convert_chat_request_to_responses_request(request)
⋮----
async def alitellm_responses_completion(request: dict[str, Any], num_retries: int, cache: dict[str, Any] | None = None)
⋮----
def _convert_chat_request_to_responses_request(request: dict[str, Any])
⋮----
"""
    Convert a chat request to a responses request
    See https://platform.openai.com/docs/api-reference/responses/create for the responses API specification.
    Also see https://platform.openai.com/docs/api-reference/chat/create for the chat API specification.
    """
⋮----
content_blocks = []
⋮----
c = msg.get("content")
⋮----
# Convert each content item from Chat API format to Responses API format
⋮----
# Convert `reasoning_effort` to reasoning format supported by the Responses API
⋮----
effort = request.pop("reasoning_effort")
⋮----
# Convert `response_format` to `text.format` for Responses API
⋮----
response_format = request.pop("response_format")
⋮----
response_format = {
text = request.pop("text", {})
⋮----
def _convert_content_item_to_responses_format(item: dict[str, Any]) -> dict[str, Any]
⋮----
"""
    Convert a content item from Chat API format to Responses API format.

    For images, converts from:
        {"type": "image_url", "image_url": {"url": "..."}}
    To:
        {"type": "input_image", "image_url": "..."}

    For text, converts from:
        {"type": "text", "text": "..."}
    To:
        {"type": "input_text", "text": "..."}

    For other types, passes through as-is.
    """
⋮----
image_url = item.get("image_url", {}).get("url", "")
⋮----
file = item.get("file", {})
⋮----
# For other items, return as-is
⋮----
def _add_dspy_identifier_to_headers(headers: dict[str, Any] | None = None)
⋮----
headers = headers or {}
</file>

<file path="dspy/clients/openai.py">
class TrainingJobOpenAI(TrainingJob)
⋮----
def __init__(self, *args, **kwargs)
⋮----
def cancel(self)
⋮----
# Cancel the provider job
⋮----
status = self.status()
⋮----
err_msg = "Jobs that are complete cannot be canceled."
⋮----
# Delete the provider file
⋮----
# Call the super's cancel method after the custom cancellation logic
⋮----
def status(self) -> TrainingStatus
⋮----
status = OpenAIProvider.get_training_status(self.provider_job_id)
⋮----
class OpenAIProvider(Provider)
⋮----
def __init__(self)
⋮----
@staticmethod
    def is_provider_model(model: str) -> bool
⋮----
# Althought it looks strange, `ft:` is a unique identifer for openai finetuned models in litellm context:
# https://github.com/BerriAI/litellm/blob/cd893134b7974d9f21477049a373b469fff747a5/litellm/utils.py#L4495
⋮----
@staticmethod
    def _remove_provider_prefix(model: str) -> str
⋮----
provider_prefix = "openai/"
⋮----
model = OpenAIProvider._remove_provider_prefix(model)
⋮----
data_path = save_data(train_data)
⋮----
provider_file_id = OpenAIProvider.upload_data(data_path)
⋮----
provider_job_id = OpenAIProvider._start_remote_training(
⋮----
# TODO(feature): Could we stream OAI logs?
⋮----
model = OpenAIProvider.get_trained_model(job)
⋮----
@staticmethod
    def does_job_exist(job_id: str) -> bool
⋮----
# TODO(nit): This call may fail for other reasons. We should check
# the error message to ensure that the job does not exist.
⋮----
@staticmethod
    def does_file_exist(file_id: str) -> bool
⋮----
# the error message to ensure that the file does not exist.
⋮----
@staticmethod
    def is_terminal_training_status(status: TrainingStatus) -> bool
⋮----
@staticmethod
    def get_training_status(job_id: str) -> TrainingStatus
⋮----
provider_status_to_training_status = {
⋮----
# Check if there is an active job
⋮----
err_msg = f"Job with ID {job_id} does not exist."
⋮----
# Retrieve the provider's job and report the status
provider_job = openai.fine_tuning.jobs.retrieve(job_id)
provider_status = provider_job.status
status = provider_status_to_training_status[provider_status]
⋮----
@staticmethod
    def validate_data_format(data_format: TrainDataFormat)
⋮----
supported_data_formats = [
⋮----
err_msg = f"OpenAI does not support the data format {data_format}."
⋮----
@staticmethod
    def upload_data(data_path: str) -> str
⋮----
# Upload the data to the provider
provider_file = openai.files.create(
⋮----
@staticmethod
    def _start_remote_training(train_file_id: str, model: str, train_kwargs: dict[str, Any] | None = None) -> str
⋮----
train_kwargs = train_kwargs or {}
provider_job = openai.fine_tuning.jobs.create(
⋮----
# Poll for the job until it is done
done = False
cur_event_id = None
reported_estimated_time = False
⋮----
# Report estimated time if not already reported
⋮----
remote_job = openai.fine_tuning.jobs.retrieve(job.provider_job_id)
timestamp = remote_job.estimated_finish
⋮----
estimated_finish_dt = datetime.fromtimestamp(timestamp)
delta_dt = estimated_finish_dt - datetime.now()
⋮----
reported_estimated_time = True
⋮----
# Get new events
page = openai.fine_tuning.jobs.list_events(fine_tuning_job_id=job.provider_job_id, limit=1)
new_event = page.data[0] if page.data else None
⋮----
dt = datetime.fromtimestamp(new_event.created_at)
⋮----
cur_event_id = new_event.id
⋮----
# Sleep and update the flag
⋮----
done = OpenAIProvider.is_terminal_training_status(job.status())
⋮----
@staticmethod
    def get_trained_model(job)
⋮----
status = job.status()
⋮----
err_msg = f"Job status is {status}."
⋮----
provider_job = openai.fine_tuning.jobs.retrieve(job.provider_job_id)
finetuned_model = provider_job.fine_tuned_model
</file>

<file path="dspy/clients/provider.py">
class TrainingJob(Future)
⋮----
# Subclasses should override the cancel method to cancel the job; then call
# the super's cancel method so that the future can be cancelled.
def cancel(self)
⋮----
@abstractmethod
    def status(self)
⋮----
class ReinforceJob
⋮----
def __init__(self, lm: "LM", train_kwargs: dict[str, Any] | None = None)
⋮----
@abstractmethod
    def initialize(self)
⋮----
@abstractmethod
    def step(self, train_data: list[dict[str, Any]], train_data_format: TrainDataFormat | str | None = None)
⋮----
@abstractmethod
    def terminate(self)
⋮----
@abstractmethod
    def save_checkpoint(self, checkpoint_name: str)
⋮----
def status(self)
⋮----
class Provider
⋮----
def __init__(self)
⋮----
@staticmethod
    def is_provider_model(model: str) -> bool
⋮----
# Subclasses should actually check whether a model is supported if they
# want to have the model provider auto-discovered.
⋮----
@staticmethod
    def launch(lm: "LM", launch_kwargs: dict[str, Any] | None = None)
⋮----
# Note that "launch" and "kill" methods might be called even if there
# is a launched LM or no launched LM to kill. These methods should be
# resillient to such cases.
⋮----
@staticmethod
    def kill(lm: "LM", launch_kwargs: dict[str, Any] | None = None)
⋮----
# We assume that LM.launch_kwargs dictionary will contain the necessary
# information for a provider to launch and/or kill an LM. This is the
# reeason why the argument here is named launch_kwargs and not
# kill_kwargs.
</file>

<file path="dspy/clients/utils_finetune.py">
class TrainingStatus(str, Enum)
⋮----
not_started = "not_started"
pending = "pending"
running = "running"
succeeded = "succeeded"
failed = "failed"
cancelled = "cancelled"
⋮----
class TrainDataFormat(str, Enum)
⋮----
CHAT = "chat"
COMPLETION = "completion"
GRPO_CHAT = "grpo_chat"
⋮----
class Message(TypedDict)
⋮----
role: Literal["user"] | Literal["assistant"] | Literal["system"]
content: str
⋮----
class MessageAssistant(TypedDict)
⋮----
role: Literal["assistant"]
⋮----
class GRPOChatData(TypedDict)
⋮----
messages: list[Message]
completion: MessageAssistant
reward: float
⋮----
class GRPOGroup(TypedDict)
⋮----
batch_id: int | None
group: list[GRPOChatData]
⋮----
class GRPOStatus(TypedDict)
⋮----
job_id: str
status: str | None = None
current_model: str
checkpoints: dict[str, str]
last_checkpoint: str | None = None
pending_batch_ids: list[int] = []
⋮----
def infer_data_format(adapter: Adapter) -> str
⋮----
def get_finetune_directory() -> str
⋮----
default_finetunedir = os.path.join(DSPY_CACHEDIR, "finetune")
finetune_dir = os.environ.get("DSPY_FINETUNEDIR") or default_finetunedir
finetune_dir = os.path.abspath(finetune_dir)
⋮----
def write_lines(file_path, data)
⋮----
# Assign a unique name to the file based on the data hash
hash = Hasher.hash(data)
file_name = f"{hash}.jsonl"
⋮----
finetune_dir = get_finetune_directory()
file_path = os.path.join(finetune_dir, file_name)
file_path = os.path.abspath(file_path)
⋮----
find_err_funcs = {
err = f"Data format {data_format} is not supported."
⋮----
find_err_func = find_err_funcs[data_format]
⋮----
err = f"Data is not a list. Found data type: {type(data)}"
⋮----
data_dict_errors = []
⋮----
err = f"Not a dictionary -- found data type: {type(data_dict)}"
⋮----
err = find_err_func(data_dict)
⋮----
err_dict = {"index": ind, "error": err}
⋮----
log_path = os.path.join(finetune_dir, "data_format_errors.log")
log_path = os.path.abspath(log_path)
⋮----
err = f"Data format errors found.  For more details, see the log file: {log_path}"
⋮----
def find_data_errors_completion(data_dict: dict[str, str]) -> str | None
⋮----
keys = ["prompt", "completion"]
⋮----
expected_keys = sorted(keys)
found_keys = sorted(data_dict.keys())
⋮----
# Following functions are modified from the OpenAI cookbook:
# https://cookbook.openai.com/examples/chat_finetuning_data_prep
def find_data_error_chat(messages: dict[str, Any]) -> str | None
⋮----
expected_keys = ["messages"]
found_keys = sorted(messages.keys())
⋮----
err = find_data_error_chat_message(message)
⋮----
def find_data_error_chat_message(message: dict[str, Any]) -> str | None
⋮----
message_keys = sorted(["role", "content"])
found_keys = sorted(message.keys())
⋮----
expected_roles = sorted(["assistant", "system", "user"])
found_role = message["role"]
</file>

<file path="dspy/datasets/alfworld/__init__.py">

</file>

<file path="dspy/datasets/alfworld/alfworld.py">
def env_worker(inq, outq)
⋮----
"""
    Worker process: creates a single AlfredTWEnv instance,
    handles 'init' (with task idx) and 'step' (with action).
    """
⋮----
buf = io.StringIO()
base_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.path.join(base_dir, "base_config.yml")
⋮----
config = yaml.safe_load(f)
⋮----
base_env = environment.AlfredTWEnv(config, train_eval="train")
⋮----
env = None
⋮----
env = base_env.init_env(batch_size=1)
⋮----
class EnvPool
⋮----
"""
    Pool of processes, each with a unique env_worker.
    Acquire a worker using a context manager for safe usage:
        with pool.session() as sess:
            sess.init(5)              # init with idx=5
            obs, rew, done, info = sess.step("go north")
            ...
    """
⋮----
def __init__(self, size=2)
⋮----
# Must call set_start_method('spawn') here, before creating any processes
⋮----
# If it's already set, ignore
⋮----
ctx = mp.get_context("spawn")
⋮----
inq = ctx.Queue()
outq = ctx.Queue()
p = ctx.Process(target=env_worker, args=(inq, outq), daemon=True)
⋮----
def _acquire(self)
⋮----
wid = self.available.get()
⋮----
def _release(self, wid)
⋮----
def close_all(self)
⋮----
"""Close all processes in the pool."""
⋮----
outq.get()  # Wait 'CLOSED'
⋮----
def session(self)
⋮----
"""Context manager that acquires/releases a single worker."""
⋮----
class _EnvSession
⋮----
"""
    A context manager that acquires a worker from the pool,
    provides .init(idx) and .step(action), then releases the worker.
    """
⋮----
def __init__(self, pool: EnvPool)
⋮----
def __enter__(self)
⋮----
def __exit__(self, exc_type, exc_val, exc_tb)
⋮----
def init(self, idx)
⋮----
return self.outq.get()  # (task_def, info)
⋮----
def step(self, action)
⋮----
return self.outq.get()  # (obs, rew, done, info)
⋮----
class AlfWorld
⋮----
def __init__(self, max_threads=20)
⋮----
dataset = [dspy.Example(idx=idx).with_inputs("idx") for idx in range(3500)]
⋮----
def __del__(self)
</file>

<file path="dspy/datasets/alfworld/base_config.yml">
dataset:
  data_path: '$ALFWORLD_DATA/json_2.1.1/train'
  eval_id_data_path: '$ALFWORLD_DATA/json_2.1.1/valid_seen'    # null/None to disable
  eval_ood_data_path: '$ALFWORLD_DATA/json_2.1.1/valid_unseen' # null/None to disable
  num_train_games: -1                                          # max training games (<=0 indicates full dataset)
  num_eval_games: -1                                           # max evaluation games (<=0 indicates full dataset)

logic:
  domain: '$ALFWORLD_DATA/logic/alfred.pddl'                   # PDDL domain file that defines the world dynamics
  grammar: '$ALFWORLD_DATA/logic/alfred.twl2'                  # Grammar file that defines the text feedbacks

env:
  type: 'AlfredTWEnv'                                          # 'AlfredTWEnv' or 'AlfredThorEnv' or 'AlfredHybrid'
  regen_game_files: False                                      # check if game is solvable by expert and save to game.tw-pddl file
  domain_randomization: False                                  # shuffle Textworld print order and object id nums
  task_types: [1, 2, 3, 4, 5, 6]                               # task-type ids: 1 - Pick & Place, 2 - Examine in Light, 3 - Clean & Place, 4 - Heat & Place, 5 - Cool & Place, 6 - Pick Two & Place
  expert_timeout_steps: 150                                    # max steps before timeout for expert to solve the task
  expert_type: "handcoded"                                     # 'handcoded' or 'downward'. Note: the downward planner is very slow for real-time use
  goal_desc_human_anns_prob: 0.0                               # prob of using human-annotated goal language instead of templated goals (1.0 indicates all human annotations from ALFRED)

  hybrid:
    start_eps: 100000                                          # starting episode of hybrid training, tw-only training upto this point
    thor_prob: 0.5                                             # prob of AlfredThorEnv during hybrid training
    eval_mode: "tw"                                            # 'tw' or 'thor' - env used for evaluation during hybrid training

  thor:
    screen_width: 300                                          # width of THOR window
    screen_height: 300                                         # height of THOR window
    smooth_nav: False                                          # smooth rotations, looks, and translations during navigation (very slow)
    save_frames_to_disk: False                                 # save frame PNGs to disk (useful for making videos)
    save_frames_path: './videos/'                              # path to save frame PNGs

controller:
  type: 'oracle'                                               # 'oracle' or 'oracle_astar' or 'mrcnn' or 'mrcnn_astar' (aka BUTLER)
  debug: False
  load_receps: True                                            # load receptacle locations from precomputed dict (if available)

mask_rcnn:
  pretrained_model_path: '$ALFWORLD_DATA/detectors/mrcnn.pth'

general:
  random_seed: 42
  use_cuda: True                                               # disable this when running on machine without cuda
  visdom: False                                                # plot training/eval curves, run with visdom server
  task: 'alfred'
  training_method: 'dagger'                                    # 'dqn' or 'dagger'
  save_path: './training/'                                     # path to save pytorch models
  observation_pool_capacity: 3                                 # k-size queue, 0 indicates no observation
  hide_init_receptacles: False                                 # remove initial observation containing navigable receptacles

  training:
    batch_size: 10
    max_episode: 50000
    smoothing_eps: 0.1
    optimizer:
      learning_rate: 0.001
      clip_grad_norm: 5

  evaluate:
    run_eval: True
    batch_size: 10
    env:
      type: "AlfredTWEnv"

  checkpoint:
    report_frequency: 1000                                    # report every N episode
    experiment_tag: 'test'                                    # name of experiment
    load_pretrained: False                                    # during test, enable this so that the agent load your pretrained model
    load_from_tag: 'not loading anything'                     # name of pre-trained model to load in save_path

  model:
    encoder_layers: 1
    decoder_layers: 1
    encoder_conv_num: 5
    block_hidden_dim: 64
    n_heads: 1
    dropout: 0.1
    block_dropout: 0.1
    recurrent: True

rl:
  action_space: "admissible"                                  # 'admissible' (candidates from text engine) or 'generation' (seq2seq-style generation) or 'beam_search_choice' or 'exhaustive' (not working)
  max_target_length: 20                                       # max token length for seq2seq generation
  beam_width: 10                                              # 1 means greedy
  generate_top_k: 3

  training:
    max_nb_steps_per_episode: 50                              # terminate after this many steps
    learn_start_from_this_episode: 0                          # delay updates until this episode
    target_net_update_frequency: 500                          # sync target net with online net per this many epochs

  replay:
    accumulate_reward_from_final: True
    count_reward_lambda: 0.0                                  # 0 to disable
    novel_object_reward_lambda: 0.0                           # 0 to disable
    discount_gamma_game_reward: 0.9
    discount_gamma_count_reward: 0.5
    discount_gamma_novel_object_reward: 0.5
    replay_memory_capacity: 500000                            # adjust this depending on your RAM size
    replay_memory_priority_fraction: 0.5
    update_per_k_game_steps: 5
    replay_batch_size: 64
    multi_step: 3
    replay_sample_history_length: 4
    replay_sample_update_from: 2

  epsilon_greedy:
    noisy_net: False                                          # if this is true, then epsilon greedy is disabled
    epsilon_anneal_episodes: 1000                             # -1 if not annealing
    epsilon_anneal_from: 0.3
    epsilon_anneal_to: 0.1

dagger:
  action_space: "generation"                                  # 'admissible' (candidates from text engine) or 'generation' (seq2seq-style generation) or 'exhaustive' (not working)
  max_target_length: 20                                       # max token length for seq2seq generation
  beam_width: 10                                              # 1 means greedy
  generate_top_k: 5
  unstick_by_beam_search: False                               # use beam-search for failed actions, set True during evaluation

  training:
    max_nb_steps_per_episode: 50                              # terminate after this many steps

  fraction_assist:
    fraction_assist_anneal_episodes: 50000
    fraction_assist_anneal_from: 1.0
    fraction_assist_anneal_to: 0.01

  fraction_random:
    fraction_random_anneal_episodes: 0
    fraction_random_anneal_from: 0.0
    fraction_random_anneal_to: 0.0

  replay:
    replay_memory_capacity: 500000
    update_per_k_game_steps: 5
    replay_batch_size: 64
    replay_sample_history_length: 4
    replay_sample_update_from: 2

vision_dagger:
  model_type: "resnet"                                        # 'resnet' (whole image features) or 'maskrcnn_whole' (whole image MaskRCNN feats) or 'maskrcnn' (top k MaskRCNN detection feats) or 'no_vision' (zero vision input)
  resnet_fc_dim: 64
  maskrcnn_top_k_boxes: 10                                    # top k box features
  use_exploration_frame_feats: False                          # append feats from initial exploration (memory intensive!)
  sequence_aggregation_method: "average"                      # 'sum' or 'average' or 'rnn'
</file>

<file path="dspy/datasets/__init__.py">
__all__ = [
</file>

<file path="dspy/datasets/colors.py">
### A bunch of colors, originally from matplotlib
all_colors = [
⋮----
class Colors(Dataset)
⋮----
def __init__(self, sort_by_suffix=True, *args, **kwargs) -> None
⋮----
colors = self.sorted_by_suffix(all_colors)
⋮----
train_size = int(
⋮----
)  # chosen to ensure that similar colors aren't repeated between train and dev
⋮----
def sorted_by_suffix(self, colors)
⋮----
sorted_colors = sorted(colors, key=lambda x: x[::-1])
⋮----
sorted_colors = sorted(colors, key=lambda x: x["color"][::-1])
</file>

<file path="dspy/datasets/dataloader.py">
class DataLoader(Dataset)
⋮----
def __init__(self)
⋮----
dataset = load_dataset(dataset_name, *args, **kwargs)
⋮----
dataset = {split_name: dataset[idx] for idx, split_name in enumerate(kwargs["split"])}
⋮----
returned_split = {}
⋮----
dataset = load_dataset("csv", data_files=file_path)["train"]
⋮----
fields = list(dataset.features)
⋮----
fields = list(df.columns)
⋮----
dataset = load_dataset("json", data_files=file_path)["train"]
⋮----
dataset = load_dataset("parquet", data_files=file_path)["train"]
⋮----
def from_rm(self, num_samples: int, fields: list[str], input_keys: list[str]) -> list[dspy.Example]
⋮----
rm = dspy.settings.rm
⋮----
dataset_shuffled = dataset.copy()
⋮----
train_end = int(len(dataset_shuffled) * train_size)
⋮----
train_end = train_size
⋮----
test_end = int(len(dataset_shuffled) * test_size)
⋮----
test_end = test_size
⋮----
test_end = len(dataset_shuffled) - train_end
⋮----
train_dataset = dataset_shuffled[:train_end]
test_dataset = dataset_shuffled[train_end : train_end + test_end]
</file>

<file path="dspy/datasets/dataset.py">
class Dataset
⋮----
@property
    def train(self) -> list[Example]
⋮----
@property
    def dev(self) -> list[Example]
⋮----
@property
    def test(self) -> list[Example]
⋮----
data_list = list(data)
⋮----
# Shuffle the data irrespective of the requested size.
base_rng = random.Random(seed)
⋮----
data_list = data_list[:size]
output: list[Example] = []
⋮----
example_obj = Example(**example, dspy_uuid=str(uuid.uuid4()), dspy_split=split)
⋮----
example_obj = example_obj.with_inputs(*self.input_keys)
⋮----
# TODO: NOTE: Ideally we use these uuids for dedup internally, for demos and internal train/val splits.
# Now, some tasks (like convQA and Colors) have overlapping examples. Here, we should allow the user to give us
# a uuid field that would respect this in some way. This means that we need a more refined concept that
# uuid (each example is unique) and more like a group_uuid.
⋮----
train_seeds = train_seeds or [1, 2, 3, 4, 5]
data_args = dotdict(train_size=train_size, eval_seed=eval_seed, dev_size=dev_size, test_size=0, **kwargs)
dataset = cls(**data_args)
⋮----
eval_set = dataset.dev
eval_sets: list[list[Example]] = []
train_sets: list[list[Example]] = []
⋮----
examples_per_seed = dev_size // len(train_seeds) if divide_eval_per_seed else dev_size
eval_offset = 0
</file>

<file path="dspy/datasets/gsm8k.py">
class GSM8K
⋮----
def __init__(self)
⋮----
dataset = load_dataset("gsm8k", "main")
⋮----
hf_official_train = dataset["train"]
hf_official_test = dataset["test"]
official_train = []
official_test = []
⋮----
question = example["question"]
⋮----
answer = example["answer"].strip().split()
⋮----
gold_reasoning = " ".join(answer[:-2])
answer = str(int(answer[-1].replace(",", "")))
⋮----
rng = random.Random(0)
⋮----
trainset = official_train[:200]
devset = official_train[200:500]
testset = official_test[:]
⋮----
trainset = [dspy.Example(**x).with_inputs("question") for x in trainset]
devset = [dspy.Example(**x).with_inputs("question") for x in devset]
testset = [dspy.Example(**x).with_inputs("question") for x in testset]
⋮----
def parse_integer_answer(answer, only_first_line=True)
⋮----
answer = answer.strip().split("\n")[0]
⋮----
# find the last token that has a number in it
answer = [token for token in answer.split() if any(c.isdigit() for c in token)][-1]
answer = answer.split(".")[0]
answer = "".join([c for c in answer if c.isdigit()])
answer = int(answer)
⋮----
answer = 0
⋮----
def gsm8k_metric(gold, pred, trace=None)
</file>

<file path="dspy/datasets/hotpotqa.py">
class HotPotQA(Dataset)
⋮----
hf_official_train = load_dataset("hotpot_qa", "fullwiki", split="train")
hf_official_dev = load_dataset("hotpot_qa", "fullwiki", split="validation")
⋮----
official_train = []
⋮----
keys = ["id", "question", "answer", "type", "supporting_facts", "context"]
⋮----
keys = ["question", "answer", "supporting_facts"]
⋮----
keys = ["question", "answer"]
⋮----
example = {k: raw_example[k] for k in keys}
⋮----
rng = random.Random(0)
⋮----
test = []
⋮----
example = {k: raw_example[k] for k in ["id", "question", "answer", "type", "supporting_facts"]}
⋮----
data_args = dotdict(train_seed=1, train_size=16, eval_seed=2023, dev_size=200 * 5, test_size=0)
dataset = HotPotQA(**data_args)
⋮----
"""
What was the population of the city where Woodward Avenue ends in 2010?
Where did the star , who is also an executive producer, of the Mick begin her carrer?
16 1000 0
Both London and German have seen attacks during war, there was one specific type of attack that Germany called the blitz, what did London call a similar attack?
Pre-Madonna was a collection of demos by the singer who was a leading presence during the emergence of what network?
Alan Mills composed the classic folk song that tells the story of what?
"""
</file>

<file path="dspy/datasets/math.py">
class MATH
⋮----
def __init__(self, subset)
⋮----
ds = load_dataset("DigitalLearningGmbH/MATH-lighteval", subset)
⋮----
# NOTE: Defaults to sub-splitting MATH's 'test' split into train/dev/test, presuming that current
# LMs are trained on MATH's train. Makes no difference for gpt-4o-mini, but might for other models.
⋮----
dataset = [
⋮----
size = min(350, len(dataset) // 3)
⋮----
def metric(self, example, pred, trace=None)
⋮----
def extract_answer(s)
⋮----
start = s.find("\\boxed{")
⋮----
idx = start + len("\\boxed{")
brace_level = 1
⋮----
answer = ""
⋮----
c = s[idx]
⋮----
answer = re.sub(r"\\text\{[^}]*\}", "", answer)
answer = re.sub(r"\\!", "", answer)
⋮----
"""
NOTE: MATH's official math_equivalence.is_equiv does not seem to have perfect recall.
Consider its behavior on reference values like `left[\frac{1}{2}, \frac{4}{3}\right]`.
"""
</file>

<file path="dspy/dsp/utils/__init__.py">

</file>

<file path="dspy/dsp/utils/dpr.py">
"""
Source: DPR Implementation from Facebook Research
https://github.com/facebookresearch/DPR/tree/master/dpr
Original license: https://github.com/facebookresearch/DPR/blob/main/LICENSE
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
class Tokens
⋮----
"""A class to represent a list of tokenized text."""
⋮----
TEXT = 0
TEXT_WS = 1
SPAN = 2
POS = 3
LEMMA = 4
NER = 5
⋮----
def __init__(self, data, annotators, opts=None)
⋮----
def __len__(self)
⋮----
"""The number of tokens."""
⋮----
def slice(self, i=None, j=None)
⋮----
"""Return a view of the list of tokens from [i, j)."""
new_tokens = copy.copy(self)
⋮----
def untokenize(self)
⋮----
"""Returns the original text (with whitespace reinserted)."""
⋮----
def words(self, uncased=False)
⋮----
"""Returns a list of the text of each token

        Args:
            uncased: lower cases text
        """
⋮----
def offsets(self)
⋮----
"""Returns a list of [start, end) character offsets of each token."""
⋮----
def pos(self)
⋮----
"""Returns a list of part-of-speech tags of each token.
        Returns None if this annotation was not included.
        """
⋮----
def lemmas(self)
⋮----
"""Returns a list of the lemmatized text of each token.
        Returns None if this annotation was not included.
        """
⋮----
def entities(self)
⋮----
"""Returns a list of named-entity-recognition tags of each token.
        Returns None if this annotation was not included.
        """
⋮----
def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True)
⋮----
"""Returns a list of all ngrams from length 1 to n.

        Args:
            n: upper limit of ngram length
            uncased: lower cases text
            filter_fn: user function that takes in an ngram list and returns
              True or False to keep or not keep the ngram
            as_string: return the ngram as a string vs list
        """
⋮----
def _skip(gram)
⋮----
words = self.words(uncased)
ngrams = [
⋮----
# Concatenate into strings
⋮----
ngrams = ["{}".format(" ".join(words[s:e])) for (s, e) in ngrams]
⋮----
def entity_groups(self)
⋮----
"""Group consecutive entity tokens with the same NER tag."""
entities = self.entities()
⋮----
non_ent = self.opts.get("non_ent", "O")
groups = []
idx = 0
⋮----
ner_tag = entities[idx]
# Check for entity tag
⋮----
# Chomp the sequence
start = idx
⋮----
class Tokenizer
⋮----
"""Base tokenizer class.
    Tokenizers implement tokenize, which should return a Tokens class.
    """
⋮----
def tokenize(self, text)
⋮----
def shutdown(self)
⋮----
def __del__(self)
⋮----
class SimpleTokenizer(Tokenizer)
⋮----
ALPHA_NUM = r"[\p{L}\p{N}\p{M}]+"
NON_WS = r"[^\p{Z}\p{C}]"
⋮----
def __init__(self, **kwargs)
⋮----
"""
        Args:
            annotators: None or empty set (only tokenizes).
        """
⋮----
data = []
matches = list(self._regexp.finditer(text))
⋮----
# Get text
token = matches[i].group()
⋮----
# Get whitespace
span = matches[i].span()
start_ws = span[0]
⋮----
end_ws = matches[i + 1].span()[0]
⋮----
end_ws = span[1]
⋮----
# Format data
⋮----
def has_answer(tokenized_answers, text)
⋮----
text = DPR_normalize(text)
⋮----
def locate_answers(tokenized_answers, text)
⋮----
"""
    Returns each occurrence of an answer as (offset, endpos) in terms of *characters*.
    """
tokenized_text = DPR_tokenize(text)
occurrences = []
⋮----
answers_words = [ans.words(uncased=True) for ans in tokenized_answers]
⋮----
STokenizer = SimpleTokenizer()
⋮----
def DPR_tokenize(text):  # noqa: N802
⋮----
def DPR_normalize(text):  # noqa: N802
⋮----
# Source: https://github.com/shmsw25/qa-hard-em/blob/master/prepro_util.py
def strip_accents(text)
⋮----
"""Strips accents from a piece of text."""
text = unicodedata.normalize("NFD", text)
output = []
⋮----
cat = unicodedata.category(char)
</file>

<file path="dspy/dsp/utils/settings.py">
logger = logging.getLogger(__name__)
⋮----
DEFAULT_CONFIG = dotdict(
⋮----
provide_traceback=False,  # Whether to include traceback information in error logs.
num_threads=8,  # Number of threads to use for parallel processing.
max_errors=10,  # Maximum errors before halting operations.
# If true, async tools can be called in sync mode by getting converted to sync.
⋮----
# Global base configuration and owner tracking
main_thread_config = copy.deepcopy(DEFAULT_CONFIG)
config_owner_thread_id = None
config_owner_async_task = None
⋮----
# Global lock for settings configuration
global_lock = threading.Lock()
⋮----
thread_local_overrides = contextvars.ContextVar("context_overrides", default=dotdict())
⋮----
class Settings
⋮----
"""
    A singleton class for DSPy configuration settings.
    Thread-safe global configuration.
    - 'configure' can be called by only one 'owner' thread (the first thread that calls it).
    - Other threads see the configured global values from 'main_thread_config'.
    - 'context' sets thread-local overrides. These overrides propagate to threads spawned
      inside that context block, when (and only when!) using a ParallelExecutor that copies overrides.

      1. Only one unique thread (which can be any thread!) can call dspy.configure.
      2. It affects a global state, visible to all. As a result, user threads work, but they shouldn't be
         mixed with concurrent changes to dspy.configure from the "main" thread.
         (TODO: In the future, add warnings: if there are near-in-time user-thread reads followed by .configure calls.)
      3. Any thread can use dspy.context. It propagates to child threads created with DSPy primitives: Parallel, asyncify, etc.
    """
⋮----
_instance = None
⋮----
def __new__(cls)
⋮----
@property
    def lock(self)
⋮----
def __getattr__(self, name)
⋮----
overrides = thread_local_overrides.get()
⋮----
def __setattr__(self, name, value)
⋮----
def __getitem__(self, key)
⋮----
def __setitem__(self, key, value)
⋮----
def __contains__(self, key)
⋮----
def get(self, key, default=None)
⋮----
def copy(self)
⋮----
@property
    def config(self)
⋮----
def _ensure_configure_allowed(self)
⋮----
current_thread_id = threading.get_ident()
⋮----
# First `configure` call assigns the owner thread id.
config_owner_thread_id = current_thread_id
⋮----
# Disallow a second `configure` calls from other threads.
⋮----
# Async task doesn't allow a second `configure` call, must use dspy.context(...) instead.
is_async_task = False
⋮----
is_async_task = True
⋮----
# This exception (e.g., "no current task") means we are not in an async loop/task,
# or asyncio module itself is not fully functional in this specific sub-thread context.
⋮----
# First `configure` call assigns the owner async task.
config_owner_async_task = asyncio.current_task()
⋮----
# We are in an async task. Now check for IPython and allow calling `configure` from IPython.
in_ipython = False
⋮----
# get_ipython is a global injected by IPython environments.
# We check its existence and type to be more robust.
in_ipython = get_ipython() is not None
⋮----
# If `IPython` is not installed or `get_ipython` failed, we are not in an IPython environment.
⋮----
def configure(self, **kwargs)
⋮----
# If no exception is raised, the `configure` call is allowed.
⋮----
# Update global config
⋮----
@contextmanager
    def context(self, **kwargs)
⋮----
"""
        Context manager for temporary configuration changes at the thread level.
        Does not affect global configuration. Changes only apply to the current thread.
        If threads are spawned inside this block using ParallelExecutor, they will inherit these overrides.
        """
⋮----
original_overrides = thread_local_overrides.get().copy()
new_overrides = dotdict({**main_thread_config, **original_overrides, **kwargs})
token = thread_local_overrides.set(new_overrides)
⋮----
def __repr__(self)
⋮----
combined_config = {**main_thread_config, **overrides}
⋮----
"""
        Save the settings to a file using cloudpickle.

        Args:
            path: The file path to save the settings to.
            modules_to_serialize (list or None): A list of modules to serialize with cloudpickle's `register_pickle_by_value`.
                If None, then no modules will be registered for serialization.
            exclude_keys (list or None): A list of keys to exclude during saving.
        """
⋮----
modules_to_serialize = modules_to_serialize or []
⋮----
exclude_keys = exclude_keys or []
data = {key: value for key, value in self.config.items() if key not in exclude_keys}
⋮----
@classmethod
    def load(cls, path: str) -> dict[str, Any]
⋮----
"""
        Load the settings from a file using cloudpickle.

        Args:
            path: The file path to load the settings from.

        Returns:
            A dict that stores the loaded settings.
        """
⋮----
configs = cloudpickle.load(f)
⋮----
settings = Settings()
</file>

<file path="dspy/dsp/utils/utils.py">
def print_message(*s, condition=True, pad=False, sep=None)
⋮----
s = " ".join([str(x) for x in s])
msg = "[{}] {}".format(datetime.datetime.now().strftime("%b %d, %H:%M:%S"), s)
⋮----
msg = msg if not pad else f"\n{msg}\n"
⋮----
def timestamp(daydir=False)
⋮----
format_str = f"%Y-%m{'/' if daydir else '-'}%d{'/' if daydir else '_'}%H.%M.%S"
result = datetime.datetime.now().strftime(format_str)
⋮----
def file_tqdm(file)
⋮----
def create_directory(path)
⋮----
def deduplicate(seq: list[str]) -> list[str]
⋮----
"""
        From Raymond Hettinger
        https://twitter.com/raymondh/status/944125570534621185
        Since Python 3.6 Dict are ordered
        Benchmark: https://gist.github.com/peterbe/67b9e40af60a1d5bcb1cfb4b2937b088
    """
⋮----
def batch(group, bsize, provide_offset=False)
⋮----
offset = 0
⋮----
batch_data = group[offset : offset + bsize]
⋮----
class dotdict(dict):  # noqa: N801
⋮----
def __getattr__(self, key)
⋮----
def __setattr__(self, key, value)
⋮----
def __delattr__(self, key)
⋮----
def __deepcopy__(self, memo)
⋮----
# Use the default dict copying method to avoid infinite recursion.
⋮----
class dotdict_lax(dict):  # noqa: N801
⋮----
__getattr__ = dict.get
__setattr__ = dict.__setitem__
__delattr__ = dict.__delitem__
⋮----
def flatten(data_list)
⋮----
result = []
⋮----
def zipstar(data_list, lazy=False)
⋮----
"""
    A much faster A, B, C = zip(*[(a, b, c), (a, b, c), ...])
    May return lists or tuples.
    """
⋮----
width = len(data_list[0])
⋮----
zipped_data = zip(*data_list, strict=False)
⋮----
def zip_first(list1, list2)
⋮----
length = len(list1) if type(list1) in [tuple, list] else None
⋮----
zipped_data = list(zip(list1, list2, strict=False))
⋮----
def int_or_float(val)
⋮----
def groupby_first_item(lst)
⋮----
groups = defaultdict(list)
⋮----
rest = rest[0] if len(rest) == 1 else rest
⋮----
def process_grouped_by_first_item(lst)
⋮----
"""
    Requires items in list to already be grouped by first item.
    """
⋮----
started = False
last_group = None
⋮----
last_group = first
started = True
⋮----
def grouper(iterable, n, fillvalue=None)
⋮----
"""
    Collect data into fixed-length chunks or blocks
        Example: grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx"
        Source: https://docs.python.org/3/library/itertools.html#itertools-recipes
    """
⋮----
args = [iter(iterable)] * n
⋮----
def lengths2offsets(lengths)
⋮----
# see https://stackoverflow.com/a/45187287
class NullContextManager
⋮----
def __init__(self, dummy_resource=None)
⋮----
def __enter__(self)
⋮----
def __exit__(self, *args)
⋮----
def load_batch_backgrounds(args, qids)
⋮----
qbackgrounds = []
⋮----
back = args.qid2backgrounds[qid]
⋮----
x = [args.collection[pid] for pid in back]
⋮----
x = [args.collectionX.get(pid, "") for pid in back]
⋮----
x = " [SEP] ".join(x)
</file>

<file path="dspy/dsp/__init__.py">

</file>

<file path="dspy/dsp/colbertv2.py">
# TODO: Ideally, this takes the name of the index and looks up its port.
⋮----
class ColBERTv2
⋮----
"""Wrapper for the ColBERTv2 Retrieval."""
⋮----
topk: list[dict[str, Any]] = colbertv2_post_request(self.url, query, k)
⋮----
topk: list[dict[str, Any]] = colbertv2_get_request(self.url, query, k)
⋮----
@request_cache()
def colbertv2_get_request_v2(url: str, query: str, k: int)
⋮----
payload = {"query": query, "k": k}
res = requests.get(url, params=payload, timeout=10)
⋮----
res_json = res.json()
⋮----
error_message = res_json.get("message", "Unknown error")
⋮----
topk = res_json["topk"][:k]
topk = [{**d, "long_text": d["text"]} for d in topk]
⋮----
@request_cache()
def colbertv2_get_request_v2_wrapped(*args, **kwargs)
⋮----
colbertv2_get_request = colbertv2_get_request_v2_wrapped
⋮----
@request_cache()
def colbertv2_post_request_v2(url: str, query: str, k: int)
⋮----
headers = {"Content-Type": "application/json; charset=utf-8"}
⋮----
res = requests.post(url, json=payload, headers=headers, timeout=10)
⋮----
@request_cache()
def colbertv2_post_request_v2_wrapped(*args, **kwargs)
⋮----
colbertv2_post_request = colbertv2_post_request_v2_wrapped
⋮----
class ColBERTv2RetrieverLocal
⋮----
def __init__(self, passages: list[str], colbert_config=None, load_only: bool = False)
⋮----
"""Colbertv2 retriever module

        Args:
            passages (list[str]): list of passages
            colbert_config (ColBERTConfig, optional): colbert config for building and searching. Defaults to None.
            load_only (bool, optional): whether to load the index or build and then load. Defaults to False.
        """
⋮----
def build_index(self)
⋮----
import colbert  # noqa: F401
⋮----
indexer = Indexer(checkpoint=self.colbert_config.checkpoint, config=self.colbert_config)
⋮----
def get_index(self)
⋮----
searcher = Searcher(index=self.colbert_config.index_name, collection=self.passages)
⋮----
def __call__(self, *args: Any, **kwargs: Any) -> Any
⋮----
def forward(self, query: str, k: int = 7, **kwargs)
⋮----
filtered_pids = kwargs.get("filtered_pids")
⋮----
device = "cuda" if torch.cuda.is_available() else "cpu"
results = self.searcher.search(
⋮----
# Number of passages to receive
⋮----
# Passing the filter function of relevant
⋮----
searcher_results = self.searcher.search(query, k=k)
results = []
for pid, rank, score in zip(*searcher_results, strict=False):  # noqa: B007
⋮----
class ColBERTv2RerankerLocal
⋮----
def __init__(self, colbert_config=None, checkpoint: str = "bert-base-uncased")
⋮----
"""_summary_

        Args:
            colbert_config (ColBERTConfig, optional): Colbert config. Defaults to None.
            checkpoint_name (str, optional): checkpoint for embeddings. Defaults to 'bert-base-uncased'.
        """
⋮----
def forward(self, query: str, passages: list[str] | None = None)
⋮----
passages = passages or []
⋮----
query_tokenizer = QueryTokenizer(self.colbert_config, verbose=1)
doc_tokenizer = DocTokenizer(self.colbert_config)
⋮----
col = ColBERT(self.checkpoint, self.colbert_config)
q = col.query(query_ids, query_masks)
⋮----
q_duplicated = q.repeat_interleave(len(passages), dim=0).contiguous()
tensor_scores = col.score(q_duplicated, doc_ids, doc_masks)
passage_score_arr = np.array([score.cpu().detach().numpy().tolist() for score in tensor_scores])
</file>

<file path="dspy/evaluate/__init__.py">
__all__ = [
</file>

<file path="dspy/evaluate/auto_evaluation.py">
class SemanticRecallPrecision(Signature)
⋮----
"""
    Compare a system's response to the ground truth to compute its recall and precision.
    If asked to reason, enumerate key ideas in each response, and whether they are present in the other response.
    """
⋮----
question: str = InputField()
ground_truth: str = InputField()
system_response: str = InputField()
recall: float = OutputField(desc="fraction (out of 1.0) of ground truth covered by the system response")
precision: float = OutputField(desc="fraction (out of 1.0) of system response covered by the ground truth")
⋮----
class DecompositionalSemanticRecallPrecision(Signature)
⋮----
"""
    Compare a system's response to the ground truth to compute recall and precision of key ideas.
    You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.
    """
⋮----
ground_truth_key_ideas: str = OutputField(desc="enumeration of key ideas in the ground truth")
system_response_key_ideas: str = OutputField(desc="enumeration of key ideas in the system response")
discussion: str = OutputField(desc="discussion of the overlap between ground truth and system response")
⋮----
def f1_score(precision, recall)
⋮----
class SemanticF1(Module)
⋮----
def __init__(self, threshold=0.66, decompositional=False)
⋮----
def forward(self, example, pred, trace=None)
⋮----
scores = self.module(question=example.question, ground_truth=example.response, system_response=pred.response)
score = f1_score(scores.precision, scores.recall)
⋮----
###########
⋮----
class AnswerCompleteness(Signature)
⋮----
"""
    Estimate the completeness of a system's responses, against the ground truth.
    You will first enumerate key ideas in each response, discuss their overlap, and then report completeness.
    """
⋮----
completeness: float = OutputField(desc="fraction (out of 1.0) of ground truth covered by the system response")
⋮----
class AnswerGroundedness(Signature)
⋮----
"""
    Estimate the groundedness of a system's responses, against real retrieved documents written by people.
    You will first enumerate whatever non-trivial or check-worthy claims are made in the system response, and then
    discuss the extent to which some or all of them can be deduced from the retrieved context and basic commonsense.
    """
⋮----
retrieved_context: str = InputField()
⋮----
system_response_claims: str = OutputField(desc="enumeration of non-trivial or check-worthy claims in the system response")
discussion: str = OutputField(desc="discussion of how supported the claims are by the retrieved context")
groundedness: float = OutputField(desc="fraction (out of 1.0) of system response supported by the retrieved context")
⋮----
class CompleteAndGrounded(Module)
⋮----
def __init__(self, threshold=0.66)
⋮----
completeness = self.completeness_module(question=example.question, ground_truth=example.response, system_response=pred.response)
groundedness = self.groundedness_module(question=example.question, retrieved_context=pred.context, system_response=pred.response)
score = f1_score(groundedness.groundedness, completeness.completeness)
</file>

<file path="dspy/evaluate/evaluate.py">
def display(obj: Any)
⋮----
"""
        Display the specified Python object in the console.

        :param obj: The Python object to display.
        """
⋮----
def HTML(x: str) -> str:  # noqa: N802
⋮----
"""
        Obtain the HTML representation of the specified string.
        """
# NB: This method exists purely for code compatibility with the IPython HTML() function in
# environments where IPython is not available. In such environments where IPython is not
# available, this method will simply return the input string.
⋮----
# TODO: Counting failures and having a max_failure count. When that is exceeded (also just at the end),
# we print the number of failures, the first N examples that failed, and the first N exceptions raised.
⋮----
logger = logging.getLogger(__name__)
⋮----
class EvaluationResult(Prediction)
⋮----
"""
    A class that represents the result of an evaluation.
    It is a subclass of `dspy.Prediction` that contains the following fields

    - score: An float value (e.g., 67.30) representing the overall performance
    - results: a list of (example, prediction, score) tuples for each example in devset
    """
⋮----
def __init__(self, score: float, results: list[tuple["dspy.Example", "dspy.Example", Any]])
⋮----
def __repr__(self)
⋮----
class Evaluate
⋮----
"""DSPy Evaluate class.

    This class is used to evaluate the performance of a DSPy program. Users need to provide a evaluation dataset and
    a metric function in order to use this class. This class supports parallel evaluation on the provided dataset.
    """
⋮----
"""
        Args:
            devset (list[dspy.Example]): the evaluation dataset.
            metric (Callable): The metric function to use for evaluation.
            num_threads (Optional[int]): The number of threads to use for parallel evaluation.
            display_progress (bool): Whether to display progress during evaluation.
            display_table (Union[bool, int]): Whether to display the evaluation results in a table.
                If a number is passed, the evaluation results will be truncated to that number before displayed.
            max_errors (Optional[int]): The maximum number of errors to allow before
                stopping evaluation. If ``None``, inherits from ``dspy.settings.max_errors``.
            provide_traceback (Optional[bool]): Whether to provide traceback information during evaluation.
            failure_score (float): The default score to use if evaluation fails due to an exception.
            save_as_csv (Optional[str]): The file name where the csv will be saved.
            save_as_json (Optional[str]): The file name where the json will be saved.

        """
⋮----
"""
        Args:
            program (dspy.Module): The DSPy program to evaluate.
            metric (Callable): The metric function to use for evaluation. if not provided, use `self.metric`.
            devset (list[dspy.Example]): the evaluation dataset. if not provided, use `self.devset`.
            num_threads (Optional[int]): The number of threads to use for parallel evaluation. if not provided, use
                `self.num_threads`.
            display_progress (bool): Whether to display progress during evaluation. if not provided, use
                `self.display_progress`.
            display_table (Union[bool, int]): Whether to display the evaluation results in a table. if not provided, use
                `self.display_table`. If a number is passed, the evaluation results will be truncated to that number before displayed.
            callback_metadata (dict): Metadata to be used for evaluate callback handlers.

        Returns:
            The evaluation results are returned as a dspy.EvaluationResult object containing the following attributes:

            - score: A float percentage score (e.g., 67.30) representing overall performance

            - results: a list of (example, prediction, score) tuples for each example in devset
        """
metric = metric if metric is not None else self.metric
devset = devset if devset is not None else self.devset
num_threads = num_threads if num_threads is not None else self.num_threads
display_progress = display_progress if display_progress is not None else self.display_progress
display_table = display_table if display_table is not None else self.display_table
save_as_csv = save_as_csv if save_as_csv is not None else self.save_as_csv
save_as_json = save_as_json if save_as_json is not None else self.save_as_json
⋮----
executor = ParallelExecutor(
⋮----
def process_item(example)
⋮----
prediction = program(**example.inputs())
score = metric(example, prediction)
⋮----
results = executor.execute(process_item, devset)
⋮----
results = [((dspy.Prediction(), self.failure_score) if r is None else r) for r in results]
results = [(example, prediction, score) for example, (prediction, score) in zip(devset, results, strict=False)]
⋮----
# Rename the 'correct' column to the name of the metric object
metric_name = metric.__name__ if isinstance(metric, types.FunctionType) else metric.__class__.__name__
# Construct a pandas DataFrame from the results
result_df = self._construct_result_table(results, metric_name)
⋮----
metric_name = (
data = self._prepare_results_output(results, metric_name)
⋮----
fieldnames = data[0].keys()
writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
⋮----
"""
        Construct a pandas DataFrame from the specified result list.
        Let's not try to change the name of this method as it may be patched by external tracing tools.

        Args:
            results: The list of results to construct the result DataFrame from.
            metric_name: The name of the metric used for evaluation.

        Returns:
            The constructed pandas DataFrame.
        """
⋮----
# Truncate every cell in the DataFrame (DataFrame.applymap was renamed to DataFrame.map in Pandas 2.1.0)
result_df = pd.DataFrame(data)
result_df = result_df.map(truncate_cell) if hasattr(result_df, "map") else result_df.applymap(truncate_cell)
⋮----
def _display_result_table(self, result_df: "pd.DataFrame", display_table: bool | int, metric_name: str)
⋮----
"""
        Display the specified result DataFrame in a table format.

        Args:
            result_df: The result DataFrame to display.
            display_table: Whether to display the evaluation results in a table.
                If a number is passed, the evaluation results will be truncated to that number before displayed.
            metric_name: The name of the metric used for evaluation.
        """
⋮----
df_to_display = result_df.copy()
truncated_rows = 0
⋮----
df_to_display = result_df.head(display_table).copy()
truncated_rows = len(result_df) - display_table
⋮----
df_to_display = stylize_metric_name(df_to_display, metric_name)
⋮----
# Simplified message about the truncated rows
message = f"""
⋮----
def prediction_is_dictlike(prediction)
⋮----
# Downstream logic for displaying dictionary-like predictions depends solely on the predictions
# having a method called `items()` for iterating through key/value pairs
⋮----
def merge_dicts(d1, d2) -> dict
⋮----
# Convert to dict if objects have toDict method (e.g., Example objects)
⋮----
d1 = d1.toDict()
⋮----
d2 = d2.toDict()
⋮----
merged = {}
⋮----
def truncate_cell(content) -> str
⋮----
"""Truncate content of a cell to 25 words."""
words = str(content).split()
⋮----
def stylize_metric_name(df: "pd.DataFrame", metric_name: str) -> "pd.DataFrame"
⋮----
"""
    Stylize the cell contents of a pandas DataFrame corresponding to the specified metric name.

    :param df: The pandas DataFrame for which to stylize cell contents.
    :param metric_name: The name of the metric for which to stylize DataFrame cell contents.
    """
def format_metric(x)
⋮----
def display_dataframe(df: "pd.DataFrame")
⋮----
"""
    Display the specified Pandas DataFrame in the console.

    :param df: The Pandas DataFrame to display.
    """
⋮----
# Pretty print the DataFrame to the console
⋮----
):  # more options can be specified also
⋮----
def configure_dataframe_for_ipython_notebook_display(df: "pd.DataFrame") -> "pd.DataFrame"
⋮----
"""Set various pandas display options for DataFrame in an IPython notebook environment."""
⋮----
def is_in_ipython_notebook_environment()
⋮----
"""
    Check if the current environment is an IPython notebook environment.

    :return: True if the current environment is an IPython notebook environment, False otherwise.
    """
⋮----
# This is a best-effort check to see if we are in an IPython notebook environment
⋮----
# FIXME: TODO: The merge_dicts stuff above is way too quick and dirty.
# TODO: the display_table can't handle False but can handle 0!
# Not sure how it works with True exactly, probably fails too.
</file>

<file path="dspy/evaluate/metrics.py">
# TODO: This should move internally. Same for passage_match. dspy.metrics.answer_exact_match, dspy.metrics.answer_passage_match
⋮----
def EM(prediction, answers_list):  # noqa: N802
⋮----
"""Compute the Exact Match (EM) metric between a prediction and reference answers.

    Returns True if any reference exactly matches the prediction after normalization;
    otherwise False. Normalization applies Unicode NFD, lowercasing, punctuation
    removal, English article removal ("a", "an", "the"), and whitespace collapse.

    Args:
        prediction (str): Predicted answer string.
        answers_list (list[str]): List of reference answers.

    Returns:
        bool: Whether any reference exactly equals the prediction after normalization.

    Example:
        ```python
        EM("The Eiffel Tower", ["Eiffel Tower", "Louvre"])  # True

        EM("paris", ["Paris"])  # True
        EM("paris", ["Paris, France"])  # False
        ```
    """
⋮----
def F1(prediction, answers_list):  # noqa: N802
⋮----
"""Compute the maximum token-level F1 score against reference answers.

    Strings are normalized (same as in `EM`) and whitespace-tokenized. The function
    returns the maximum F1 over all provided references.

    Args:
        prediction (str): Predicted answer string.
        answers_list (list[str]): List of reference answers.

    Returns:
        float: Highest F1 score in [0.0, 1.0].

    Example:
        ```python
        round(F1("Eiffel Tower is in Paris", ["Paris"]), 2)  # 0.33
        ```
    """
⋮----
def HotPotF1(prediction, answers_list):  # noqa: N802
⋮----
"""Compute the maximum HotPotQA-style F1 score against reference answers.

    Like `F1`, but if either normalized side is one of {"yes", "no", "noanswer"}
    and they differ, the score is 0. Otherwise, standard token-level F1 is used.

    Args:
        prediction (str): Predicted answer.
        answers_list (list[str]): List of reference answers.

    Returns:
        float: Highest HotPotQA-style F1 in [0.0, 1.0].

    Example:
        ```python
        HotPotF1("yes", ["no"])  # 0.0
        ```
    """
⋮----
def normalize_text(s)
⋮----
"""Normalize text for string and token comparisons.

    Steps:
        1) Unicode NFD normalization
        2) lowercasing
        3) punctuation removal
        4) English article removal ("a", "an", "the")
        5) whitespace collapse

    Args:
        s (str): Input string.

    Returns:
        str: Normalized string.

    Example:
        ```python
        normalize_text("The,  Eiffel  Tower!")  # "eiffel tower"
        ```
    """
s = unicodedata.normalize("NFD", s)
⋮----
def remove_articles(text)
⋮----
def white_space_fix(text)
⋮----
def remove_punc(text)
⋮----
exclude = set(string.punctuation)
⋮----
def lower(text)
⋮----
def em_score(prediction, ground_truth)
⋮----
"""Compute boolean exact match after normalization.

    Args:
        prediction (str): Predicted answer.
        ground_truth (str): Reference answer.

    Returns:
        bool: True if normalized strings are identical; otherwise False.

    Example:
        ```python
        em_score("Paris", "paris")  # True
        ```
    """
⋮----
def f1_score(prediction, ground_truth)
⋮----
"""Compute token-level F1 between prediction and reference (after normalization).

    Strings are normalized (see `normalize_text`) and split by whitespace. F1 is
    computed from token precision and recall. If there is no token overlap, returns 0.
    If both sides are empty, a diagnostic message is printed; score remains 0.

    Args:
        prediction (str): Predicted answer.
        ground_truth (str): Reference answer.

    Returns:
        float: F1 score in [0.0, 1.0].

    Example:
        ```python
        round(f1_score("the Eiffel Tower", "Eiffel Tower"), 2)  # 1.0
        ```
    """
prediction_tokens = normalize_text(prediction).split()
ground_truth_tokens = normalize_text(ground_truth).split()
⋮----
common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
num_same = sum(common.values())
⋮----
# Unlike most tasks, QReCC and SQuAD-2.0 assign 1.0 in this edge case. We don't for uniformity.
⋮----
precision = 1.0 * num_same / len(prediction_tokens)
recall = 1.0 * num_same / len(ground_truth_tokens)
f1 = (2 * precision * recall) / (precision + recall)
⋮----
def hotpot_f1_score(prediction, ground_truth)
⋮----
"""Compute HotPotQA-style token F1 with special labels.

    If either normalized string is in {"yes", "no", "noanswer"} and they differ,
    the score is 0. Otherwise compute standard token F1 after normalization.

    Args:
        prediction (str): Predicted answer.
        ground_truth (str): Reference answer.

    Returns:
        float: HotPotQA-style F1 score in [0.0, 1.0].

    Example:
        ```python
        hotpot_f1_score("no", "yes")  # 0.0
        ```
    """
normalized_prediction = normalize_text(prediction)
normalized_ground_truth = normalize_text(ground_truth)
⋮----
prediction_tokens = normalized_prediction.split()
ground_truth_tokens = normalized_ground_truth.split()
⋮----
def precision_score(prediction, ground_truth)
⋮----
"""Compute token-level precision of prediction against reference (after normalization).

    Precision is (# overlapping tokens) / (# tokens in prediction). If there is no
    token overlap, returns 0. If both sides are empty, a diagnostic message is printed;
    precision remains 0.

    Args:
        prediction (str): Predicted answer.
        ground_truth (str): Reference answer.

    Returns:
        float: Precision in [0.0, 1.0].

    Example:
        ```python
        precision_score("eiffel tower in paris", "eiffel tower")  # 0.67
        ```
    """
⋮----
def _passage_match(passages: list[str], answers: list[str]) -> bool
⋮----
"""Return True if any passage contains any answer (normalized & DPR-normalized)."""
⋮----
def passage_has_answers(passage: str, answers: list[str]) -> bool
⋮----
"""Return True if the passage contains any of the answers."""
⋮----
def _answer_match(prediction, answers, frac=1.0)
⋮----
"""Return True if prediction matches any answer.

    When `frac >= 1.0`, require exact match (EM). Otherwise, return whether the
    maximum token-level F1 across answers is at least `frac`.
    """
⋮----
def answer_exact_match(example, pred, trace=None, frac=1.0)
⋮----
"""Evaluate exact match or F1-thresholded match for an example/prediction pair.

    If `example.answer` is a string, compare `pred.answer` against it. If it's a list,
    compare against any of the references. When `frac >= 1.0` (default), use EM;
    otherwise require that the maximum F1 across references is at least `frac`.

    Args:
        example: `dspy.Example` object with field `answer` (str or list[str]).
        pred: `dspy.Prediction` object with field `answer` (str).
        trace: Unused; reserved for compatibility.
        frac (float, optional): Threshold in [0.0, 1.0]. `1.0` means EM.

    Returns:
        bool: True if the match condition holds; otherwise False.

    Example:
        ```python
        import dspy

        example = dspy.Example(answer=["Eiffel Tower", "Louvre"])
        pred = dspy.Prediction(answer="The Eiffel Tower")

        answer_exact_match(example, pred, frac=1.0)  # equivalent to EM, True
        answer_exact_match(example, pred, frac=0.5)  # True
        ```
    """
⋮----
def answer_passage_match(example, pred, trace=None)
⋮----
"""Return True if any passage in `pred.context` contains the answer(s).

    Strings are normalized (and passages also use DPR normalization internally).

    Args:
        example: `dspy.Example` object with field `answer` (str or list[str]).
        pred: `dspy.Prediction` object with field `context` (list[str]) containing passages.
        trace: Unused; reserved for compatibility.

    Returns:
        bool: True if any passage contains any reference answer; otherwise False.

    Example:
        ```python
        import dspy

        example = dspy.Example(answer="Eiffel Tower")
        pred = dspy.Prediction(context=["The Eiffel Tower is in Paris.", "..."])

        answer_passage_match(example, pred)  # True
        ```
    """
</file>

<file path="dspy/experimental/__init__.py">
__all__ = [
</file>

<file path="dspy/predict/avatar/__init__.py">

</file>

<file path="dspy/predict/avatar/avatar.py">
def get_number_with_suffix(number: int) -> str
⋮----
class Avatar(dspy.Module)
⋮----
def _get_field(self, field_info: FieldInfo)
⋮----
def _update_signature(self, idx: int, omit_action: bool = False)
⋮----
def _call_tool(self, tool_name: str, tool_input_query: str) -> str
⋮----
def forward(self, **kwargs)
⋮----
args = {
⋮----
idx = 1
tool_name = None
action_results: list[ActionOutput] = []
max_iters = None if "max_iters" not in kwargs else kwargs["max_iters"]
⋮----
actor_output = self.actor(**args)
action = getattr(actor_output, f"action_{idx}")
⋮----
tool_name = action.tool_name
tool_input_query = action.tool_input_query
⋮----
tool_output = self._call_tool(tool_name, tool_input_query)
⋮----
final_answer = self.actor(**args)
</file>

<file path="dspy/predict/avatar/models.py">
class Tool(BaseModel)
⋮----
tool: Any
name: str
desc: str | None
input_type: str | None = None
⋮----
def __str__(self) -> str
⋮----
def __repr__(self) -> str
⋮----
class Action(BaseModel)
⋮----
tool_name: Any = Field(..., description="Name of the tool to use.")
tool_input_query: Any = Field(..., description="Query to pass as input to the tool.")
⋮----
class ActionOutput(BaseModel)
⋮----
tool_name: str
tool_input_query: str
tool_output: str
</file>

<file path="dspy/predict/avatar/signatures.py">
class Actor(dspy.Signature)
⋮----
"""You will be given `Tools` which will be a list of tools to use to accomplish the `Goal`. Given the user query, your task is to decide which tool to use and what input values to provide.

    You will output action needed to accomplish the `Goal`. `Action` should have a tool to use and the input query to pass to the tool.

    Note: You can opt to use no tools and provide the final answer directly. You can also one tool multiple times with different input queries if applicable."""
⋮----
goal: str = dspy.InputField(
tools: list[str] = dspy.InputField(
action_1: Action = dspy.OutputField(
</file>

<file path="dspy/predict/__init__.py">
__all__ = [
</file>

<file path="dspy/predict/aggregation.py">
def default_normalize(s)
⋮----
def majority(prediction_or_completions, normalize=default_normalize, field=None)
⋮----
"""
    Returns the most common completion for the target field (or the last field) in the signature.
    When normalize returns None, that completion is ignored.
    In case of a tie, earlier completion are prioritized.
    """
⋮----
# Get the completions
⋮----
completions = prediction_or_completions.completions
⋮----
completions = prediction_or_completions
⋮----
signature = completions.signature
⋮----
signature = None
⋮----
field = list(signature.output_fields.keys())[-1]
⋮----
field = list(completions[0].keys())[-1]
⋮----
# Normalize
normalize = normalize if normalize else lambda x: x
normalized_values = [normalize(completion[field]) for completion in completions]
normalized_values_ = [x for x in normalized_values if x is not None]
⋮----
# Count
value_counts = {}
⋮----
majority_value = max(value_counts, key=value_counts.get)
⋮----
# Return the first completion with the majority value in the field
⋮----
# if input_type == Prediction:
</file>

<file path="dspy/predict/best_of_n.py">
class BestOfN(Module)
⋮----
N: int,  # noqa: N803
⋮----
"""
        Runs a module up to `N` times with different rollout IDs at `temperature=1.0` and
        returns the best prediction out of `N` attempts or the first prediction that passes the
        `threshold`.

        Args:
            module (Module): The module to run.
            N (int): The number of times to run the module.
            reward_fn (Callable[[dict, Prediction], float]): The reward function which takes in the args passed to the module, the resulting prediction, and returns a scalar reward.
            threshold (float): The threshold for the reward function.
            fail_count (Optional[int], optional): The number of times the module can fail before raising an error. Defaults to N if not provided.

        Example:
            ```python
            import dspy

            dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

            # Define a QA module with chain of thought
            qa = dspy.ChainOfThought("question -> answer")

            # Define a reward function that checks for one-word answers
            def one_word_answer(args, pred):
                return 1.0 if len(pred.answer.split()) == 1 else 0.0

            # Create a refined module that tries up to 3 times
            best_of_3 = dspy.BestOfN(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0)

            # Use the refined module
            result = best_of_3(question="What is the capital of Belgium?").answer
            # Returns: Brussels
            ```
        """
⋮----
self.reward_fn = lambda *args: reward_fn(*args)  # to prevent this from becoming a parameter
⋮----
self.fail_count = fail_count or N  # default to N if fail_count is not provided
⋮----
def forward(self, **kwargs)
⋮----
lm = self.module.get_lm() or dspy.settings.lm
start = lm.kwargs.get("rollout_id", 0)
rollout_ids = [start + i for i in range(self.N)]
⋮----
lm_ = lm.copy(rollout_id=rid, temperature=1.0)
mod = self.module.deepcopy()
⋮----
pred = mod(**kwargs)
trace = dspy.settings.trace.copy()
⋮----
# NOTE: Not including the trace of reward_fn.
reward = self.reward_fn(kwargs, pred)
</file>

<file path="dspy/predict/chain_of_thought.py">
# NOTE: This restores the legacy rationale_field behavior after PR #8822.
⋮----
class ChainOfThought(Module)
⋮----
"""
        A module that reasons step by step in order to predict the output of a task.

        Args:
            signature (Type[dspy.Signature]): The signature of the module.
            rationale_field (Optional[Union[dspy.OutputField, pydantic.fields.FieldInfo]]): The field that will contain the reasoning.
            rationale_field_type (Type): The type of the rationale field.
            **config: The configuration for the module.
        """
⋮----
signature = ensure_signature(signature)
prefix = "Reasoning: Let's think step by step in order to"
desc = "${reasoning}"
rationale_field_type = rationale_field.annotation if rationale_field else rationale_field_type
rationale_field = rationale_field if rationale_field else dspy.OutputField(prefix=prefix, desc=desc)
extended_signature = signature.prepend(name="reasoning", field=rationale_field, type_=rationale_field_type)
⋮----
def forward(self, **kwargs)
⋮----
async def aforward(self, **kwargs)
</file>

<file path="dspy/predict/code_act.py">
logger = logging.getLogger(__name__)
⋮----
class CodeAct(ReAct, ProgramOfThought)
⋮----
"""
    CodeAct is a module that utilizes the Code Interpreter and predefined tools to solve the problem.
    """
⋮----
def __init__(self, signature: str | type[Signature], tools: list[Callable], max_iters: int = 5, interpreter: PythonInterpreter | None = None)
⋮----
"""
        Initializes the CodeAct class with the specified model, temperature, and max tokens.

        Args:
            signature (Union[str, Type[Signature]]): The signature of the module.
            tools (list[Callable]): The tool callables to be used. CodeAct only accepts functions and not callable objects.
            max_iters (int): The maximum number of iterations to generate the answer.
            interpreter: PythonInterpreter instance to use. If None, a new one is instantiated.
        Example:
            ```python
            from dspy.predict import CodeAct
            def factorial(n):
                if n == 1:
                    return 1
                return n * factorial(n-1)

            act = CodeAct("n->factorial", tools=[factorial])
            act(n=5) # 120
            ```
        """
⋮----
tools = [t if isinstance(t, Tool) else Tool(t) for t in tools]
⋮----
tools = {tool.name: tool for tool in tools}
⋮----
instructions = self._build_instructions(self.signature, tools)
⋮----
codeact_signature = (
⋮----
extract_signature = dspy.Signature(
⋮----
# It will raises exception when dspy cannot find available deno instance by now.
⋮----
def _build_instructions(self, signature, tools)
⋮----
instructions = [f"{signature.instructions}\n"] if signature.instructions else []
inputs = ", ".join([f"`{k}`" for k in signature.input_fields.keys()])
outputs = ", ".join([f"`{k}`" for k in signature.output_fields.keys()])
⋮----
def forward(self, **kwargs)
⋮----
# Define the tool functions in the interpreter
⋮----
trajectory = {}
max_iters = kwargs.pop("max_iters", self.max_iters)
⋮----
code_data = self.codeact(trajectory=trajectory, **kwargs)
output = None
⋮----
extract = self._call_with_potential_trajectory_truncation(self.extractor, trajectory, **kwargs)
</file>

<file path="dspy/predict/knn.py">
class KNN
⋮----
def __init__(self, k: int, trainset: list[Example], vectorizer: Embedder)
⋮----
"""
        A k-nearest neighbors retriever that finds similar examples from a training set.

        Args:
            k: Number of nearest neighbors to retrieve
            trainset: List of training examples to search through
            vectorizer: The `Embedder` to use for vectorization

        Example:
            ```python
            import dspy
            from sentence_transformers import SentenceTransformer

            # Create a training dataset with examples
            trainset = [
                dspy.Example(input="hello", output="world"),
                # ... more examples ...
            ]

            # Initialize KNN with a sentence transformer model
            knn = KNN(
                k=3,
                trainset=trainset,
                vectorizer=dspy.Embedder(SentenceTransformer("all-MiniLM-L6-v2").encode)
            )

            # Find similar examples
            similar_examples = knn(input="hello")
            ```
        """
⋮----
trainset_casted_to_vectorize = [
⋮----
def __call__(self, **kwargs) -> list
⋮----
input_example_vector = self.embedding([" | ".join([f"{key}: {val}" for key, val in kwargs.items()])])
scores = np.dot(self.trainset_vectors, input_example_vector.T).squeeze()
nearest_samples_idxs = scores.argsort()[-self.k :][::-1]
</file>

<file path="dspy/predict/multi_chain_comparison.py">
class MultiChainComparison(Module)
⋮----
def __init__(self, signature, M=3, temperature=0.7, **config):  # noqa: N803
⋮----
signature = ensure_signature(signature)
⋮----
signature = signature.append(
⋮----
signature = signature.prepend(
⋮----
def forward(self, completions, **kwargs)
⋮----
attempts = []
⋮----
rationale = c.get("rationale", c.get("reasoning")).strip().split("\n")[0].strip()
answer = str(c[self.last_key]).strip().split("\n")[0].strip()
⋮----
kwargs = {
</file>

<file path="dspy/predict/parallel.py">
class Parallel
⋮----
def forward(self, exec_pairs: list[tuple[Any, Example]], num_threads: int | None = None) -> list[Any]
⋮----
num_threads = num_threads if num_threads is not None else self.num_threads
⋮----
executor = ParallelExecutor(
⋮----
def process_pair(pair)
⋮----
result = None
⋮----
result = module(**example.inputs())
⋮----
result = module(example)
⋮----
result = module(**example)
⋮----
result = module(*example)
⋮----
# Execute the processing function over the execution pairs
results = executor.execute(process_pair, exec_pairs)
⋮----
# Populate failed examples and exceptions from the executor
⋮----
def __call__(self, *args: Any, **kwargs: Any) -> Any
</file>

<file path="dspy/predict/parameter.py">
class Parameter
</file>

<file path="dspy/predict/predict.py">
logger = logging.getLogger(__name__)
⋮----
class Predict(Module, Parameter)
⋮----
"""Basic DSPy module that maps inputs to outputs using a language model.

    Args:
        signature: The input/output signature describing the task.
        callbacks: Optional list of callbacks for instrumentation.
        **config: Default keyword arguments forwarded to the underlying
            language model. These values can be overridden for a single
            invocation by passing a ``config`` dictionary when calling the
            module. For example::

                predict = dspy.Predict("q -> a", rollout_id=1, temperature=1.0)
                predict(q="What is 1 + 52?", config={"rollout_id": 2, "temperature": 1.0})
    """
⋮----
def __init__(self, signature: str | type[Signature], callbacks: list[BaseCallback] | None = None, **config)
⋮----
def reset(self)
⋮----
def dump_state(self, json_mode=True)
⋮----
state_keys = ["traces", "train"]
state = {k: getattr(self, k) for k in state_keys}
⋮----
demo = demo.copy()
⋮----
# FIXME: Saving BaseModels as strings in examples doesn't matter because you never re-access as an object
⋮----
def load_state(self, state: dict) -> "Predict"
⋮----
"""Load the saved state of a `Predict` object.

        Args:
            state: The saved state of a `Predict` object.

        Returns:
            Self to allow method chaining.
        """
excluded_keys = ["signature", "extended_signature", "lm"]
⋮----
# `excluded_keys` are fields that go through special handling.
⋮----
if "extended_signature" in state:  # legacy, up to and including 2.5, for CoT.
⋮----
def _get_positional_args_error_message(self)
⋮----
input_fields = list(self.signature.input_fields.keys())
⋮----
def __call__(self, *args, **kwargs)
⋮----
async def acall(self, *args, **kwargs)
⋮----
def _forward_preprocess(self, **kwargs)
⋮----
# Extract the three privileged keyword arguments.
⋮----
signature = ensure_signature(kwargs.pop("signature", self.signature))
demos = kwargs.pop("demos", self.demos)
config = {**self.config, **kwargs.pop("config", {})}
⋮----
# Get the right LM to use.
lm = kwargs.pop("lm", self.lm) or settings.lm
⋮----
# Many users mistakenly use `dspy.configure(lm="openai/gpt-4o-mini")` instead of
# `dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))`, so we are providing a specific error message.
⋮----
# If temperature is unset or <=0.15, and n > 1, set temperature to 0.7 to keep randomness.
temperature = config.get("temperature") or lm.kwargs.get("temperature")
num_generations = config.get("n") or lm.kwargs.get("n") or lm.kwargs.get("num_generations") or 1
⋮----
# If the `prediction` is the standard predicted outputs format
# (https://platform.openai.com/docs/guides/predicted-outputs), we remove it from input kwargs and add it
# to the lm kwargs.
⋮----
# Populate default values for missing input fields.
⋮----
present = [k for k in signature.input_fields if k in kwargs]
missing = [k for k in signature.input_fields if k not in kwargs]
⋮----
def _forward_postprocess(self, completions, signature, **kwargs)
⋮----
pred = Prediction.from_completions(completions, signature=signature)
⋮----
trace = settings.trace
⋮----
def _should_stream(self)
⋮----
stream_listeners = settings.stream_listeners or []
should_stream = settings.send_stream is not None
⋮----
should_stream = any(stream_listener.predict == self for stream_listener in stream_listeners)
⋮----
def forward(self, **kwargs)
⋮----
adapter = settings.adapter or ChatAdapter()
⋮----
completions = adapter(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)
⋮----
async def aforward(self, **kwargs)
⋮----
completions = await adapter.acall(lm, lm_kwargs=config, signature=signature, demos=demos, inputs=kwargs)
⋮----
def update_config(self, **kwargs)
⋮----
def get_config(self)
⋮----
def __repr__(self)
⋮----
def serialize_object(obj)
⋮----
"""
    Recursively serialize a given object into a JSON-compatible format.
    Supports Pydantic models, lists, dicts, and primitive types.
    """
⋮----
# Use model_dump with mode="json" to ensure all fields (including HttpUrl, datetime, etc.)
# are converted to JSON-serializable types (strings)
⋮----
# # TODO: FIXME: Hmm, I guess expected behavior is that contexts can
# affect execution. Well, we need to determine whether context dominates, __init__ demoninates, or forward dominates.
# Generally, unless overwritten, we'd see n=None, temperature=None.
# That will eventually mean we have to learn them.
</file>

<file path="dspy/predict/program_of_thought.py">
logger = logging.getLogger(__name__)
⋮----
class ProgramOfThought(Module)
⋮----
"""
    A DSPy module that runs Python programs to solve a problem.
    This module requires deno to be installed. Please install deno following https://docs.deno.com/runtime/getting_started/installation/

    Example:
    ```
    import dspy

    lm = dspy.LM('openai/gpt-4o-mini')
    dspy.configure(lm=lm)
    pot = dspy.ProgramOfThought("question -> answer")
    pot(question="what is 1+1?")
    ```
    """
⋮----
def __init__(self, signature: str | type[Signature], max_iters: int = 3, interpreter: PythonInterpreter | None = None)
⋮----
"""
        Args:
            signature: The signature of the module.
            max_iters: The maximum number of iterations to retry code generation and execution.
            interpreter: PythonInterpreter instance to use. If None, a new one is instantiated.
        """
⋮----
self.signature = signature = ensure_signature(signature)
⋮----
# It will raises exception when dspy cannot find available deno instance by now.
⋮----
def _generate_signature(self, mode)
⋮----
signature_dict = dict(self.input_fields)
fields_for_mode = {
⋮----
def _generate_instruction(self, mode)
⋮----
mode_inputs = ", ".join(
mode_outputs = ", ".join(
final_outputs = ", ".join(
⋮----
instr = [
⋮----
else:  # mode == 'answer'
⋮----
def _parse_code(self, code_data)
⋮----
code = code_data.get("generated_code", "").split("---", 1)[0].split("\n\n\n", 1)[0]
code_match = re.search(r"```python[ \n](.*?)[ \n]```?", code, re.DOTALL)
code_block = (code_match.group(1) if code_match else code).replace("\\n", "\n")
⋮----
lines = code_block.split("\n")
last_line_match = re.match(r"^(\w+)\s*=", lines[-1].strip())
⋮----
code_block = re.sub(
⋮----
def _execute_code(self, code)
⋮----
"""
        Execute the code using PythonInterpreter and return the output or error.
        """
⋮----
result = self.interpreter.execute(code)
⋮----
result = result.output
# Since it's more complex structure now, just blindly use json to represents all.
output = json.dumps(result)
⋮----
def forward(self, **kwargs)
⋮----
input_kwargs = {field_name: kwargs[field_name] for field_name in self.input_fields}
code_data = self.code_generate(**input_kwargs)
output = None
⋮----
hop = 1
# Retying code generation and execution until no error or reach max_iters
⋮----
code_data = self.code_regenerate(**input_kwargs)
⋮----
output_gen_result = self.generate_output(**input_kwargs)
</file>

<file path="dspy/predict/react.py">
logger = logging.getLogger(__name__)
⋮----
class ReAct(Module)
⋮----
def __init__(self, signature: type["Signature"], tools: list[Callable], max_iters: int = 20)
⋮----
"""
        ReAct stands for "Reasoning and Acting," a popular paradigm for building tool-using agents.
        In this approach, the language model is iteratively provided with a list of tools and has
        to reason about the current situation. The model decides whether to call a tool to gather more
        information or to finish the task based on its reasoning process. The DSPy version of ReAct is
        generalized to work over any signature, thanks to signature polymorphism.

        Args:
            signature: The signature of the module, which defines the input and output of the react module.
            tools (list[Callable]): A list of functions, callable objects, or `dspy.Tool` instances.
            max_iters (Optional[int]): The maximum number of iterations to run. Defaults to 10.

        Example:

        ```python
        def get_weather(city: str) -> str:
            return f"The weather in {city} is sunny."

        react = dspy.ReAct(signature="question->answer", tools=[get_weather])
        pred = react(question="What is the weather in Tokyo?")
        ```
        """
⋮----
self.signature = signature = ensure_signature(signature)
⋮----
tools = [t if isinstance(t, Tool) else Tool(t) for t in tools]
tools = {tool.name: tool for tool in tools}
⋮----
inputs = ", ".join([f"`{k}`" for k in signature.input_fields.keys()])
outputs = ", ".join([f"`{k}`" for k in signature.output_fields.keys()])
instr = [f"{signature.instructions}\n"] if signature.instructions else []
⋮----
react_signature = (
⋮----
fallback_signature = dspy.Signature(
⋮----
def _format_trajectory(self, trajectory: dict[str, Any])
⋮----
adapter = dspy.settings.adapter or dspy.ChatAdapter()
trajectory_signature = dspy.Signature(f"{', '.join(trajectory.keys())} -> x")
⋮----
def forward(self, **input_args)
⋮----
trajectory = {}
max_iters = input_args.pop("max_iters", self.max_iters)
⋮----
pred = self._call_with_potential_trajectory_truncation(self.react, trajectory, **input_args)
⋮----
extract = self._call_with_potential_trajectory_truncation(self.extract, trajectory, **input_args)
⋮----
async def aforward(self, **input_args)
⋮----
pred = await self._async_call_with_potential_trajectory_truncation(self.react, trajectory, **input_args)
⋮----
extract = await self._async_call_with_potential_trajectory_truncation(self.extract, trajectory, **input_args)
⋮----
def _call_with_potential_trajectory_truncation(self, module, trajectory, **input_args)
⋮----
trajectory = self.truncate_trajectory(trajectory)
⋮----
async def _async_call_with_potential_trajectory_truncation(self, module, trajectory, **input_args)
⋮----
def truncate_trajectory(self, trajectory)
⋮----
"""Truncates the trajectory so that it fits in the context window.

        Users can override this method to implement their own truncation logic.
        """
keys = list(trajectory.keys())
⋮----
# Every tool call has 4 keys: thought, tool_name, tool_args, and observation.
⋮----
def _fmt_exc(err: BaseException, *, limit: int = 5) -> str
⋮----
"""
    Return a one-string traceback summary.
    * `limit` - how many stack frames to keep (from the innermost outwards).
    """
⋮----
"""
Thoughts and Planned Improvements for dspy.ReAct.

TOPIC 01: How Trajectories are Formatted, or rather when they are formatted.

Right now, both sub-modules are invoked with a `trajectory` argument, which is a string formatted in `forward`. Though
the formatter uses a general adapter.format_fields, the tracing of DSPy only sees the string, not the formatting logic.

What this means is that, in demonstrations, even if the user adjusts the adapter for a fixed program, the demos' format
will not update accordingly, but the inference-time trajectories will.

One way to fix this is to support `format=fn` in the dspy.InputField() for "trajectory" in the signatures. But this
means that care must be taken that the adapter is accessed at `forward` runtime, not signature definition time.

Another potential fix is to more natively support a "variadic" input field, where the input is a list of dictionaries,
or a big dictionary, and have each adapter format it accordingly.

Trajectories also affect meta-programming modules that view the trace later. It's inefficient O(n^2) to view the
trace of every module repeating the prefix.


TOPIC 03: Simplifying ReAct's __init__ by moving modular logic to the Tool class.
    * Handling exceptions and error messages.
    * More cleanly defining the "finish" tool, perhaps as a runtime-defined function?


TOPIC 04: Default behavior when the trajectory gets too long.


TOPIC 05: Adding more structure around how the instruction is formatted.
    * Concretely, it's now a string, so an optimizer can and does rewrite it freely.
    * An alternative would be to add more structure, such that a certain template is fixed but values are variable?


TOPIC 06: Idiomatically allowing tools that maintain state across iterations, but not across different `forward` calls.
    * So the tool would be newly initialized at the start of each `forward` call, but maintain state across iterations.
    * This is pretty useful for allowing the agent to keep notes or count certain things, etc.
"""
</file>

<file path="dspy/predict/refine.py">
class OfferFeedback(Signature)
⋮----
"""
    In the discussion, assign blame to each module that contributed to the final reward being below the threshold, if
    any. Then, prescribe concrete advice of how the module should act on its future input when we retry the process, if
    it were to receive the same or similar inputs. If a module is not to blame, the advice should be N/A.
    The module will not see its own history, so it needs to rely on entirely concrete and actionable advice from you
    to avoid the same mistake on the same or similar inputs.
    """
⋮----
program_code: str = InputField(desc="The code of the program that we are analyzing")
modules_defn: str = InputField(desc="The definition of each module in the program, including its I/O")
program_inputs: str = InputField(desc="The inputs to the program that we are analyzing")
program_trajectory: str = InputField(desc="The trajectory of the program's execution, showing each module's I/O")
program_outputs: str = InputField(desc="The outputs of the program that we are analyzing")
reward_code: str = InputField(desc="The code of the reward function that we are analyzing")
target_threshold: float = InputField(desc="The target threshold for the reward function")
reward_value: float = InputField(desc="The reward value assigned to the program's outputs")
module_names: list[str] = InputField(desc="The names of the modules in the program, for which we seek advice")
discussion: str = OutputField(desc="Discussing blame of where each module went wrong, if it did")
advice: dict[str, str] = OutputField(
⋮----
class Refine(Module)
⋮----
N: int,  # noqa: N803
⋮----
"""
        Refines a module by running it up to N times with different rollout IDs at `temperature=1.0`
        and returns the best prediction.

        This module runs the provided module multiple times with varying rollout identifiers and selects
        either the first prediction that exceeds the specified threshold or the one with the highest reward.
        If no prediction meets the threshold, it automatically generates feedback to improve future predictions.


        Args:
            module (Module): The module to refine.
            N (int): The number of times to run the module. must
            reward_fn (Callable): The reward function.
            threshold (float): The threshold for the reward function.
            fail_count (Optional[int], optional): The number of times the module can fail before raising an error

        Example:
            ```python
            import dspy

            dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

            # Define a QA module with chain of thought
            qa = dspy.ChainOfThought("question -> answer")

            # Define a reward function that checks for one-word answers
            def one_word_answer(args, pred):
                return 1.0 if len(pred.answer.split()) == 1 else 0.0

            # Create a refined module that tries up to 3 times
            best_of_3 = dspy.Refine(module=qa, N=3, reward_fn=one_word_answer, threshold=1.0)

            # Use the refined module
            result = best_of_3(question="What is the capital of Belgium?").answer
            # Returns: Brussels
            ```
        """
⋮----
self.reward_fn = lambda *args: reward_fn(*args)  # to prevent this from becoming a parameter
⋮----
self.fail_count = fail_count or N  # default to N if fail_count is not provided
⋮----
def forward(self, **kwargs)
⋮----
lm = self.module.get_lm() or dspy.settings.lm
start = lm.kwargs.get("rollout_id", 0)
rollout_ids = [start + i for i in range(self.N)]
⋮----
advice = None
adapter = dspy.settings.adapter or dspy.ChatAdapter()
⋮----
lm_ = lm.copy(rollout_id=rid, temperature=1.0)
mod = self.module.deepcopy()
⋮----
predictor2name = {predictor: name for name, predictor in mod.named_predictors()}
signature2name = {predictor.signature: name for name, predictor in mod.named_predictors()}
module_names = [name for name, _ in mod.named_predictors()]
⋮----
outputs = mod(**kwargs)
⋮----
class WrapperAdapter(adapter.__class__)
⋮----
def __call__(self, lm, lm_kwargs, signature, demos, inputs)
⋮----
inputs["hint_"] = advice.get(signature2name[signature], "N/A")  # noqa: B023
signature = signature.append(
⋮----
trace = dspy.settings.trace.copy()
⋮----
# TODO: Remove the hint from the trace, if it's there.
⋮----
# NOTE: Not including the trace of reward_fn.
reward = self.reward_fn(kwargs, outputs)
⋮----
modules = {"program_code": self.module_code, "modules_defn": inspect_modules(mod)}
trajectory = [{"module_name": predictor2name[p], "inputs": i, "outputs": dict(o)} for p, i, o in trace]
trajectory = {
reward = {
⋮----
advise_kwargs = dict(**modules, **trajectory, **reward, module_names=module_names)
# only dumps if it's a list or dict
advise_kwargs = {
advice = dspy.Predict(OfferFeedback)(**advise_kwargs).advice
# print(f"Advice for each module: {advice}")
⋮----
def inspect_modules(program)
⋮----
separator = "-" * 80
output = [separator]
⋮----
signature = predictor.signature
instructions = textwrap.dedent(signature.instructions)
instructions = ("\n" + "\t" * 2).join([""] + instructions.splitlines())
⋮----
def recursive_mask(o)
⋮----
# If the object is already serializable, return it.
⋮----
# If it's a dictionary, apply recursively to its values.
⋮----
# If it's a list, apply recursively.
⋮----
# If it's a tuple, apply recursively.
⋮----
# Otherwise, replace it with a placeholder string (or use repr(o)).
</file>

<file path="dspy/predict/retry.py">
# import copy
⋮----
# import dspy
⋮----
# from .predict import Predict
⋮----
# class Retry(Predict):
#     def __init__(self, module):
#         super().__init__(module.signature)
#         self.module = module
#         self.original_signature = module.signature
#         self.original_forward = module.forward
#         self.new_signature = self._create_new_signature(self.original_signature)
⋮----
#     def _create_new_signature(self, signature):
#         # Add "Past" input fields for each output field
#         for key, value in signature.output_fields.items():
#             actual_prefix = value.json_schema_extra["prefix"].split(":")[0] + ":"
#             signature = signature.append(f"past_{key}", dspy.InputField(
#                 prefix="Previous " + actual_prefix,
#                 desc=f"past {actual_prefix[:-1]} with errors",
#                 format=value.json_schema_extra.get("format"),
#             ))
⋮----
#         signature = signature.append("feedback", dspy.InputField(
#             prefix="Instructions:",
#             desc="Some instructions you must satisfy",
#             format=str,
#         ))
⋮----
#         return signature
⋮----
#     def forward(self, *, past_outputs, **kwargs):
#         # Take into account the possible new signature, as in TypedPredictor
#         new_signature = kwargs.pop("new_signature", None)
#         if new_signature:
#             self.original_signature = new_signature
#             self.new_signature = self._create_new_signature(self.original_signature)
⋮----
#         # Convert the dict past_outputs={"answer": ...} to kwargs
#         # {past_answer=..., ...}
#         for key, value in past_outputs.items():
#             past_key = f"past_{key}"
#             if past_key in self.new_signature.input_fields:
#                 kwargs[past_key] = value
#         # Tell the wrapped module to use the new signature.
#         # Note: This only works if the wrapped module is a Predict or ChainOfThought.
#         kwargs["new_signature"] = self.new_signature
#         return self.original_forward(**kwargs)
⋮----
#     def __call__(self, **kwargs):
#         copy.deepcopy(kwargs)
#         kwargs["_trace"] = False
#         kwargs.setdefault("demos", self.demos if self.demos is not None else [])
⋮----
#         # perform backtracking
#         if dspy.settings.backtrack_to == self:
#             for key, value in dspy.settings.backtrack_to_args.items():
#                 kwargs.setdefault(key, value)
#             pred = self.forward(**kwargs)
#         else:
#             pred = self.module(**kwargs)
⋮----
#         # now pop multiple reserved keys
#         # NOTE(shangyin) past_outputs seems not useful to include in demos,
#         # therefore dropped
#         for key in ["_trace", "demos", "signature", "new_signature", "config", "lm", "past_outputs"]:
#             kwargs.pop(key, None)
⋮----
#         if dspy.settings.trace is not None:
#             trace = dspy.settings.trace
#             trace.append((self, {**kwargs}, pred))
#         return pred
</file>

<file path="dspy/predict/rlm.py">
"""
Recursive Language Model (RLM) module for DSPy.

RLMs are an inference strategy where LLMs treat long contexts as part of an external
environment rather than feeding them directly to the model. The LLM writes Python code
to programmatically examine, decompose, and recursively call sub-LLMs over snippets.

Reference: "Recursive Language Models" (Zhang, Kraska, Khattab, 2025)
"""
⋮----
logger = logging.getLogger(__name__)
⋮----
# TODO: Optimize this prompt across a diverse benchmark
⋮----
ACTION_INSTRUCTIONS_TEMPLATE = """You are tasked with producing the following outputs given the inputs {inputs}:
⋮----
# Pattern to match markdown code fences: ```python\n...\n``` or ```\n...\n```
_CODE_FENCE_PATTERN = re.compile(r"^```(?:python|py)?\s*\n(.*)\n```\s*$", re.DOTALL)
⋮----
def _strip_code_fences(code: str) -> str
⋮----
code = code.strip()
match = _CODE_FENCE_PATTERN.match(code)
⋮----
@experimental
class RLM(Module)
⋮----
"""Recursive Language Model module.

    Uses a sandboxed REPL to let the LLM programmatically explore large contexts
    through code execution. The LLM writes Python code to examine data, call
    sub-LLMs for semantic analysis, and build up answers iteratively.

    The default interpreter is PythonInterpreter (Deno/Pyodide/WASM), but you
    can provide any CodeInterpreter implementation (e.g., MockInterpreter, or write a custom one using E2B or Modal).

    Note: RLM instances are not thread-safe when using a custom interpreter.
    Create separate RLM instances for concurrent use, or use the default
    PythonInterpreter which creates a fresh instance per forward() call.

    Example:
        ```python
        # Basic usage
        rlm = dspy.RLM("context, query -> output", max_iterations=10)
        result = rlm(context="...very long text...", query="What is the magic number?")
        print(result.output)
        ```
    """
⋮----
"""
        Args:
            signature: Defines inputs and outputs. String like "context, query -> answer"
                      or a Signature class.
            max_iterations: Maximum REPL interaction iterations.
            max_llm_calls: Maximum sub-LLM calls (llm_query/llm_query_batched) per execution.
            max_output_chars: Maximum characters to include from REPL output.
            verbose: Whether to log detailed execution info.
            tools: List of tool functions or dspy.Tool objects callable from interpreter code.
                  Built-in tools: llm_query(prompt), llm_query_batched(prompts).
            sub_lm: LM for llm_query/llm_query_batched. Defaults to dspy.settings.lm.
                   Allows using a different (e.g., cheaper) model for sub-queries.
            interpreter: CodeInterpreter implementation to use. Defaults to PythonInterpreter.
        """
⋮----
# Build the action and extract signatures
⋮----
# =========================================================================
# Tool Creation and Validation
⋮----
# Reserved tool names that conflict with built-in sandbox functions
_RESERVED_TOOL_NAMES = frozenset({"llm_query", "llm_query_batched", "SUBMIT", "print"})
⋮----
def _normalize_tools(self, tools: list[Callable] | None) -> dict[str, Tool]
⋮----
"""Normalize tools list to a dict of Tool objects keyed by name."""
⋮----
def to_tool(func: Callable | Tool) -> Tool
⋮----
# List of callables/Tools -> normalize to Tool objects
tool_list = [to_tool(t) for t in tools]
⋮----
def _validate_tools(self, tools: dict[str, Tool]) -> None
⋮----
"""Validate user-provided tools have valid names."""
⋮----
def _format_tool_docs(self, tools: dict[str, Tool]) -> str
⋮----
"""Format user-provided tools for inclusion in instructions."""
⋮----
lines = ["\nAdditional tools available (use these instead of standard library equivalents):"]
⋮----
# Build signature string from Tool's args
params = []
⋮----
arg_type = arg_schema.get("type", "Any")
⋮----
params_str = ", ".join(params)
sig_str = f"{tool.name}({params_str})"
⋮----
# Get description with newlines escaped
desc = (tool.desc or "No description").replace("\n", "  ")
⋮----
def _make_llm_tools(self, max_workers: int = 8) -> dict[str, Callable]
⋮----
"""Create llm_query and llm_query_batched tools with a fresh call counter."""
state = {"call_count": 0}
lock = threading.Lock()
lm = self.sub_lm
⋮----
def _check_and_increment(n: int = 1) -> None
⋮----
def _query_lm(prompt: str) -> str
⋮----
target_lm = lm if lm is not None else dspy.settings.lm
⋮----
response = target_lm(prompt)
⋮----
item = response[0]
⋮----
def llm_query(prompt: str) -> str
⋮----
"""Query the LLM with a prompt string."""
⋮----
def llm_query_batched(prompts: list[str]) -> list[str]
⋮----
"""Query the LLM with multiple prompts concurrently."""
⋮----
results: dict[int, str] = {}
⋮----
future_to_idx = {executor.submit(_query_lm, p): i for i, p in enumerate(prompts)}
⋮----
idx = future_to_idx[future]
⋮----
@property
    def tools(self) -> dict[str, Tool]
⋮----
"""User-provided tools (excludes internal llm_query/llm_query_batched)."""
⋮----
# Signature Building
⋮----
def _build_signatures(self) -> tuple[Signature, Signature]
⋮----
"""Build the action and extract signatures from templates."""
inputs_str = ", ".join(f"`{n}`" for n in self.signature.input_fields)
⋮----
# Simple names for SUBMIT() examples
final_output_names = ", ".join(self.signature.output_fields.keys())
⋮----
output_fields = "\n".join(
⋮----
# Include original signature instructions (docstring) if present
task_instructions = f"{self.signature.instructions}\n\n" if self.signature.instructions else ""
⋮----
# Format tool documentation for user-provided tools
tool_docs = self._format_tool_docs(self._user_tools)
⋮----
action_sig = (
⋮----
# Extract signature: includes the original signature's output fields and task instructions.
extract_instructions = """Based on the REPL trajectory, extract the final outputs now.
⋮----
# Prepend original task instructions to extract instructions so the LLM knows what task to extract for
extended_task_instructions = ""
⋮----
extended_task_instructions = "The trajectory was generated with the following objective: \n" + task_instructions + "\n"
full_extract_instructions = extended_task_instructions + extract_instructions
⋮----
extract_sig = dspy.Signature(
extract_sig = extract_sig.prepend("repl_history", dspy.InputField(desc="Your REPL interactions so far"), type_=REPLHistory)
extract_sig = extract_sig.prepend("variables_info", dspy.InputField(desc="Metadata about the variables available in the REPL"), type_=str)
⋮----
# Input/Output Processing
⋮----
def _get_output_fields_info(self) -> list[dict]
⋮----
"""Get output field info for sandbox registration."""
fields = []
⋮----
annotation = getattr(field, "annotation", str)
field_info = {"name": name}
# Only include type for simple types that work in function signatures
# Complex types like Literal, Union, etc. are not included
⋮----
def _build_variables(self, **input_args: Any) -> list[REPLVariable]
⋮----
"""Build REPLVariable list from input arguments with field metadata."""
variables = []
⋮----
field_info = self.signature.input_fields.get(name)
⋮----
def _format_output(self, output: str) -> str
⋮----
"""Format and truncate REPL output."""
⋮----
def _validate_inputs(self, input_args: dict[str, Any]) -> None
⋮----
"""Raise ValueError if required input fields are missing."""
missing = set(self.signature.input_fields.keys()) - set(input_args.keys())
⋮----
# CodeInterpreter Lifecycle
⋮----
def _prepare_execution_tools(self) -> dict[str, Callable]
⋮----
"""Create fresh LLM tools and merge with user-provided tools."""
execution_tools = self._make_llm_tools()
# Extract underlying functions from Tool objects for the interpreter
⋮----
def _inject_execution_context(self, interpreter: CodeInterpreter, execution_tools: dict[str, Callable]) -> None
⋮----
"""Inject execution tools and output fields into an interpreter.

        This ensures llm_query, llm_query_batched, and typed FINAL signatures are available,
        even for user-provided interpreters. Each forward() call gets fresh tools with a
        fresh call counter, so we must inject on every execution.
        """
⋮----
# Reset registration flag to force re-registration with fresh tools
⋮----
@contextmanager
    def _interpreter_context(self, execution_tools: dict[str, Callable]) -> Iterator[CodeInterpreter]
⋮----
"""Yield interpreter, creating PythonInterpreter if none provided at init."""
⋮----
repl = PythonInterpreter(
⋮----
# Execution Core
⋮----
"""Use extract module to get final output when max iterations reached."""
⋮----
variables_info = [variable.format() for variable in variables]
extract_pred = self.extract(
⋮----
"""Validate and parse FinalOutput. Returns (parsed_outputs, None) or (None, error)."""
raw_output = result.output
⋮----
# Validate raw_output is a dict
⋮----
# Validate all required output fields are present
missing = set(output_field_names) - set(raw_output.keys())
⋮----
# Parse and validate each output field
parsed_outputs = {}
type_errors = []
⋮----
field = self.signature.output_fields[name]
⋮----
"""Process interpreter result, returning Prediction if final, else updated history.

        This shared helper reduces duplication between sync and async execution paths.

        Args:
            pred: The prediction containing reasoning and code attributes
            result: Result from interpreter.execute() - FinalOutput, list, str, or error string
            history: Current REPL history
            output_field_names: List of expected output field names

        Returns:
            Prediction if FINAL was called successfully, else updated REPLHistory
        """
# Strip markdown fences from code for history (format() will re-add them)
code = _strip_code_fences(pred.code)
# Handle error strings from caught exceptions
⋮----
output = self._format_output(result)
⋮----
# Handle FINAL output
⋮----
final_history = history.append(
⋮----
# Format non-final result as output
⋮----
output = "\n".join(map(str, result))
⋮----
output = str(result) if result else ""
⋮----
output = self._format_output(output)
⋮----
"""Execute one iteration. Returns Prediction if done, else updated REPLHistory."""
⋮----
action = self.generate_action(
⋮----
code = _strip_code_fences(action.code)
result = repl.execute(code, variables=dict(input_args))
⋮----
result = f"[Error] {e}"
⋮----
# Public Interface
⋮----
def forward(self, **input_args) -> Prediction
⋮----
"""Execute RLM to produce outputs from the given inputs.

        Args:
            **input_args: Input values matching the signature's input fields

        Returns:
            Prediction with output field(s) from the signature and 'trajectory' for debugging

        Raises:
            ValueError: If required input fields are missing
        """
⋮----
output_field_names = list(self.signature.output_fields.keys())
execution_tools = self._prepare_execution_tools()
variables = self._build_variables(**input_args)
⋮----
history: REPLHistory = REPLHistory()
⋮----
result: Prediction | REPLHistory = self._execute_iteration(
⋮----
history = result
⋮----
# Max iterations reached - use extract fallback
⋮----
"""Async version: Use extract module when max iterations reached."""
⋮----
extract_pred = await self.extract.acall(
⋮----
"""Async version: Execute one iteration."""
⋮----
pred = await self.generate_action.acall(
⋮----
async def aforward(self, **input_args) -> Prediction
⋮----
"""Async version of forward(). Execute RLM to produce outputs.

        Args:
            **input_args: Input values matching the signature's input fields

        Returns:
            Prediction with output field(s) from the signature and 'trajectory' for debugging

        Raises:
            ValueError: If required input fields are missing
        """
⋮----
history = REPLHistory()
⋮----
result = await self._aexecute_iteration(
</file>

<file path="dspy/primitives/__init__.py">
__all__ = [
</file>

<file path="dspy/primitives/base_module.py">
# NOTE: Note: It's important (temporary decision) to maintain named_parameters that's different in behavior from
# named_sub_modules for the time being.
⋮----
logger = logging.getLogger(__name__)
⋮----
class BaseModule
⋮----
def __init__(self)
⋮----
def named_parameters(self)
⋮----
"""
        Unlike PyTorch, handles (non-recursive) lists of parameters too.
        """
⋮----
visited = set()
named_parameters = []
⋮----
def add_parameter(param_name, param_value)
⋮----
# When a sub-module is pre-compiled, keep it frozen.
⋮----
def named_sub_modules(self, type_=None, skip_compiled=False) -> Generator[tuple[str, "BaseModule"], None, None]
⋮----
"""Find all sub-modules in the module, as well as their names.

        Say `self.children[4]['key'].sub_module` is a sub-module. Then the name will be
        `children[4]['key'].sub_module`. But if the sub-module is accessible at different
        paths, only one of the paths will be returned.
        """
⋮----
type_ = BaseModule
⋮----
queue = deque([("self", self)])
seen = {id(self)}
⋮----
def add_to_queue(name, item)
⋮----
def parameters(self)
⋮----
def deepcopy(self)
⋮----
"""Deep copy the module.

        This is a tweak to the default python deepcopy that only deep copies `self.parameters()`, and for other
        attributes, we just do the shallow copy.
        """
⋮----
# If the instance itself is copyable, we can just deep copy it.
# Otherwise we will have to create a new instance and copy over the attributes one by one.
⋮----
# Create an empty instance.
new_instance = self.__class__.__new__(self.__class__)
# Set attribuetes of the copied instance.
⋮----
# Try to deep copy the attribute
⋮----
# Fallback to shallow copy if deep copy fails
⋮----
# If even the shallow copy fails, we just copy over the reference.
⋮----
def reset_copy(self)
⋮----
"""Deep copy the module and reset all parameters."""
new_instance = self.deepcopy()
⋮----
def dump_state(self, json_mode=True)
⋮----
def load_state(self, state)
⋮----
def save(self, path, save_program=False, modules_to_serialize=None)
⋮----
"""Save the module.

        Save the module to a directory or a file. There are two modes:
        - `save_program=False`: Save only the state of the module to a json or pickle file, based on the value of
            the file extension.
        - `save_program=True`: Save the whole module to a directory via cloudpickle, which contains both the state and
            architecture of the model.

        If `save_program=True` and `modules_to_serialize` are provided, it will register those modules for serialization
        with cloudpickle's `register_pickle_by_value`. This causes cloudpickle to serialize the module by value rather
        than by reference, ensuring the module is fully preserved along with the saved program. This is useful
        when you have custom modules that need to be serialized alongside your program. If None, then no modules
        will be registered for serialization.

        We also save the dependency versions, so that the loaded model can check if there is a version mismatch on
        critical dependencies or DSPy version.

        Args:
            path (str): Path to the saved state file, which should be a .json or .pkl file when `save_program=False`,
                and a directory when `save_program=True`.
            save_program (bool): If True, save the whole module to a directory via cloudpickle, otherwise only save
                the state.
            modules_to_serialize (list): A list of modules to serialize with cloudpickle's `register_pickle_by_value`.
                If None, then no modules will be registered for serialization.

        """
metadata = {}
⋮----
path = Path(path)
⋮----
# Create the directory (and any parent directories)
⋮----
modules_to_serialize = modules_to_serialize or []
⋮----
state = self.dump_state()
⋮----
state = self.dump_state(json_mode=False)
⋮----
def load(self, path, allow_pickle=False)
⋮----
"""Load the saved module. You may also want to check out dspy.load, if you want to
        load an entire program, not just the state for an existing program.

        Args:
            path (str): Path to the saved state file, which should be a .json or a .pkl file
            allow_pickle (bool): If True, allow loading .pkl files, which can run arbitrary code.
                This is dangerous and should only be used if you are sure about the source of the file and in a trusted environment.
        """
⋮----
state = orjson.loads(f.read())
⋮----
state = cloudpickle.load(f)
⋮----
dependency_versions = get_dependency_versions()
saved_dependency_versions = state["metadata"]["dependency_versions"]
</file>

<file path="dspy/primitives/code_interpreter.py">
"""
Abstract interpreter interface for code execution environments.

This module defines the CodeInterpreter protocol that allows RLM and other
code-executing modules to work with different interpreter implementations:
- PythonInterpreter: Local Deno/Pyodide WASM interpreter
- MockInterpreter: Scriptable responses for testing
"""
⋮----
# Types that can be used directly in Python function signatures for SUBMIT()
SIMPLE_TYPES = (str, int, float, bool, list, dict, type(None))
⋮----
class CodeInterpreterError(RuntimeError)
⋮----
"""Error raised during code interpretation.

    This exception covers two distinct failure modes:

    1. **Execution errors**: The sandbox ran user code that failed.
       - NameError, TypeError, ValueError, etc.
       - Tool call failures (unknown tool, tool raised exception)
       - These are normal user code errors.

    2. **Protocol errors**: Communication between host and sandbox failed.
       - Malformed JSON from sandbox
       - Sandbox process crashed or became unresponsive
       - Invalid JSON-RPC message structure
       - These may indicate a corrupted sandbox needing restart.

    The error message typically includes the original error type (e.g., "NameError: ...")
    which can help distinguish the failure mode.

    Note: SyntaxError is raised separately (not wrapped) for invalid Python syntax.
    """
⋮----
class FinalOutput
⋮----
"""Returned by interpreter.execute() when SUBMIT() is called.

    This signals that the code execution loop should terminate and return
    the contained output to the caller.
    """
⋮----
def __init__(self, output: Any)
⋮----
def __repr__(self) -> str
⋮----
def __eq__(self, other: object) -> bool
⋮----
@runtime_checkable
class CodeInterpreter(Protocol)
⋮----
"""Protocol for code execution environments (interpreters).

    Implementations must provide:
    - start(): Initialize the interpreter (optional, can be lazy)
    - execute(): Run code and return results
    - shutdown(): Clean up resources

    The interpreter maintains state across execute() calls within a session,
    allowing variables defined in one call to be used in subsequent calls.

    Lifecycle:
        1. Create instance (config only, no resources allocated)
        2. start() - Initialize interpreter (explicit) or let execute() do it (lazy)
        3. execute() - Run code (can be called many times)
        4. shutdown() - Release resources

    Example implementations:
        - LocalInterpreter: Deno/Pyodide WASM interpreter (local)
        - MockInterpreter: Scriptable responses for testing

    Pooling:
        For interpreter pooling, call start() to pre-warm instances, then
        distribute execute() calls across the pool.
    """
⋮----
@property
    def tools(self) -> dict[str, Callable[..., str]]
⋮----
"""Tools available for interpreter code to call.

        Tools are host-side functions that can be invoked from within the
        interpreter. Each tool accepts keyword arguments and returns a string.

        Implementations should accept tools via constructor and expose them
        through this property.
        """
⋮----
def start(self) -> None
⋮----
"""Initialize the interpreter and allocate resources.

        This method prepares the interpreter for code execution. It can be called
        explicitly to pre-warm the interpreter, or implementations may call it
        lazily on first execute().

        For pooling scenarios, call start() on multiple instances to have
        them ready before distributing work.

        Calling start() multiple times should be safe (idempotent).
        """
⋮----
"""Execute Python code and return the result.

        Args:
            code: Python code to execute
            variables: Variables to inject into the namespace before execution.
                      These are available as top-level variables in the code.

        Returns:
            One of:
            - FinalOutput: If SUBMIT() was called in code
            - str: Captured stdout from print() statements
            - list: Multiple output lines
            - None: If no output was produced

        Raises:
            CodeInterpreterError: On runtime errors (undefined vars, tool failures, etc.)
            SyntaxError: On invalid Python syntax

        Note:
            State persists across calls. Variables defined in one execute()
            call are available in subsequent calls until shutdown().

            If start() was not called, implementations should call it lazily.
        """
⋮----
def shutdown(self) -> None
⋮----
"""Release resources and terminate the interpreter session.

        After shutdown(), the interpreter should not be used again.
        A new instance should be created for a fresh session.
        """
</file>

<file path="dspy/primitives/example.py">
class Example
⋮----
"""A flexible data container for DSPy examples and training data.

    The `Example` class is the standard data format used in DSPy evaluation and optimization.

    Key features:
        - Dictionary-like access patterns (item access, iteration, etc.)
        - Flexible initialization from dictionaries, other `Example` instances, or keyword arguments
        - Input/output field separation for training data
        - Serialization support for saving/loading `Example` instances
        - Immutable operations that return new `Example` instances

    Examples:

        Basic usage with keyword arguments:

        ```python
        import dspy

        # Create an example with input and output fields
        example = dspy.Example(
            question="What is the capital of France?",
            answer="Paris",
        )
        print(example.question)  # "What is the capital of France?"
        print(example.answer)   # "Paris"
        ```

        Initialize from a dictionary:

        ```python
        data = {"question": "What is 2+2?", "answer": "4"}
        example = dspy.Example(data)
        print(example["question"])  # "What is 2+2?"
        ```

        Copy from another Example:

        ```python
        original = dspy.Example(question="Hello", answer="World")
        copy = dspy.Example(original)
        print(copy.question)  # "Hello"
        ```

        Working with input/output separation:

        ```python
        # Mark which fields are inputs for training
        example = dspy.Example(
            question="What is the weather?",
            answer="It's sunny",
        ).with_inputs("question")

        # Get only input fields
        inputs = example.inputs()
        print(inputs.question)  # "What is the weather?"

        # Get only output fields (labels)
        labels = example.labels()
        print(labels.answer)  # "It's sunny"
        ```

        Dictionary-like operations:

        ```python
        example = dspy.Example(name="Alice", age=30)

        # Check if key exists
        if "name" in example:
            print("Name field exists")

        # Get with default value
        city = example.get("city", "Unknown")
        print(city)  # "Unknown"
        ```
    """
⋮----
def __init__(self, base=None, **kwargs)
⋮----
"""Initialize an Example instance.

        Args:
            base: Optional base data source. Can be:
                - Another Example instance (copies its data)
                - A dictionary (copies its key-value pairs)
                - None (creates empty Example)
            **kwargs: Additional key-value pairs to store in the Example.
        """
# Internal storage and other attributes
⋮----
# Initialize from a base Example if provided
⋮----
# Initialize from a dict if provided
⋮----
# Update with provided kwargs
⋮----
def __getattr__(self, key)
⋮----
def __setattr__(self, key, value)
⋮----
def __getitem__(self, key)
⋮----
def __setitem__(self, key, value)
⋮----
def __delitem__(self, key)
⋮----
def __contains__(self, key)
⋮----
def __len__(self)
⋮----
def __repr__(self)
⋮----
# return f"Example({self._store})" + f" (input_keys={self._input_keys}, demos={self._demos})"
d = {k: v for k, v in self._store.items() if not k.startswith("dspy_")}
⋮----
def __str__(self)
⋮----
def __eq__(self, other)
⋮----
def __hash__(self)
⋮----
def keys(self, include_dspy=False)
⋮----
def values(self, include_dspy=False)
⋮----
def items(self, include_dspy=False)
⋮----
def get(self, key, default=None)
⋮----
def with_inputs(self, *keys)
⋮----
copied = self.copy()
⋮----
def inputs(self)
⋮----
# return items that are in input_keys
d = {key: self._store[key] for key in self._store if key in self._input_keys}
# return type(self)(d)
new_instance = type(self)(base=d)
new_instance._input_keys = self._input_keys  # Preserve input_keys in new instance
⋮----
def labels(self)
⋮----
# return items that are NOT in input_keys
input_keys = self.inputs().keys()
d = {key: self._store[key] for key in self._store if key not in input_keys}
⋮----
def __iter__(self)
⋮----
def copy(self, **kwargs)
⋮----
def without(self, *keys)
⋮----
def toDict(self):  # noqa: N802
⋮----
def convert_to_serializable(value)
⋮----
# Handle Pydantic models (e.g., dspy.History)
⋮----
serializable_store = {}
</file>

<file path="dspy/primitives/module.py">
logger = logging.getLogger(__name__)
⋮----
class ProgramMeta(type)
⋮----
"""Metaclass ensuring every ``dspy.Module`` instance is properly initialised."""
⋮----
def __call__(cls, *args, **kwargs)
⋮----
# Create the instance without invoking ``__init__`` so we can inject
# the base initialization beforehand.
obj = cls.__new__(cls, *args, **kwargs)
⋮----
# ``_base_init`` sets attributes that should exist on all modules
# even when a subclass forgets to call ``super().__init__``.
⋮----
# Guarantee existence of critical attributes if ``__init__`` didn't
# create them.
⋮----
class Module(BaseModule, metaclass=ProgramMeta)
⋮----
def _base_init(self)
⋮----
def __init__(self, callbacks=None)
⋮----
# LM calling history of the module.
⋮----
def __getstate__(self)
⋮----
state = self.__dict__.copy()
⋮----
def __setstate__(self, state)
⋮----
@with_callbacks
    def __call__(self, *args, **kwargs) -> Prediction
⋮----
caller_modules = settings.caller_modules or []
caller_modules = list(caller_modules)
⋮----
output = self.forward(*args, **kwargs)
tokens = usage_tracker.get_total_tokens()
⋮----
@with_callbacks
    async def acall(self, *args, **kwargs) -> Prediction
⋮----
output = await self.aforward(*args, **kwargs)
⋮----
def named_predictors(self)
⋮----
def predictors(self)
⋮----
def set_lm(self, lm)
⋮----
def get_lm(self)
⋮----
all_used_lms = [param.lm for _, param in self.named_predictors()]
⋮----
def __repr__(self)
⋮----
s = []
⋮----
def map_named_predictors(self, func)
⋮----
"""Applies a function to all named predictors."""
⋮----
def inspect_history(self, n: int = 1)
⋮----
"""
        Processes a list of dspy.Example instances in parallel using the Parallel module.

        Args:
            examples: List of dspy.Example instances to process.
            num_threads: Number of threads to use for parallel processing.
            max_errors: Maximum number of errors allowed before stopping execution.
                If ``None``, inherits from ``dspy.settings.max_errors``.
            return_failed_examples: Whether to return failed examples and exceptions.
            provide_traceback: Whether to include traceback information in error logs.
            disable_progress_bar: Whether to display the progress bar.
            timeout: Seconds before a straggler task is resubmitted. Set to 0 to disable.
            straggler_limit: Only check for stragglers when this many or fewer tasks remain.

        Returns:
            List of results, and optionally failed examples and exceptions.
        """
# Create a list of execution pairs (self, example)
exec_pairs = [(self, example.inputs()) for example in examples]
⋮----
# Create an instance of Parallel
parallel_executor = Parallel(
⋮----
# Execute the forward method of Parallel
⋮----
results = parallel_executor.forward(exec_pairs)
⋮----
def _set_lm_usage(self, tokens: dict[str, Any], output: Any)
⋮----
# Some optimizers (e.g., GEPA bootstrap tracing) temporarily patch
# module.forward to return a tuple: (prediction, trace).
# When usage tracking is enabled, ensure we attach usage to the
# prediction object if present.
prediction_in_output = None
⋮----
prediction_in_output = output
⋮----
prediction_in_output = output[0]
⋮----
def __getattribute__(self, name)
⋮----
attr = super().__getattribute__(name)
⋮----
# Check if forward is called through __call__ or directly
stack = inspect.stack()
forward_called_directly = len(stack) <= 1 or stack[1].function != "__call__"
⋮----
def set_attribute_by_name(obj, name, value)
</file>

<file path="dspy/primitives/prediction.py">
class Prediction(Example)
⋮----
"""A prediction object that contains the output of a DSPy module.
    
    Prediction inherits from Example.
    
    To allow feedback-augmented scores, Prediction supports comparison operations
    (<, >, <=, >=) for Predictions with a `score` field. The comparison operations
    compare the 'score' values as floats. For equality comparison, Predictions are equal
    if their underlying data stores are equal (inherited from Example).
    
    Arithmetic operations (+, /, etc.) are also supported for Predictions with a 'score'
    field, operating on the score value.
    """
⋮----
def __init__(self, *args, **kwargs)
⋮----
def get_lm_usage(self)
⋮----
def set_lm_usage(self, value)
⋮----
@classmethod
    def from_completions(cls, list_or_dict, signature=None)
⋮----
obj = cls()
⋮----
def __repr__(self)
⋮----
store_repr = ",\n    ".join(f"{k}={v!r}" for k, v in self._store.items())
⋮----
num_completions = len(self._completions)
⋮----
def __str__(self)
⋮----
def __float__(self)
⋮----
def __add__(self, other)
⋮----
def __radd__(self, other)
⋮----
def __truediv__(self, other)
⋮----
def __rtruediv__(self, other)
⋮----
def __lt__(self, other)
⋮----
def __le__(self, other)
⋮----
def __gt__(self, other)
⋮----
def __ge__(self, other)
⋮----
@property
    def completions(self)
⋮----
class Completions
⋮----
def __init__(self, list_or_dict, signature=None)
⋮----
kwargs = {}
⋮----
kwargs = list_or_dict
⋮----
length = len(next(iter(kwargs.values())))
⋮----
def items(self)
⋮----
def __getitem__(self, key)
⋮----
def __getattr__(self, name)
⋮----
def __len__(self)
⋮----
# Return the length of the list for one of the keys
# It assumes all lists have the same length
⋮----
def __contains__(self, key)
⋮----
items_repr = ",\n    ".join(f"{k}={v!r}" for k, v in self._completions.items())
⋮----
# return str(self._completions)
</file>

<file path="dspy/primitives/python_interpreter.py">
"""
Local interpreter for secure Python code execution using Deno/Pyodide.

This module provides PythonInterpreter, which runs Python code in a sandboxed
WASM environment using Deno and Pyodide. It implements the Interpreter
protocol defined in interpreter.py.
"""
⋮----
__all__ = ["PythonInterpreter", "FinalOutput", "CodeInterpreterError"]
⋮----
logger = logging.getLogger(__name__)
⋮----
# Pyodide's FFI crashes at exactly 128MB (134,217,728 bytes). Use filesystem
# injection for strings above 100MB to stay safely below this limit.
LARGE_VAR_THRESHOLD = 100 * 1024 * 1024
⋮----
# =============================================================================
# JSON-RPC 2.0 Helpers
⋮----
# JSON-RPC 2.0 protocol errors (reserved range: -32700 to -32600)
JSONRPC_PROTOCOL_ERRORS = {
⋮----
# Application errors (range: -32000 to -32099)
JSONRPC_APP_ERRORS = {
⋮----
def _jsonrpc_request(method: str, params: dict, id: int | str) -> str
⋮----
"""Create a JSON-RPC 2.0 request (expects response)."""
⋮----
def _jsonrpc_notification(method: str, params: dict | None = None) -> str
⋮----
"""Create a JSON-RPC 2.0 notification (no response expected)."""
msg = {"jsonrpc": "2.0", "method": method}
⋮----
def _jsonrpc_result(result: Any, id: int | str) -> str
⋮----
"""Create a JSON-RPC 2.0 success response."""
⋮----
def _jsonrpc_error(code: int, message: str, id: int | str, data: dict | None = None) -> str
⋮----
"""Create a JSON-RPC 2.0 error response."""
err = {"code": code, "message": message}
⋮----
class PythonInterpreter
⋮----
"""Local interpreter for secure Python execution using Deno and Pyodide.

    Implements the Interpreter protocol for secure code execution in a
    WASM-based sandbox. Code runs in an isolated Pyodide environment with
    no access to the host filesystem, network, or environment by default.

    Prerequisites:
        Deno must be installed: https://docs.deno.com/runtime/getting_started/installation/

    Example:
        ```python
        # Basic execution
        with PythonInterpreter() as interp:
            result = interp("print(1 + 2)")  # Returns "3"

        # With host-side tools
        def my_tool(question: str) -> str:
            return "answer"

        with PythonInterpreter(tools={"my_tool": my_tool}) as interp:
            result = interp("print(my_tool(question='test'))")
        ```
    """
⋮----
"""
        Args:
            deno_command: command list to launch Deno.
            enable_read_paths: Files or directories to allow reading from in the sandbox.
            enable_write_paths: Files or directories to allow writing to in the sandbox.
                All write paths will also be able to be read from for mounting.
            enable_env_vars: Environment variable names to allow in the sandbox.
            enable_network_access: Domains or IPs to allow network access in the sandbox.
            sync_files: If set, syncs changes within the sandbox back to original files after execution.
            tools: Dictionary mapping tool names to callable functions.
                   Each function should accept keyword arguments and return a string.
                   Tools are callable directly from sandbox code by name.
            output_fields: List of output field definitions for typed SUBMIT signature.
                   Each dict should have 'name' and optionally 'type' keys.
        """
⋮----
# TODO later on add enable_run (--allow-run) by proxying subprocess.run through Deno.run() to fix 'emscripten does not support processes' error
⋮----
args = ["deno", "run"]
⋮----
# Allow reading runner.js and explicitly enabled paths
allowed_read_paths = [self._get_runner_path()]
⋮----
# Also allow reading Deno's cache directory so Pyodide can load its files
deno_dir = self._get_deno_dir()
⋮----
user_vars = [str(v).strip() for v in self.enable_env_vars]
⋮----
# For runner.js to load in env vars
⋮----
def _check_thread_ownership(self) -> None
⋮----
"""Ensure this interpreter is only used from a single thread."""
current_thread = threading.current_thread().ident
⋮----
@staticmethod
@functools.lru_cache(maxsize=1)
    def _get_deno_dir() -> str | None
⋮----
result = subprocess.run(
⋮----
info = json.loads(result.stdout)
⋮----
def _get_runner_path(self) -> str
⋮----
current_dir = os.path.dirname(os.path.abspath(__file__))
⋮----
def _mount_files(self)
⋮----
paths_to_mount = []
⋮----
virtual_path = f"/sandbox/{os.path.basename(path)}"
⋮----
def _sync_files(self)
⋮----
sync_msg = _jsonrpc_notification("sync_file", {"virtual_path": virtual_path, "host_path": str(path)})
⋮----
def _extract_parameters(self, fn: Callable) -> list[dict]
⋮----
"""Extract parameter info from a callable for sandbox registration."""
sig = inspect.signature(fn)
params = []
⋮----
p = {"name": name}
# Only include type for simple types that work in function signatures
# Complex types like Union, Optional, etc. are not included
⋮----
def _register_tools(self) -> None
⋮----
"""Register tools and output fields with the sandbox."""
⋮----
# Build registration params with typed tool signatures
params = {}
⋮----
tools_info = []
⋮----
# Skip if nothing to register
⋮----
def _handle_tool_call(self, request: dict) -> None
⋮----
"""Handle a tool call request from the sandbox."""
request_id = request["id"]
params = request.get("params", {})
tool_name = params.get("name")
args = params.get("args", [])
kwargs = params.get("kwargs", {})
⋮----
result = self.tools[tool_name](*args, **kwargs)
is_json = isinstance(result, (list, dict))
response = _jsonrpc_result(
⋮----
error_type = type(e).__name__
error_code = JSONRPC_APP_ERRORS.get(error_type, JSONRPC_APP_ERRORS["Unknown"])
response = _jsonrpc_error(error_code, str(e), request_id, {"type": error_type})
⋮----
def _ensure_deno_process(self) -> None
⋮----
install_instructions = (
⋮----
def _send_request(self, method: str, params: dict, context: str) -> dict
⋮----
"""Send a JSON-RPC request and return the parsed response."""
⋮----
request_id = self._request_id
msg = _jsonrpc_request(method, params, request_id)
⋮----
response_line = self.deno_process.stdout.readline().strip()
⋮----
exit_code = self.deno_process.poll()
⋮----
stderr = self.deno_process.stderr.read() if self.deno_process.stderr else ""
⋮----
response = json.loads(response_line)
⋮----
def _health_check(self) -> None
⋮----
"""Verify the subprocess is alive by executing a simple expression."""
response = self._send_request("execute", {"code": "print(1+1)"}, "during health check")
⋮----
def _to_json_compatible(self, value: Any) -> Any
⋮----
"""Recursively convert Python values to JSON-compatible types."""
⋮----
def _inject_variables(self, code: str, variables: dict[str, Any]) -> str
⋮----
"""Insert Python assignments for each variable at the top of the code."""
⋮----
large_vars = {}
small_assignments = []
⋮----
serialized = self._serialize_value(v)
⋮----
large_assignments = [f"{k} = json.loads(open('/tmp/dspy_vars/{k}.json').read())" for k in large_vars]
assignments = ["import json"] + small_assignments + large_assignments
⋮----
assignments = small_assignments
⋮----
def _serialize_value(self, value: Any) -> str
⋮----
"""Serialize a Python value to a Python literal string for injection.

        Sets and tuples are converted to lists for JSON round-trip compatibility,
        since the sandbox returns values via JSON which doesn't support these types.
        """
⋮----
# Must check bool before int since bool is a subclass of int
⋮----
# Tuples become lists for JSON compatibility
items = ", ".join(self._serialize_value(item) for item in value)
⋮----
items = ", ".join(
⋮----
# Sets become sorted lists (or unsorted if mixed types) for JSON compatibility
⋮----
sorted_items = sorted(value)
⋮----
sorted_items = list(value)
items = ", ".join(self._serialize_value(item) for item in sorted_items)
⋮----
def _inject_large_var(self, name: str, value: str) -> None
⋮----
"""Inject a large variable via the virtual filesystem."""
⋮----
variables = variables or {}
code = self._inject_variables(code, variables)
⋮----
# Send the code as JSON-RPC request
⋮----
execute_request_id = self._request_id
input_data = _jsonrpc_request("execute", {"code": code}, execute_request_id)
⋮----
# If the process died, restart and try again once
⋮----
# Read and handle messages until we get the final output.
# Loop is needed because tool calls require back-and-forth communication.
⋮----
output_line = self.deno_process.stdout.readline().strip()
⋮----
# Possibly the subprocess died or gave no output
err_output = self.deno_process.stderr.read()
⋮----
# Skip non-JSON lines (e.g., Pyodide package loading messages)
⋮----
# Parse that line as JSON
⋮----
msg = json.loads(output_line)
⋮----
# Malformed JSON starting with '{' - log and continue
⋮----
# Handle incoming requests (tool calls from sandbox)
⋮----
# Handle success response
⋮----
result = msg["result"]
⋮----
# Check for SUBMIT (encoded as success with "final" field)
⋮----
# Handle error response
⋮----
# Errors with id=null are unsolicited errors (e.g., unhandled async rejections)
# Treat them as errors for the current request
⋮----
error = msg["error"]
error_code = error.get("code", JSONRPC_APP_ERRORS["Unknown"])
error_message = error.get("message", "Unknown error")
error_data = error.get("data", {})
error_type = error_data.get("type", "Error")
⋮----
# Unexpected message format - neither a recognized method nor a response
⋮----
def start(self) -> None
⋮----
"""Initialize the Deno/Pyodide sandbox.

        This pre-warms the sandbox by starting the Deno subprocess.
        Can be called explicitly for pooling, or will be called lazily
        on first execute().

        Idempotent: safe to call multiple times.
        """
⋮----
def __enter__(self)
⋮----
def __exit__(self, *_)
⋮----
def shutdown(self) -> None
</file>

<file path="dspy/primitives/repl_types.py">
"""
REPL data types for RLM and interpreter interactions.

These types represent the state and history of REPL-based execution:
- REPLVariable: Metadata about variables available in the REPL
- REPLEntry: A single interaction (reasoning, code, output)
- REPLHistory: Container for the full interaction history
"""
⋮----
__all__ = ["REPLVariable", "REPLEntry", "REPLHistory"]
⋮----
class REPLVariable(pydantic.BaseModel)
⋮----
"""Metadata about a variable available in the REPL environment."""
⋮----
name: str
type_name: str
desc: str = ""
constraints: str = ""
total_length: int
preview: str
⋮----
model_config = pydantic.ConfigDict(frozen=True)
⋮----
"""Create REPLVariable from an actual value and optional field info.

        Args:
            name: Variable name
            value: The actual value
            field_info: Optional pydantic FieldInfo with desc/constraints metadata
            preview_chars: Max characters for preview
        """
jsonable = serialize_for_json(value)
⋮----
value_str = json.dumps(jsonable, indent=2)
⋮----
value_str = str(jsonable)
is_truncated = len(value_str) > preview_chars
preview = value_str[:preview_chars] + ("..." if is_truncated else "")
⋮----
# Extract desc and constraints from field_info if provided
desc = ""
constraints = ""
⋮----
raw_desc = field_info.json_schema_extra.get("desc", "")
# Skip placeholder descs like "${name}"
⋮----
desc = raw_desc
constraints = field_info.json_schema_extra.get("constraints", "")
⋮----
def format(self) -> str
⋮----
"""Format variable metadata for prompt inclusion."""
lines = [f"Variable: `{self.name}` (access it in your code)"]
⋮----
@pydantic.model_serializer()
    def serialize_model(self) -> str
⋮----
class REPLEntry(pydantic.BaseModel)
⋮----
"""A single REPL interaction entry containing reasoning, code, and output."""
⋮----
reasoning: str = ""
code: str
output: str
⋮----
def format(self, index: int, max_output_chars: int = 5000) -> str
⋮----
"""Format this entry for inclusion in prompts."""
output = self.output
⋮----
output = output[:max_output_chars] + f"\n... (truncated to {max_output_chars}/{len(self.output):,} chars)"
reasoning_line = f"Reasoning: {self.reasoning}\n" if self.reasoning else ""
⋮----
class REPLHistory(pydantic.BaseModel)
⋮----
"""Container for REPL interaction history.

    Immutable: append() returns a new instance with the entry added.
    """
⋮----
entries: list[REPLEntry] = Field(default_factory=list)
⋮----
def format(self, max_output_chars: int = 5000) -> str
⋮----
def append(self, *, reasoning: str = "", code: str, output: str) -> REPLHistory
⋮----
"""Return a new REPLHistory with the entry appended."""
new_entry = REPLEntry(reasoning=reasoning, code=code, output=output)
⋮----
def __len__(self) -> int
⋮----
def __iter__(self) -> Iterator[REPLEntry]
⋮----
def __bool__(self) -> bool
</file>

<file path="dspy/primitives/runner.js">
// Adapted from "Simon Willison's TILs" (https://til.simonwillison.net/deno/pyodide-sandbox)
⋮----
// =============================================================================
// Python Code Templates
// =============================================================================
⋮----
// Setup code run before each user code execution.
// Captures stdout, defines SUBMIT for early termination, and
// provides a helper to extract exception args across the JS/Python boundary.
⋮----
// Generate a tool wrapper function with typed signature.
// Parameters is an array of {name, type?, default?} objects.
// Convert a JavaScript/JSON value to Python literal syntax
const toPythonLiteral = (value) =>
⋮----
return JSON.stringify(value);  // Works for strings, numbers, arrays, objects
⋮----
const makeToolWrapper = (toolName, parameters = []) =>
⋮----
// Build signature parts: "query: str, limit: int = 10"
⋮----
// If no parameters, fall back to *args, **kwargs for flexibility
⋮----
// Generate SUBMIT function with output field signature.
// Outputs is an array of {name, type?} objects.
const makeSubmitWrapper = (outputs) =>
⋮----
// Fallback to single-arg SUBMIT if no outputs defined
⋮----
// =============================================================================
// JSON-RPC 2.0 Helpers
// =============================================================================
⋮----
// JSON-RPC 2.0 protocol errors (reserved range: -32700 to -32600)
⋮----
// Application errors (range: -32000 to -32099)
⋮----
const jsonrpcRequest = (method, params, id)
⋮----
const jsonrpcNotification = (method, params = null) =>
⋮----
const jsonrpcResult = (result, id)
⋮----
const jsonrpcError = (code, message, id, data = null) =>
⋮----
// Global handler to prevent uncaught promise rejections from crashing Deno
// These can occur during async Python <-> JS interop
⋮----
// Tool call support: allows Python code to call host-side functions
// The stdin reader is shared so tool_call can read responses during execution
⋮----
// This function is called from Python to invoke a host-side tool
async function toolCallBridge(name, argsJson)
⋮----
// Parse args to extract positional and keyword args
⋮----
// Send tool call request to host using JSON-RPC
⋮----
// Wait for response from host
⋮----
// Expect JSON-RPC result or error with matching id
⋮----
// Deserialize result based on type
⋮----
// Re-throw with context so Python can catch it properly
⋮----
// Expose the bridge to Python
⋮----
// Main loop using shared stdin reader
⋮----
// JSON-RPC parse error
⋮----
// Validate JSON-RPC format
⋮----
const requestId = input.id; // May be undefined for notifications
⋮----
// Handle notifications (no response expected)
⋮----
} catch (e) { /* ignore sync errors */ }
⋮----
// Handle requests (expect response)
⋮----
// Check if directory exists before creating
⋮----
// Directory exists, continue to next
⋮----
// Directory doesn't exist, create it
⋮----
// Register tools with typed signatures
⋮----
// Support both old format (string) and new format (object with parameters)
⋮----
// Register SUBMIT with output signature
⋮----
try { pyodide.FS.mkdir('/tmp'); } catch (e) { /* exists */ }
try { pyodide.FS.mkdir('/tmp/dspy_vars'); } catch (e) { /* exists */ }
⋮----
let setupCompleted = false;  // Track if PYTHON_SETUP_CODE ran successfully
⋮----
setupCompleted = true;  // Mark setup as complete - old_stdout/old_stderr now exist
⋮----
// Run the user's code
⋮----
// If result is None, output prints; otherwise output the result
⋮----
// We have an error => check if it's a SyntaxError or something else
// The Python error class name is stored in error.type: https://pyodide.org/en/stable/usage/api/js-api.html#pyodide.ffi.PythonError
⋮----
// error.message is mostly blank.
⋮----
// Handle FinalOutput as a success result, not an error
⋮----
// Get error args for other exception types
⋮----
// Only python exceptions have args.
⋮----
// Regarding https://pyodide.org/en/stable/usage/type-conversions.html#type-translations-errors,
// we do a additional `json.dumps` and `JSON.parse` on the values, to avoid the possible memory leak.
⋮----
// Map error type to JSON-RPC error code
⋮----
// Always restore stdout/stderr if setup completed, even after errors.
// This prevents stream corruption where subsequent executions capture
// StringIO buffers as old_stdout/old_stderr instead of real streams.
⋮----
// Ignore restoration errors to avoid masking the original error
⋮----
// Unknown method
</file>

<file path="dspy/propose/__init__.py">
__all__ = [
</file>

<file path="dspy/propose/dataset_summary_generator.py">
class ObservationSummarizer(dspy.Signature)
⋮----
observations = dspy.InputField(desc="Observations I have made about my dataset")
summary = dspy.OutputField(desc="Two to Three sentence summary of only the most significant highlights of my observations")
⋮----
class DatasetDescriptor(dspy.Signature)
⋮----
examples = dspy.InputField(desc="Sample data points from the dataset")
observations = dspy.OutputField(desc="Somethings that holds true for most or all of the data you observed")
⋮----
class DatasetDescriptorWithPriorObservations(dspy.Signature)
⋮----
prior_observations = dspy.InputField(desc="Some prior observations I made about the data")
observations = dspy.OutputField(desc="Somethings that holds true for most or all of the data you observed or COMPLETE if you have nothing to add")
⋮----
def order_input_keys_in_string(unordered_repr)
⋮----
# Regex pattern to match the input keys structure
pattern = r"input_keys=\{([^\}]+)\}"
⋮----
# Function to reorder keys
def reorder_keys(match)
⋮----
# Extracting the keys from the match
keys_str = match.group(1)
# Splitting the keys, stripping extra spaces, and sorting them
keys = sorted(key.strip() for key in keys_str.split(","))
# Formatting the sorted keys back into the expected structure
⋮----
# Using re.sub to find all matches of the pattern and replace them using the reorder_keys function
ordered_repr = re.sub(pattern, reorder_keys, unordered_repr)
⋮----
def create_dataset_summary(trainset, view_data_batch_size, prompt_model, log_file=None, verbose=False)
⋮----
upper_lim = min(len(trainset), view_data_batch_size)
prompt_model = prompt_model if prompt_model else dspy.settings.lm
⋮----
observation = dspy.Predict(DatasetDescriptor, n=1, temperature=1.0)(examples=order_input_keys_in_string(trainset[0:upper_lim].__repr__()))
observations = observation["observations"]
⋮----
skips = 0
⋮----
max_calls = 10
calls = 0
⋮----
upper_lim = min(len(trainset), b+view_data_batch_size)
⋮----
output = dspy.Predict(DatasetDescriptorWithPriorObservations, n=1, temperature=1.0)(prior_observations=observations, examples=order_input_keys_in_string(trainset[b:upper_lim].__repr__()))
⋮----
summary = dspy.Predict(ObservationSummarizer, n=1, temperature=1.0)(observations=observations)
</file>

<file path="dspy/propose/grounded_proposer.py">
# Hardcoded variables (TODO: update)
MAX_INSTRUCT_IN_HISTORY = 5  # 10
⋮----
TIPS = {
⋮----
### SIGNATURES USED TO HELP WITH INSTRUCTION GENERATION ###
⋮----
class DescribeProgram(dspy.Signature)
⋮----
program_code = dspy.InputField(
program_example = dspy.InputField(
program_description = dspy.OutputField(
⋮----
class DescribeModule(dspy.Signature)
⋮----
program_description = dspy.InputField(
module = dspy.InputField(
module_description = dspy.OutputField(
⋮----
class GenerateSingleModuleInstruction(dspy.Signature)
⋮----
dataset_description = dspy.InputField(
⋮----
module_description = dspy.InputField(
task_demos = dspy.InputField(
⋮----
previous_instructions = dspy.InputField(
basic_instruction = dspy.InputField(
⋮----
tip = dspy.InputField(
proposed_instruction = dspy.OutputField(
⋮----
### CLASS RESPONSIBLE FOR GENERATING A NEW INSTRUCTION, USING THE HELPER SIGNATURES ABOVE ###
⋮----
class GenerateModuleInstruction(dspy.Module)
⋮----
def gather_examples_from_sets(candidate_sets, max_examples)
⋮----
"""Helper function to gather up to augmented examples from given sets."""
count = 0
⋮----
fields_to_use = get_signature(program.predictors()[pred_i]).fields
⋮----
# Construct full program demo or single module demo depending on settings
basic_instruction = get_signature(program.predictors()[pred_i]).instructions
task_demos = ""
⋮----
# Combine current and adjacent sets
adjacent_sets = (
⋮----
# Gather examples up to the required count
example_strings = gather_examples_from_sets(adjacent_sets, num_demos_in_context)
task_demos = "\n\n".join(example_strings) + "\n\n"
⋮----
# Default to no demos provided if no examples were gathered, or if we're using the first demo set
⋮----
task_demos = "No task demos provided."
⋮----
# Summarize the program
program_description = "Not available"
module_code = "Not provided"
module_description = "Not provided"
⋮----
program_description = strip_prefix(
⋮----
inputs = []
outputs = []
⋮----
# Access the '__dspy_field_type' from the extra metadata
dspy_field_type = field.json_schema_extra.get("__dspy_field_type")
⋮----
# Based on the '__dspy_field_type', append to the respective list
⋮----
module_code = f"{program.predictors()[pred_i].__class__.__name__}({', '.join(inputs)}) -> {', '.join(outputs)}"
⋮----
module_description = self.describe_module(
⋮----
# Generate an instruction for our chosen module
⋮----
instruct = self.generate_module_instruction(
⋮----
proposed_instruction = strip_prefix(instruct.proposed_instruction)
⋮----
### CLASS USED TO GENERATE THE FULL SET OF INSTRUCTIONS GIVEN THE SPECIFIED CRITERIA ###
⋮----
class GroundedProposer(Proposer)
⋮----
N, # noqa: N803
⋮----
"""This method is responsible for returning the full set of new instructions for our program, given the specified criteria."""
⋮----
proposed_instructions = {}
⋮----
# Randomly select whether or not we're using instruction history
use_history = self.rng.random() < 0.5
⋮----
# When no demo candidates are provided, default to N
num_demos = N
⋮----
num_demos = max(len(demo_candidates[0]), 1)
⋮----
# Create an instruction for each predictor
⋮----
selected_tip = None
⋮----
# Randomly select the tip
selected_tip_key = self.rng.choice(list(TIPS.keys()))
selected_tip = TIPS[selected_tip_key]
⋮----
"""This method is responsible for returning a single instruction for a given predictor, using the specified criteria."""
⋮----
# Create an instruction history string for our predictor
instruction_history = create_predictor_level_history_string(
⋮----
# Create our instruction generator class (given specific criteria for this round of proposal)
instruction_generator = GenerateModuleInstruction(
⋮----
# Generate a new instruction for our predictor using a unique rollout id to bypass cache
rollout_lm = self.prompt_model.copy(
⋮----
proposed_instruction = instruction_generator(
⋮----
# Log the trace used to generate the new instruction, along with the new instruction itself
</file>

<file path="dspy/propose/propose_base.py">
class Proposer(ABC)
⋮----
def __init__(self)
⋮----
@abstractmethod
    def propose_instructions_for_program(self)
⋮----
def propose_instruction_for_predictor(self)
</file>

<file path="dspy/propose/utils.py">
# Won't be able to read code from jupyter notebooks
extract_symbols = None
⋮----
def strip_prefix(text)
⋮----
pattern = r"^[\*\s]*(([\w\'\-]+\s+){0,4}[\w\'\-]+):\s*"
modified_text = re.sub(pattern, "", text)
⋮----
def create_instruction_set_history_string(base_program, trial_logs, top_n)
⋮----
program_history = []
⋮----
trial = trial_logs[trial_num]
⋮----
trial_program = base_program.deepcopy()
⋮----
# Deduplicate program history based on the program's instruction set
seen_programs = set()
unique_program_history = []
⋮----
program = entry["program"]
instruction_set = get_program_instruction_set_string(program)
⋮----
# Get the top n programs from program history
top_n_program_history = sorted(unique_program_history, key=lambda x: x["score"], reverse=True)[:top_n]
⋮----
# Create formatted string
instruction_set_history_string = ""
⋮----
score = entry["score"]
⋮----
def parse_list_of_instructions(instruction_string)
⋮----
# Try to convert the string representation of a list to an actual list using JSON
⋮----
instructions = json.loads(instruction_string)
⋮----
# If JSON decoding fails, extract strings within quotes
instructions = re.findall(r'"([^"]*)"', instruction_string)
⋮----
def get_program_instruction_set_string(program)
⋮----
instruction_list = []
⋮----
pred_instructions = get_signature(pred).instructions
⋮----
# Joining the list into a single string that looks like a list
⋮----
def create_predictor_level_history_string(base_program, predictor_i, trial_logs, top_n)
⋮----
instruction_aggregate = {}
instruction_history = []
⋮----
# Load trial programs
⋮----
# Aggregate scores for each instruction
⋮----
predictor = history_item["program"].predictors()[predictor_i]
instruction = get_signature(predictor).instructions
score = history_item["score"]
⋮----
# Calculate average score for each instruction and prepare for sorting
predictor_history = []
⋮----
average_score = data["total_score"] / data["count"]
⋮----
# Deduplicate and sort by average score, then select top N
seen_instructions = set()
unique_predictor_history = []
⋮----
top_instructions = sorted(unique_predictor_history, key=lambda x: x[1], reverse=True)[:top_n]
⋮----
# Create formatted history string
predictor_history_string = ""
⋮----
def create_example_string(fields, example)
⋮----
# Building the output string
output = []
⋮----
name = field_values.json_schema_extra["prefix"]
⋮----
# Determine the value from input_data or prediction_data
value = example.get(field_name)
⋮----
# Construct the string for the current field
field_str = f"{name} {value}"
⋮----
# Joining all the field strings
⋮----
def get_dspy_source_code(module)
⋮----
header = []
base_code = ""
⋮----
# Don't get source code for Predict or ChainOfThought modules (NOTE we will need to extend this list as more DSPy.modules are added)
# TODO: if type(module).__name__ not in ["Predict", "ChainOfThought", "ReAct"]:
⋮----
base_code = inspect.getsource(type(module))
⋮----
obj = type(module)
cell_code = "".join(inspect.linecache.getlines(new_getfile(obj)))
class_code = extract_symbols(cell_code, obj.__name__)[0][0]
base_code = str(class_code)
⋮----
completed_set = set()
⋮----
iterable = iter(getattr(module, attribute))
⋮----
iterable = [getattr(module, attribute)]
⋮----
# Skip items that are unhashable (like module history)
⋮----
code = get_dspy_source_code(item).strip()
</file>

<file path="dspy/retrievers/__init__.py">
__all__ = ["Embeddings", "Retrieve"]
</file>

<file path="dspy/retrievers/databricks_rm.py">
_databricks_sdk_installed = find_spec("databricks.sdk") is not None
⋮----
@dataclass
class Document
⋮----
page_content: str
metadata: dict[str, Any]
type: str
⋮----
def to_dict(self) -> dict[str, Any]
⋮----
class DatabricksRM(dspy.Retrieve)
⋮----
"""
    A retriever module that uses a Databricks Mosaic AI Vector Search Index to return the top-k
    embeddings for a given query.

    Examples:
        Below is a code snippet that shows how to set up a Databricks Vector Search Index
        and configure a DatabricksRM DSPy retriever module to query the index.

        (example adapted from "Databricks: How to create and query a Vector Search Index:
        https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-index)

        ```python
        from databricks.vector_search.client import VectorSearchClient

        # Create a Databricks Vector Search Endpoint
        client = VectorSearchClient()
        client.create_endpoint(
            name="your_vector_search_endpoint_name",
            endpoint_type="STANDARD"
        )

        # Create a Databricks Direct Access Vector Search Index
        index = client.create_direct_access_index(
            endpoint_name="your_vector_search_endpoint_name",
            index_name="your_index_name",
            primary_key="id",
            embedding_dimension=1024,
            embedding_vector_column="text_vector",
            schema={
              "id": "int",
              "field2": "str",
              "field3": "float",
              "text_vector": "array<float>"
            }
        )

        # Create a DatabricksRM retriever module to query the Databricks Direct Access Vector
        # Search Index
        retriever = DatabricksRM(
            databricks_index_name = "your_index_name",
            docs_id_column_name="id",
            text_column_name="field2",
            k=3
        )
        ```

        Below is a code snippet that shows how to query the Databricks Direct Access Vector
        Search Index using the DatabricksRM retriever module:

        ```python
        retrieved_results = DatabricksRM(query="Example query text"))
        ```
    """
⋮----
"""
        Args:
            databricks_index_name (str): The name of the Databricks Vector Search Index to query.
            databricks_endpoint (Optional[str]): The URL of the Databricks Workspace containing
                the Vector Search Index. Defaults to the value of the ``DATABRICKS_HOST``
                environment variable. If unspecified, the Databricks SDK is used to identify the
                endpoint based on the current environment.
            databricks_token (Optional[str]): The Databricks Workspace authentication token to use
                when querying the Vector Search Index. Defaults to the value of the
                ``DATABRICKS_TOKEN`` environment variable. If unspecified, the Databricks SDK is
                used to identify the token based on the current environment.
            databricks_client_id (str): Databricks service principal id. If not specified,
                the token is resolved from the current environment (DATABRICKS_CLIENT_ID).
            databricks_client_secret (str): Databricks service principal secret. If not specified,
                the endpoint is resolved from the current environment (DATABRICKS_CLIENT_SECRET).
            columns (Optional[list[str]]): Extra column names to include in response,
                in addition to the document id and text columns specified by
                ``docs_id_column_name`` and ``text_column_name``.
            filters_json (Optional[str]): A JSON string specifying additional query filters.
                Example filters: ``{"id <": 5}`` selects records that have an ``id`` column value
                less than 5, and ``{"id >=": 5, "id <": 10}`` selects records that have an ``id``
                column value greater than or equal to 5 and less than 10.
            k (int): The number of documents to retrieve.
            docs_id_column_name (str): The name of the column in the Databricks Vector Search Index
                containing document IDs.
            docs_uri_column_name (Optional[str]): The name of the column in the Databricks Vector Search Index
                containing document URI.
            text_column_name (str): The name of the column in the Databricks Vector Search Index
                containing document text to retrieve.
            use_with_databricks_agent_framework (bool): Whether to use the `DatabricksRM` in a way that is
                compatible with the Databricks Mosaic Agent Framework.
        """
⋮----
def _extract_doc_ids(self, item: dict[str, Any]) -> str
⋮----
"""Extracts the document id from a search result

        Args:
            item: dict[str, Any]: a record from the search results.
        Returns:
            str: document id.
        """
⋮----
docs_dict = json.loads(item["metadata"])
⋮----
def _get_extra_columns(self, item: dict[str, Any]) -> dict[str, Any]
⋮----
"""Extracts search result column values, excluding the "text" and not "id" columns

        Args:
            item: dict[str, Any]: a record from the search results.
        Returns:
            dict[str, Any]: Search result column values, excluding the "text", "id" and "uri" columns.
        """
extra_columns = {
⋮----
"""
        Retrieve documents from a Databricks Mosaic AI Vector Search Index that are relevant to the
        specified query.

        Args:
            query (Union[str, list[float]]): The query text or numeric query vector for which to
                retrieve relevant documents.
            query_type (str): The type of search query to perform against the Databricks Vector
                Search Index. Must be either 'ANN' (approximate nearest neighbor) or 'HYBRID'
                (hybrid search).
            filters_json (Optional[str]): A JSON string specifying additional query filters.
                Example filters: ``{"id <": 5}`` selects records that have an ``id`` column value
                less than 5, and ``{"id >=": 5, "id <": 10}`` selects records that have an ``id``
                column value greater than or equal to 5 and less than 10. If specified, this
                parameter overrides the `filters_json` parameter passed to the constructor.

        Returns:
            A list of dictionaries when ``use_with_databricks_agent_framework`` is ``True``,
            or a ``dspy.Prediction`` object when ``use_with_databricks_agent_framework`` is
            ``False``.
        """
⋮----
# Older versions of DSPy used a `query_type` argument to disambiguate between text
# and vector queries, rather than checking the type of the `query` argument. This
# differs from the Databricks Vector Search definition of `query_type`, which
# specifies the search algorithm to use (e.g. "ANN" or "HYBRID"). To maintain
# backwards compatibility with older versions of DSPy, we map the old `query_type`
# values to the Databricks Vector Search default query type of "ANN".
query_type = "ANN"
⋮----
query_text = query
query_vector = None
⋮----
query_vector = query
query_text = None
⋮----
results = self._query_via_databricks_sdk(
⋮----
results = self._query_via_requests(
⋮----
# Checking if defined columns are present in the index columns
col_names = [column["name"] for column in results["manifest"]["columns"]]
⋮----
# Extracting the results
items = []
⋮----
item = {}
⋮----
# Sorting results by score in descending order
sorted_docs = sorted(items, key=lambda x: x["score"], reverse=True)[: self.k]
⋮----
# Returning the prediction
⋮----
"""
        Query a Databricks Vector Search Index via the Databricks SDK.
        Assumes that the databricks-sdk Python library is installed.

        Args:
            index_name (str): Name of the Databricks vector search index to query
            k (int): Number of relevant documents to retrieve.
            columns (list[str]): Column names to include in response.
            query_text (Optional[str]): Text query for which to find relevant documents. Exactly
                one of query_text or query_vector must be specified.
            query_vector (Optional[list[float]]): Numeric query vector for which to find relevant
                documents. Exactly one of query_text or query_vector must be specified.
            filters_json (Optional[str]): JSON string representing additional query filters.
            databricks_token (str): Databricks authentication token. If not specified,
                the token is resolved from the current environment.
            databricks_endpoint (str): Databricks index endpoint url. If not specified,
                the endpoint is resolved from the current environment.
            databricks_client_id (str): Databricks service principal id. If not specified,
                the token is resolved from the current environment (DATABRICKS_CLIENT_ID).
            databricks_client_secret (str): Databricks service principal secret. If not specified,
                the endpoint is resolved from the current environment (DATABRICKS_CLIENT_SECRET).
        Returns:
        Returns:
            dict[str, Any]: Parsed JSON response from the Databricks Vector Search Index query.
        """
⋮----
# Use client ID and secret for authentication if they are provided
databricks_client = WorkspaceClient(
⋮----
# Fallback for token-based authentication
⋮----
"""
        Query a Databricks Vector Search Index via the Python requests library.

        Args:
            index_name (str): Name of the Databricks vector search index to query
            k (int): Number of relevant documents to retrieve.
            columns (list[str]): Column names to include in response.
            databricks_token (str): Databricks authentication token.
            databricks_endpoint (str): Databricks index endpoint url.
            query_text (Optional[str]): Text query for which to find relevant documents. Exactly
                one of query_text or query_vector must be specified.
            query_vector (Optional[list[float]]): Numeric query vector for which to find relevant
                documents. Exactly one of query_text or query_vector must be specified.
            filters_json (Optional[str]): JSON string representing additional query filters.

        Returns:
            dict[str, Any]: Parsed JSON response from the Databricks Vector Search Index query.
        """
⋮----
headers = {
payload = {
⋮----
response = requests.post(
results = response.json()
</file>

<file path="dspy/retrievers/embeddings.py">
class Embeddings
⋮----
def __call__(self, query: str)
⋮----
def forward(self, query: str)
⋮----
def _batch_forward(self, queries: list[str])
⋮----
q_embeds = self.embedder(queries)
q_embeds = self._normalize(q_embeds) if self.normalize else q_embeds
⋮----
pids = self._faiss_search(q_embeds, self.k * 10) if self.index else None
pids = np.tile(np.arange(len(self.corpus)), (len(queries), 1)) if pids is None else pids
⋮----
def _build_faiss(self)
⋮----
nbytes = 32
partitions = int(2 * np.sqrt(len(self.corpus)))
dim = self.corpus_embeddings.shape[1]
⋮----
quantizer = faiss.IndexFlatL2(dim)
index = faiss.IndexIVFPQ(quantizer, dim, partitions, nbytes, 8)
⋮----
def _faiss_search(self, query_embeddings: np.ndarray, num_candidates: int)
⋮----
def _rerank_and_predict(self, q_embeds: np.ndarray, candidate_indices: np.ndarray)
⋮----
candidate_embeddings = self.corpus_embeddings[candidate_indices]
scores = np.einsum("qd,qkd->qk", q_embeds, candidate_embeddings)
⋮----
top_k_indices = np.argsort(-scores, axis=1)[:, : self.k]
top_indices = candidate_indices[np.arange(len(q_embeds))[:, None], top_k_indices]
⋮----
return [([self.corpus[idx] for idx in indices], [idx for idx in indices]) for indices in top_indices]  # noqa: C416
⋮----
def _normalize(self, embeddings: np.ndarray)
⋮----
norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
⋮----
def save(self, path: str)
⋮----
"""
        Save the embeddings index to disk.

        This saves the corpus, embeddings, FAISS index (if present), and configuration
        to allow for fast loading without recomputing embeddings.

        Args:
            path: Directory path where the embeddings will be saved
        """
⋮----
# Save configuration and corpus
config = {
⋮----
# Save embeddings
⋮----
# Save FAISS index if it exists
⋮----
# If FAISS is not available, we can't save the index
# but we can still save the embeddings for brute force search
⋮----
def load(self, path: str, embedder)
⋮----
"""
        Load the embeddings index from disk into the current instance.

        Args:
            path: Directory path where the embeddings were saved
            embedder: The embedder function to use for new queries

        Returns:
            self: Returns self for method chaining

        Raises:
            FileNotFoundError: If the save directory or required files don't exist
            ValueError: If the saved config is invalid or incompatible
        """
⋮----
config_path = os.path.join(path, "config.json")
embeddings_path = os.path.join(path, "corpus_embeddings.npy")
⋮----
# Load configuration and corpus
⋮----
config = json.load(f)
⋮----
# Validate required config fields
required_fields = ["k", "normalize", "corpus", "has_faiss_index"]
⋮----
# Restore configuration
⋮----
# Load embeddings
⋮----
# Load FAISS index if it was saved and FAISS is available
faiss_index_path = os.path.join(path, "faiss_index.bin")
⋮----
# If FAISS is not available, fall back to brute force
⋮----
@classmethod
    def from_saved(cls, path: str, embedder)
⋮----
"""
        Create an Embeddings instance from a saved index.

        This is the recommended way to load saved embeddings as it creates a new
        instance without unnecessarily computing embeddings.

        Args:
            path: Directory path where the embeddings were saved
            embedder: The embedder function to use for new queries

        Returns:
            Embeddings instance loaded from disk

        Example:
            ```python
            # Save embeddings
            embeddings = Embeddings(corpus, embedder)
            embeddings.save("./saved_embeddings")

            # Load embeddings later
            loaded_embeddings = Embeddings.from_saved("./saved_embeddings", embedder)
            ```
        """
# Create a minimal instance without triggering embedding computation
instance = cls.__new__(cls)
# Initialize the search function (required since we bypassed __init__)
</file>

<file path="dspy/retrievers/retrieve.py">
def single_query_passage(passages)
⋮----
passages_dict = {key: [] for key in list(passages[0].keys())}
⋮----
class Retrieve(Parameter)
⋮----
name = "Search"
input_variable = "query"
desc = "takes a search query and returns one or more potentially relevant passages from a corpus"
⋮----
def __init__(self, k=3, callbacks=None)
⋮----
def reset(self)
⋮----
def dump_state(self)
⋮----
state_keys = ["k"]
⋮----
def load_state(self, state)
⋮----
@with_callbacks
    def __call__(self, *args, **kwargs)
⋮----
k = k if k is not None else self.k
⋮----
passages = dspy.settings.rm(query, k=k, **kwargs)
⋮----
# it's not an iterable yet; make it one.
# TODO: we should unify the type signatures of dspy.Retriever
passages = [passages]
passages = [psg.long_text for psg in passages]
⋮----
# TODO: Consider doing Prediction.from_completions with the individual sets of passages (per query) too.
</file>

<file path="dspy/retrievers/weaviate_rm.py">
class WeaviateRM(dspy.Retrieve)
⋮----
"""A retrieval module that uses Weaviate to return the top passages for a given query.

    Assumes that a Weaviate collection has been created and populated with the following payload:
        - content: The text of the passage

    Args:
        weaviate_collection_name (str): The name of the Weaviate collection.
        weaviate_client (WeaviateClient): An instance of the Weaviate client.
        k (int, optional): The default number of top passages to retrieve. Default to 3.
        tenant_id (str, optional): The tenant to retrieve objects from.

    Examples:
        Below is a code snippet that shows how to use Weaviate as the default retriever:
        ```python
        import weaviate

        llm = dspy.Cohere(model="command-r-plus", api_key=api_key)
        weaviate_client = weaviate.connect_to_[local, wcs, custom, embedded]("your-path-here")
        retriever_model = WeaviateRM("my_collection_name", weaviate_client=weaviate_client)
        dspy.configure(lm=llm, rm=retriever_model)

        retrieve = dspy.Retrieve(k=1)
        topK_passages = retrieve("what are the stages in planning, sanctioning and execution of public works").passages
        ```

        Below is a code snippet that shows how to use Weaviate in the forward() function of a module
        ```python
        self.retrieve = WeaviateRM("my_collection_name", weaviate_client=weaviate_client, k=num_passages)
        ```
    """
⋮----
# Check the type of weaviate_client (this is added to support v3 and v4)
⋮----
def forward(self, query_or_queries: str | list[str], k: int | None = None, **kwargs) -> Prediction
⋮----
"""Search with Weaviate for self.k top passages for query or queries.

        Args:
            query_or_queries (Union[str, list[str]]): The query or queries to search for.
            k (Optional[int]): The number of top passages to retrieve. Defaults to self.k.
            kwargs :

        Returns:
            dspy.Prediction: An object containing the retrieved passages.
        """
k = k if k is not None else self.k
queries = [query_or_queries] if isinstance(query_or_queries, str) else query_or_queries
queries = [q for q in queries if q]
⋮----
tenant = kwargs.pop("tenant_id", self._tenant_id)
⋮----
results = self._weaviate_collection.query.with_tenant(tenant).hybrid(query=query, limit=k, **kwargs)
⋮----
results = self._weaviate_collection.query.hybrid(query=query, limit=k, **kwargs)
⋮----
parsed_results = [result.properties[self._weaviate_collection_text_key] for result in results.objects]
⋮----
q = self._weaviate_client.query.get(
⋮----
q = q.with_tenant(tenant)
results = q.with_hybrid(query=query).with_limit(k).do()
⋮----
results = results["data"]["Get"][self._weaviate_collection_name]
parsed_results = [result[self._weaviate_collection_text_key] for result in results]
⋮----
def get_objects(self, num_samples: int, fields: list[str]) -> list[dict]
⋮----
"""Get objects from Weaviate using the cursor API."""
⋮----
objects = []
counter = 0
for item in self._weaviate_collection.iterator(): # TODO: add tenancy scoping
⋮----
new_object = {}
⋮----
def insert(self, new_object_properties: dict)
⋮----
) # TODO: add tenancy scoping
</file>

<file path="dspy/signatures/__init__.py">
__all__ = [
</file>

<file path="dspy/signatures/field.py">
# The following arguments can be used in DSPy InputField and OutputField in addition
# to the standard pydantic.Field arguments. We just hope pydanitc doesn't add these,
# as it would give a name clash.
DSPY_FIELD_ARG_NAMES = ["desc", "prefix", "format", "parser", "__dspy_field_type"]
⋮----
PYDANTIC_CONSTRAINT_MAP = {
⋮----
def move_kwargs(**kwargs)
⋮----
# Pydantic doesn't allow arbitrary arguments to be given to fields,
# but asks that
# > any extra data you want to add to the JSON schema should be passed
# > as a dictionary to the json_schema_extra keyword argument.
# See: https://docs.pydantic.dev/2.6/migration/#changes-to-pydanticfield
pydantic_kwargs = {}
json_schema_extra = {}
⋮----
# Also copy over the pydantic "description" if no dspy "desc" is given.
⋮----
constraints = _translate_pydantic_field_constraints(**kwargs)
⋮----
def _translate_pydantic_field_constraints(**kwargs)
⋮----
"""Extracts Pydantic constraints and translates them into human-readable format."""
⋮----
constraints = []
⋮----
def InputField(**kwargs): # noqa: N802
⋮----
def OutputField(**kwargs): # noqa: N802
⋮----
def new_to_old_field(field)
⋮----
class OldField
⋮----
"""A more ergonomic datatype that infers prefix and desc if omitted."""
⋮----
def __init__(self, *, prefix=None, desc=None, input, format=None)
⋮----
self.prefix = prefix  # This can be None initially and set later
⋮----
def finalize(self, key, inferred_prefix)
⋮----
"""Set the prefix if it's not provided explicitly."""
⋮----
def __repr__(self)
⋮----
def __eq__(self, __value: object) -> bool
⋮----
class OldInputField(OldField)
⋮----
def __init__(self, *, prefix=None, desc=None, format=None)
⋮----
class OldOutputField(OldField)
</file>

<file path="dspy/signatures/signature.py">
"""Signature class for DSPy.

You typically subclass the Signature class, like this:
    class MySignature(dspy.Signature):
        input: str = InputField(desc="...")
        output: int = OutputField(desc="...")

You can call Signature("input1, input2 -> output1, output2") to create a new signature type.
You can also include instructions, Signature("input -> output", "This is a test").
But it's generally better to use the make_signature function.

If you are not sure if your input is a string representation, (like "input1, input2 -> output1, output2"),
or a signature, you can use the ensure_signature function.

For compatibility with the legacy dsp format, you can use the signature_to_template function.
"""
⋮----
def _default_instructions(cls) -> str
⋮----
inputs_ = ", ".join([f"`{field}`" for field in cls.input_fields])
outputs_ = ", ".join([f"`{field}`" for field in cls.output_fields])
⋮----
class SignatureMeta(type(BaseModel))
⋮----
def __call__(cls, *args, **kwargs)
⋮----
# We don't create an actual Signature instance, instead, we create a new Signature class.
custom_types = kwargs.pop("custom_types", None)
⋮----
custom_types = cls._detect_custom_types_from_caller(args[0])
⋮----
@staticmethod
    def _detect_custom_types_from_caller(signature_str)
⋮----
"""Detect custom types from the caller's frame based on the signature string.

        Note: This method relies on Python's frame introspection which has some limitations:
        1. May not work in all Python implementations (e.g., compiled with optimizations)
        2. Looks up a limited number of frames in the call stack
        3. Cannot find types that are imported but not in the caller's namespace

        For more reliable custom type resolution, explicitly provide types using the
        `custom_types` parameter when creating a Signature.
        """
⋮----
# Extract potential type names from the signature string, including dotted names
# Match both simple types like 'MyType' and dotted names like 'Module.Type'
type_pattern = r":\s*([A-Za-z_][A-Za-z0-9_]*(?:\.[A-Za-z_][A-Za-z0-9_]*)*)"
type_names = re.findall(type_pattern, signature_str)
⋮----
# Get type references from caller frames by walking the stack
found_types = {}
⋮----
needed_types = set()
dotted_types = {}
⋮----
parts = type_name.split(".")
base_name = parts[0]
⋮----
frame = None
⋮----
frame = sys._getframe(1)  # Start one level up (skip this function)
⋮----
max_frames = 100
frame_count = 0
⋮----
# If we found all needed types, stop looking
⋮----
frame = frame.f_back
⋮----
# Handle environments where frame introspection is not available
⋮----
def __new__(mcs, signature_name, bases, namespace, **kwargs)
⋮----
# At this point, the orders have been swapped already.
field_order = [name for name, value in namespace.items() if isinstance(value, FieldInfo)]
# Set `str` as the default type for all fields
⋮----
# Try to get from explicit __annotations__ first (e.g., from __future__ import annotations)
raw_annotations = namespace.get("__annotations__")
⋮----
# In 3.14 with PEP 649, get the annotate function and call it
annotate_func = annotationlib.get_annotate_from_class_namespace(namespace)
⋮----
raw_annotations = annotationlib.call_annotate_function(
⋮----
raw_annotations = {}
⋮----
raw_annotations = namespace.get("__annotations__", {})
⋮----
# Python 3.13 and earlier
⋮----
continue  # Don't add types to non-field attributes
⋮----
# Create ordered annotations dictionary that preserves field order
ordered_annotations = {name: raw_annotations[name] for name in field_order if name in raw_annotations}
# Add any remaining annotations that weren't in field_order
⋮----
# Let Pydantic do its thing
cls = super().__new__(mcs, signature_name, bases, namespace, **kwargs)
⋮----
# If we don't have instructions, it might be because we are a derived generic type.
# In that case, we should inherit the instructions from the base class.
⋮----
doc = getattr(base, "__doc__", "")
⋮----
# The more likely case is that the user has just not given us a type.
# In that case, we should default to the input/output format.
⋮----
# Ensure all fields are declared with InputField or OutputField
⋮----
# Ensure all fields have a prefix
⋮----
def _validate_fields(cls)
⋮----
extra = field.json_schema_extra or {}
field_type = extra.get("__dspy_field_type")
⋮----
@property
    def instructions(cls) -> str
⋮----
@instructions.setter
    def instructions(cls, instructions: str) -> None
⋮----
@property
    def input_fields(cls) -> dict[str, FieldInfo]
⋮----
@property
    def output_fields(cls) -> dict[str, FieldInfo]
⋮----
@property
    def fields(cls) -> dict[str, FieldInfo]
⋮----
# Make sure to give input fields before output fields
⋮----
@property
    def signature(cls) -> str
⋮----
"""The string representation of the signature."""
input_fields = ", ".join(cls.input_fields.keys())
output_fields = ", ".join(cls.output_fields.keys())
⋮----
def _get_fields_with_type(cls, field_type) -> dict[str, FieldInfo]
⋮----
def __repr__(cls)
⋮----
"""Output a representation of the signature.

        Uses the form:
        Signature(question, context -> answer
            question: str = InputField(desc="..."),
            context: list[str] = InputField(desc="..."),
            answer: int = OutputField(desc="..."),
        ).
        """
field_reprs = []
⋮----
field_repr = "\n    ".join(field_reprs)
⋮----
class Signature(BaseModel, metaclass=SignatureMeta)
⋮----
""
⋮----
# Note: Don't put a docstring here, as it will become the default instructions
# for any signature that doesn't define it's own instructions.
⋮----
@classmethod
    def with_instructions(cls, instructions: str) -> type["Signature"]
⋮----
"""Return a new Signature class with identical fields and new instructions.

        This method does not mutate `cls`. It constructs a fresh Signature
        class using the current fields and the provided `instructions`.

        Args:
            instructions (str): Instruction text to attach to the new signature.

        Returns:
            A new Signature class whose fields match `cls.fields`
            and whose instructions equal `instructions`.

        Example:
            ```python
            import dspy

            class MySig(dspy.Signature):
                input_text: str = dspy.InputField(desc="Input text")
                output_text: str = dspy.OutputField(desc="Output text")

            NewSig = MySig.with_instructions("Translate to French.")
            assert NewSig is not MySig
            assert NewSig.instructions == "Translate to French."
            ```
        """
⋮----
@classmethod
    def with_updated_fields(cls, name: str, type_: type | None = None, **kwargs: dict[str, Any]) -> type["Signature"]
⋮----
"""Create a new Signature class with the updated field information.

        Returns a new Signature class with the field, name, updated
        with fields[name].json_schema_extra[key] = value.

        Args:
            name: The name of the field to update.
            type_: The new type of the field.
            kwargs: The new values for the field.

        Returns:
            A new Signature class (not an instance) with the updated field information.
        """
fields_copy = deepcopy(cls.fields)
# Update `fields_copy[name].json_schema_extra` with the new kwargs, on conflicts
# we use the new value in kwargs.
⋮----
@classmethod
    def prepend(cls, name, field, type_=None) -> type["Signature"]
⋮----
"""Insert a field at index 0 of the `inputs` or `outputs` section.

        Args:
            name (str): Field name to add.
            field: `InputField` or `OutputField` instance to insert.
            type_ (type | None): Optional explicit type annotation. If `type_` is `None`, the effective type is
                resolved by `insert`.

        Returns:
            A new `Signature` class with the field inserted first.

        Example:
            ```python
            import dspy

            class MySig(dspy.Signature):
                input_text: str = dspy.InputField(desc="Input sentence")
                output_text: str = dspy.OutputField(desc="Translated sentence")

            NewSig = MySig.prepend("context", dspy.InputField(desc="Context for translation"))
            print(list(NewSig.fields.keys()))
            ```
        """
⋮----
@classmethod
    def append(cls, name, field, type_=None) -> type["Signature"]
⋮----
"""Insert a field at the end of the `inputs` or `outputs` section.

        Args:
            name (str): Field name to add.
            field: `InputField` or `OutputField` instance to insert.
            type_ (type | None): Optional explicit type annotation. If `type_` is `None`, the effective type is
                resolved by `insert`.

        Returns:
            A new Signature class with the field appended.

        Example:
            ```python
            import dspy

            class MySig(dspy.Signature):
                input_text: str = dspy.InputField(desc="Input sentence")
                output_text: str = dspy.OutputField(desc="Translated sentence")

            NewSig = MySig.append("confidence", dspy.OutputField(desc="Translation confidence"))
            print(list(NewSig.fields.keys()))
            ```
        """
⋮----
@classmethod
    def delete(cls, name) -> type["Signature"]
⋮----
"""Return a new Signature class without the given field.

        If `name` is not present, the fields are unchanged (no error raised).

        Args:
            name (str): Field name to remove.

        Returns:
            A new Signature class with the field removed (or unchanged if the field was absent).

        Example:
            ```python
            import dspy

            class MySig(dspy.Signature):
                input_text: str = dspy.InputField(desc="Input sentence")
                temp_field: str = dspy.InputField(desc="Temporary debug field")
                output_text: str = dspy.OutputField(desc="Translated sentence")

            NewSig = MySig.delete("temp_field")
            print(list(NewSig.fields.keys()))

            # No error is raised if the field is not present
            Unchanged = NewSig.delete("nonexistent")
            print(list(Unchanged.fields.keys()))
            ```
        """
fields = dict(cls.fields)
⋮----
@classmethod
    def insert(cls, index: int, name: str, field, type_: type | None = None) -> type["Signature"]
⋮----
"""Insert a field at a specific position among inputs or outputs.

        Negative indices are supported (e.g., `-1` appends). If `type_` is omitted, the field's
        existing `annotation` is used; if that is missing, `str` is used.

        Args:
            index (int): Insertion position within the chosen section; negatives append.
            name (str): Field name to add.
            field: InputField or OutputField instance to insert.
            type_ (type | None): Optional explicit type annotation.

        Returns:
            A new Signature class with the field inserted.

        Raises:
            ValueError: If `index` falls outside the valid range for the chosen section.

        Example:
            ```python
            import dspy

            class MySig(dspy.Signature):
                input_text: str = dspy.InputField(desc="Input sentence")
                output_text: str = dspy.OutputField(desc="Translated sentence")

            NewSig = MySig.insert(0, "context", dspy.InputField(desc="Context for translation"))
            print(list(NewSig.fields.keys()))

            NewSig2 = NewSig.insert(-1, "confidence", dspy.OutputField(desc="Translation confidence"))
            print(list(NewSig2.fields.keys()))
            ```
        """
# It's possible to set the type as annotation=type in pydantic.Field(...)
# But this may be annoying for users, so we allow them to pass the type
⋮----
type_ = field.annotation
⋮----
type_ = str
⋮----
input_fields = list(cls.input_fields.items())
output_fields = list(cls.output_fields.items())
⋮----
# Choose the list to insert into based on the field type
lst = input_fields if field.json_schema_extra["__dspy_field_type"] == "input" else output_fields
# We support negative insert indices
⋮----
new_fields = dict(input_fields + output_fields)
⋮----
@classmethod
    def equals(cls, other) -> bool
⋮----
"""Compare the JSON schema of two Signature classes."""
⋮----
@classmethod
    def dump_state(cls)
⋮----
state = {"instructions": cls.instructions, "fields": []}
⋮----
@classmethod
    def load_state(cls, state)
⋮----
signature_copy = Signature(deepcopy(cls.fields), cls.instructions)
⋮----
def ensure_signature(signature: str | type[Signature], instructions=None) -> type[Signature]
⋮----
"""Create a new Signature subclass with the specified fields and instructions.

    Args:
        signature: Either a string in the format "input1, input2 -> output1, output2"
            or a dictionary mapping field names to tuples of (type, FieldInfo).
        instructions: Optional string containing instructions/prompt for the signature.
            If not provided, defaults to a basic description of inputs and outputs.
        signature_name: Optional string to name the generated Signature subclass.
            Defaults to "StringSignature".
        custom_types: Optional dictionary mapping type names to their actual type objects.
            Useful for resolving custom types that aren't built-ins or in the typing module.

    Returns:
        A new signature class with the specified fields and instructions.

    Examples:

    ```
    # Using string format
    sig1 = make_signature("question, context -> answer")

    # Using dictionary format
    sig2 = make_signature({
        "question": (str, InputField()),
        "answer": (str, OutputField())
    })

    # Using custom types
    class MyType:
        pass

    sig3 = make_signature("input: MyType -> output", custom_types={"MyType": MyType})
    ```
    """
# Prepare the names dictionary for type resolution
names = None
⋮----
names = dict(typing.__dict__)
⋮----
fields = _parse_signature(signature, names) if isinstance(signature, str) else signature
⋮----
# Validate the fields, this is important because we sometimes forget the
# slightly unintuitive syntax with tuples of (type, Field)
fixed_fields = {}
⋮----
type_ = type_field.annotation
field = type_field
⋮----
# It might be better to be explicit about the type, but it currently would break
# program of thought and teleprompters, so we just silently default to string.
⋮----
# Default prompt when no instructions are provided
⋮----
sig = Signature(signature, "")  # Simple way to parse input/output fields
instructions = _default_instructions(sig)
⋮----
def _parse_signature(signature: str, names=None) -> dict[str, tuple[type, Field]]
⋮----
fields = {}
⋮----
def _parse_field_string(field_string: str, names=None) -> dict[str, str]
⋮----
"""Extract the field name and type from field string in the string-based Signature.

    It takes a string like "x: int, y: str" and returns a dictionary mapping field names to their types.
    For example, "x: int, y: str" -> [("x", int), ("y", str)]. This function utitlizes the Python AST to parse the
    fields and types.
    """
⋮----
args = ast.parse(f"def f({field_string}): pass").body[0].args.args
field_names = [arg.arg for arg in args]
types = [str if arg.annotation is None else _parse_type_node(arg.annotation, names) for arg in args]
⋮----
def _parse_type_node(node, names=None) -> Any
⋮----
"""Recursively parse an AST node representing a type annotation.

    This function converts Python's Abstract Syntax Tree (AST) nodes into actual Python types.
    It's used to parse type annotations in signature strings like "x: list[int] -> y: str".

    Examples:
        - For "x: int", the AST node represents 'int' and returns the int type
        - For "x: list[str]", it processes a subscript node to return typing.list[str]
        - For "x: Optional[int]", it handles the Union type to return Optional[int]
        - For "x: MyModule.CustomType", it processes attribute access to return the actual type

    Args:
        node: An AST node from Python's ast module, representing a type annotation.
            Common node types include:
            - ast.Name: Simple types like 'int', 'str'
            - ast.Attribute: Nested types like 'typing.List'
            - ast.Subscript: Generic types like 'list[int]'
        names: Optional dictionary mapping type names to their actual type objects.
            Defaults to Python's typing module contents plus NoneType.

    Returns:
        The actual Python type represented by the AST node.

    Raises:
        ValueError: If the AST node represents an unknown or invalid type annotation.
    """
⋮----
def resolve_name(type_name: str)
⋮----
# Check if it's a built-in known type or in the provided names
⋮----
# Common built-in types
builtin_types = [int, str, float, bool, list, tuple, dict, set, frozenset, complex, bytes, bytearray]
⋮----
# Check if it matches any known built-in type by name
⋮----
# Attempt to import a module with this name dynamically
# This allows handling of module-based annotations like `dspy.Image`.
⋮----
mod = importlib.import_module(type_name)
⋮----
# If we don't know the type or module, raise an error
⋮----
base = _parse_type_node(node.value, names)
attr_name = node.attr
⋮----
full_name = f"{node.value.id}.{attr_name}"
⋮----
base_type = _parse_type_node(node.value, names)
slice_node = node.slice
if isinstance(slice_node, ast.Index):  # For older Python versions
slice_node = slice_node.value
⋮----
arg_types = tuple(_parse_type_node(elt, names) for elt in slice_node.elts)
⋮----
arg_types = (_parse_type_node(slice_node, names),)
⋮----
# Special handling for Union, Optional
⋮----
# Handle PEP 604: int | None, str | float, etc.
left = _parse_type_node(node.left, names)
right = _parse_type_node(node.right, names)
⋮----
# Optional[X] is Union[X, NoneType]
⋮----
keys = [kw.arg for kw in node.keywords]
values = []
⋮----
def infer_prefix(attribute_name: str) -> str
⋮----
"""Infer a prefix from an attribute name by converting it to a human-readable format.

    Examples:
        "camelCaseText" -> "Camel Case Text"
        "snake_case_text" -> "Snake Case Text"
        "text2number" -> "Text 2 Number"
        "HTMLParser" -> "HTML Parser"
    """
# Step 1: Convert camelCase to snake_case
# Example: "camelCase" -> "camel_Case"
s1 = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", attribute_name)
⋮----
# Handle consecutive capitals
# Example: "camel_Case" -> "camel_case"
intermediate_name = re.sub("([a-z0-9])([A-Z])", r"\1_\2", s1)
⋮----
# Step 2: Handle numbers by adding underscores around them
# Example: "text2number" -> "text_2_number"
with_underscores_around_numbers = re.sub(
⋮----
r"([a-zA-Z])(\d)",  # Match letter followed by number
r"\1_\2",  # Add underscore between them
⋮----
# Example: "2text" -> "2_text"
⋮----
r"(\d)([a-zA-Z])",  # Match number followed by letter
⋮----
# Step 3: Convert to Title Case while preserving acronyms
words = with_underscores_around_numbers.split("_")
title_cased_words = []
⋮----
# Preserve acronyms like 'HTML', 'API' as-is
⋮----
# Capitalize first letter: 'text' -> 'Text'
⋮----
# Join words with spaces
# Example: ["Text", "2", "Number"] -> "Text 2 Number"
</file>

<file path="dspy/signatures/utils.py">
def get_dspy_field_type(field: FieldInfo) -> Literal["input", "output"]
⋮----
field_type = field.json_schema_extra.get("__dspy_field_type")
</file>

<file path="dspy/streaming/__init__.py">
__all__ = [
</file>

<file path="dspy/streaming/messages.py">
@dataclass
class StreamResponse
⋮----
predict_name: str
signature_field_name: str
chunk: str
is_last_chunk: bool
⋮----
@dataclass
class StatusMessage
⋮----
"""Dataclass that wraps a status message for status streaming."""
⋮----
message: str
⋮----
def sync_send_to_stream(stream, message)
⋮----
"""Send message to stream in a sync context, regardless of event loop state."""
⋮----
async def _send()
⋮----
# If we're in an event loop, offload to a new thread with its own event loop
def run_in_new_loop()
⋮----
new_loop = asyncio.new_event_loop()
⋮----
future = executor.submit(run_in_new_loop)
⋮----
# Not in an event loop, safe to use a new event loop in this thread
⋮----
class StatusMessageProvider
⋮----
"""Provides customizable status message streaming for DSPy programs.

    This class serves as a base for creating custom status message providers. Users can subclass
    and override its methods to define specific status messages for different stages of program execution,
    each method must return a string.

    Example:
    ```python
    class MyStatusMessageProvider(StatusMessageProvider):
        def lm_start_status_message(self, instance, inputs):
            return f"Calling LM with inputs {inputs}..."

        def module_end_status_message(self, outputs):
            return f"Module finished with output: {outputs}!"

    program = dspy.streamify(dspy.Predict("q->a"), status_message_provider=MyStatusMessageProvider())
    ```
    """
⋮----
def tool_start_status_message(self, instance: Any, inputs: dict[str, Any])
⋮----
"""Status message before a `dspy.Tool` is called."""
⋮----
def tool_end_status_message(self, outputs: Any)
⋮----
"""Status message after a `dspy.Tool` is called."""
⋮----
def module_start_status_message(self, instance: Any, inputs: dict[str, Any])
⋮----
"""Status message before a `dspy.Module` or `dspy.Predict` is called."""
⋮----
def module_end_status_message(self, outputs: Any)
⋮----
"""Status message after a `dspy.Module` or `dspy.Predict` is called."""
⋮----
def lm_start_status_message(self, instance: Any, inputs: dict[str, Any])
⋮----
"""Status message before a `dspy.LM` is called."""
⋮----
def lm_end_status_message(self, outputs: Any)
⋮----
"""Status message after a `dspy.LM` is called."""
⋮----
class StatusStreamingCallback(BaseCallback)
⋮----
def __init__(self, status_message_provider: StatusMessageProvider | None = None)
⋮----
stream = settings.send_stream
⋮----
status_message = self.status_message_provider.tool_start_status_message(instance, inputs)
⋮----
status_message = self.status_message_provider.tool_end_status_message(outputs)
⋮----
status_message = self.status_message_provider.lm_start_status_message(instance, inputs)
⋮----
status_message = self.status_message_provider.lm_end_status_message(outputs)
⋮----
status_message = self.status_message_provider.module_start_status_message(instance, inputs)
⋮----
status_message = self.status_message_provider.module_end_status_message(outputs)
</file>

<file path="dspy/streaming/streamify.py">
logger = logging.getLogger(__name__)
⋮----
"""
    Wrap a DSPy program so that it streams its outputs incrementally, rather than returning them
    all at once. It also provides status messages to the user to indicate the progress of the program, and users
    can implement their own status message provider to customize the status messages and what module to generate
    status messages for.

    Args:
        program: The DSPy program to wrap with streaming functionality.
        status_message_provider: A custom status message generator to use instead of the default one. Users can
            implement their own status message generator to customize the status messages and what module to generate
            status messages for.
        stream_listeners: A list of stream listeners to capture the streaming output of specific fields of sub predicts
            in the program. When provided, only the target fields in the target predict will be streamed to the user.
        include_final_prediction_in_output_stream: Whether to include the final prediction in the output stream, only
            useful when `stream_listeners` is provided. If `False`, the final prediction will not be included in the
            output stream. When the program hit cache, or no listeners captured anything, the final prediction will
            still be included in the output stream even if this is `False`.
        is_async_program: Whether the program is async. If `False`, the program will be wrapped with `asyncify`,
            otherwise the program will be called with `acall`.
        async_streaming: Whether to return an async generator or a sync generator. If `False`, the streaming will be
            converted to a sync generator.

    Returns:
        A function that takes the same arguments as the original program, but returns an async
            generator that yields the program's outputs incrementally.

    Example:

    ```python
    import asyncio
    import dspy

    dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))
    # Create the program and wrap it with streaming functionality
    program = dspy.streamify(dspy.Predict("q->a"))

    # Use the program with streaming output
    async def use_streaming():
        output = program(q="Why did a chicken cross the kitchen?")
        return_value = None
        async for value in output:
            if isinstance(value, dspy.Prediction):
                return_value = value
            else:
                print(value)
        return return_value

    output = asyncio.run(use_streaming())
    print(output)
    ```

    Example with custom status message provider:
    ```python
    import asyncio
    import dspy

    dspy.configure(lm=dspy.LM("openai/gpt-4o-mini"))

    class MyStatusMessageProvider(StatusMessageProvider):
        def module_start_status_message(self, instance, inputs):
            return f"Predicting..."

        def tool_end_status_message(self, outputs):
            return f"Tool calling finished with output: {outputs}!"

    # Create the program and wrap it with streaming functionality
    program = dspy.streamify(dspy.Predict("q->a"), status_message_provider=MyStatusMessageProvider())

    # Use the program with streaming output
    async def use_streaming():
        output = program(q="Why did a chicken cross the kitchen?")
        return_value = None
        async for value in output:
            if isinstance(value, dspy.Prediction):
                return_value = value
            else:
                print(value)
        return return_value

    output = asyncio.run(use_streaming())
    print(output)
    ```

    Example with stream listeners:

    ```python
    import asyncio
    import dspy

    dspy.configure(lm=dspy.LM("openai/gpt-4o-mini", cache=False))

    # Create the program and wrap it with streaming functionality
    predict = dspy.Predict("question->answer, reasoning")
    stream_listeners = [
        dspy.streaming.StreamListener(signature_field_name="answer"),
        dspy.streaming.StreamListener(signature_field_name="reasoning"),
    ]
    stream_predict = dspy.streamify(predict, stream_listeners=stream_listeners)

    async def use_streaming():
        output = stream_predict(
            question="why did a chicken cross the kitchen?",
            include_final_prediction_in_output_stream=False,
        )
        return_value = None
        async for value in output:
            if isinstance(value, dspy.Prediction):
                return_value = value
            else:
                print(value)
        return return_value

    output = asyncio.run(use_streaming())
    print(output)
    ```

    You should see the streaming chunks (in the format of `dspy.streaming.StreamResponse`) in the console output.
    """
stream_listeners = stream_listeners or []
⋮----
predict_id_to_listener = find_predictor_for_stream_listeners(program, stream_listeners)
⋮----
predict_id_to_listener = {}
⋮----
program = program.acall
⋮----
program = asyncify(program)
⋮----
callbacks = list(settings.callbacks)
status_streaming_callback = StatusStreamingCallback(status_message_provider)
⋮----
async def generator(args, kwargs, stream: MemoryObjectSendStream)
⋮----
prediction = await program(*args, **kwargs)
⋮----
async def async_streamer(*args, **kwargs)
⋮----
# No listeners are configured, yield the chunk directly for backwards compatibility.
⋮----
# We are receiving a chunk from the LM's response stream, delegate it to the listeners to
# determine if we should yield a value to the user.
⋮----
# In some special cases such as Citation API, it is possible that multiple listeners
# return values at the same time due to the chunk buffer of the listener.
⋮----
# Flush remaining buffered tokens before yielding the Prediction instance
⋮----
# This wildcard case allows for customized streaming behavior.
# It is useful when a users have a custom LM which returns stream chunks in a custom format.
# We let those chunks pass through to the user to handle them as needed.
⋮----
def sync_streamer(*args, **kwargs)
⋮----
output = async_streamer(*args, **kwargs)
⋮----
def apply_sync_streaming(async_generator: AsyncGenerator) -> Generator
⋮----
"""Convert the async streaming generator to a sync generator."""
queue = Queue()  # Queue to hold items from the async generator
stop_sentinel = object()  # Sentinel to signal the generator is complete
⋮----
# To propagate prediction request ID context to the child thread
context = contextvars.copy_context()
⋮----
def producer()
⋮----
"""Runs in a background thread to fetch items asynchronously."""
⋮----
async def runner()
⋮----
# Signal completion
⋮----
# Start the producer in a background thread
thread = threading.Thread(target=producer, daemon=True)
⋮----
# Consume items from the queue
⋮----
item = queue.get()  # Block until an item is available
⋮----
async def streaming_response(streamer: AsyncGenerator) -> AsyncGenerator
⋮----
"""
    Convert a DSPy program output stream to an OpenAI-compatible output stream that can be
    used by a service as an API response to a streaming request.

    Args:
        streamer: An async generator that yields values from a DSPy program output stream.
    Returns:
        An async generator that yields OpenAI-compatible streaming response chunks.
    """
⋮----
data = {"prediction": dict(value.items(include_dspy=False))}
⋮----
data = {"chunk": value.json()}
⋮----
# The chunk value is an OpenAI-compatible streaming chunk value,
# e.g. "data: {"finish_reason": "stop", "index": 0, "is_finished": True, ...}",
# so yield it directly
</file>

<file path="dspy/streaming/streaming_listener.py">
ADAPTER_SUPPORT_STREAMING = [ChatAdapter, XMLAdapter, JSONAdapter]
⋮----
class StreamListener
⋮----
"""Class that listens to the stream to capture the streeaming of a specific output field of a predictor."""
⋮----
"""
        Args:
            signature_field_name: The name of the field to listen to.
            predict: The predictor to listen to. If None, when calling `streamify()` it will automatically look for
                the predictor that has the `signature_field_name` in its signature.
            predict_name: The name of the predictor to listen to. If None, when calling `streamify()` it will
                automatically look for the predictor that has the `signature_field_name` in its signature.
            allow_reuse: If True, the stream listener can be reused for multiple streams. Please note that this could
                hurt the performance because the same stream chunk is sent to multiple listeners.
        """
⋮----
"end_pattern_contains": "</",  # Any closing tag start
⋮----
def _buffered_message_end_with_start_identifier(self, concat_message: str, start_identifier: str) -> str
⋮----
def _could_form_end_identifier(self, concat_message: str, adapter_name: str) -> bool
⋮----
"""Check if the buffered message could potentially form the end identifier.

        This prevents unnecessary buffering when the tokens clearly cannot form the end pattern.
        For example, if buffered message is "hello world" and end pattern is "[[ ## ... ## ]]",
        we know it cannot form the pattern, so we should yield immediately.

        Args:
            concat_message: The concatenated buffered message
            adapter_name: The name of the adapter being used

        Returns:
            True if the message could potentially form part of the end identifier
        """
adapter_config = self.adapter_identifiers[adapter_name]
end_pattern_prefixes = adapter_config.get("end_pattern_prefixes", [])
end_pattern_contains = adapter_config.get("end_pattern_contains")
⋮----
# First check: does it end with a potential start of the pattern?
⋮----
# Second check: if there's a pattern marker, check if message contains it
# This handles cases like "[[ ## com" where we have partial field name
⋮----
def receive(self, chunk: ModelResponseStream)
⋮----
adapter_name = settings.adapter.__class__.__name__ if settings.adapter else "ChatAdapter"
⋮----
start_identifier = self.adapter_identifiers[adapter_name]["start_identifier"]
end_identifier = self.adapter_identifiers[adapter_name]["end_identifier"]
start_indicator = self.adapter_identifiers[adapter_name]["start_indicator"]
⋮----
# Clear up the state for the next stream.
⋮----
# Handle custom streamable types
⋮----
# For non-custom streamable types, the streaming chunks come from the content field of the ModelResponseStream.
⋮----
chunk_message = chunk.choices[0].delta.content
⋮----
# If the cache is hit, the chunk_message could be the full response. When it happens we can
# directly end the stream listening. In some models like gemini, each stream chunk can be multiple
# tokens, so it's possible that response only has one chunk, we also fall back to this logic.
message_after_start_identifier = chunk_message[
⋮----
# We look for the pattern of start_identifier, i.e., "[[ ## {self.signature_field_name} ## ]]" for
# ChatAdapter to identify the start of the stream of our target field. Once the start_indicator, i.e., "[["
# for ChatAdapter, is found, we start checking the next tokens
⋮----
# We keep appending the tokens to the queue until we have a full identifier or the concanated
# tokens no longer match our expected identifier.
⋮----
concat_message = "".join(self.field_start_queue)
⋮----
# We have a full identifier, we can start the stream.
⋮----
# Keep the part after the start_identifier from the concat_message, we need to write it to the buffer.
value_start_index = concat_message.find(start_identifier) + len(start_identifier)
chunk_message = concat_message[value_start_index:].lstrip()
⋮----
# For JSONAdapter, we rely on partial json parsing to detect the end of the field we are listening
# to, so we need to maintain a few extra states to help us with that.
# We add an extra "{" to the beginning of the field_accumulated_messages, so we can detect the
# appearance of the next key.
⋮----
# If the buffered message ends with part of the start_identifier, we keep looking for the
# start_identifier from the token stream.
⋮----
# Doesn't match the expected identifier, reset the queue.
⋮----
# The stream is started, we keep returning the token until we see the start of the next field.
⋮----
token = None
concat_message = "".join(self.field_end_queue.queue).strip()
⋮----
# Buffer cannot form end identifier, safe to flush out the tokens in the buffer.
token = self.flush()
⋮----
# We keep the last 10 tokens in the buffer if they can potentially form the end_identifier to avoid
# sending the DSPy boilerplate tokens to users. 10 is a heuristic number that is sufficient to capture
# the end_identifier for all LMs.
token = self.field_end_queue.get()
⋮----
# TODO: Put adapter streaming handling into individial classes, e.g., `JSONAdapterStreamListener`,
# `ChatAdapterStreamListener`, `XMLAdapterStreamListener` instead of having many adhoc code in the
# `StreamListener` class.
⋮----
# JSONAdapter uses partial json parsing to detect the end of the field we are listening to, instead of
# relying on the end_identifier.
⋮----
# Other adapters rely on the end_identifier to detect the end of the field we are listening to.
⋮----
def _json_adapter_handle_stream_chunk(self, token: str, chunk_message: str) -> StreamResponse | None
⋮----
# When the accumulated tokens end with a curly bracket, that means the streaming for the `dspy.Predict` we
# are listening to is probably finished, we need to run a check and decide whether to end the stream.
⋮----
# If the parse doesn't raise an error, that means the accumulated tokens is a valid json object. Because
# we add an extra "{" to the beginning of the field_accumulated_messages, so we know the streaming is
# finished.
⋮----
last_token = self.flush()
right_curly_bracket_index = last_token.rfind("}")
token = (
⋮----
parsed = jiter.from_json(
⋮----
# If partial json parsing finds a second key, that means the streaming for the field we are listening to
# is finished.
⋮----
keys = list(parsed.keys())
next_field_name = None
⋮----
next_field_name = key
⋮----
last_token_index = last_token.find(next_field_name)
token = token + last_token[:last_token_index] if token else last_token[:last_token_index]
⋮----
def _default_handle_stream_chunk(self, token: str, end_identifier: str) -> StreamResponse | None
⋮----
# The next field is identified, we can end the stream and flush out all tokens in the buffer.
⋮----
token = token + last_token if token else last_token
token = token.rstrip()  # Remove the trailing \n\n
⋮----
def flush(self) -> str
⋮----
"""Flush all tokens in the field end queue.

        This method is called to flush out the last a few tokens when the stream is ended. These tokens
        are in the buffer because we don't directly yield the tokens received by the stream listener
        with the purpose to not yield the end_identifier tokens, e.g., "[[ ## ... ## ]]" for ChatAdapter.
        """
last_tokens = "".join(self.field_end_queue.queue)
⋮----
boundary_index = last_tokens.find(f"</{self.signature_field_name}>")
⋮----
boundary_index = len(last_tokens)
⋮----
boundary_index = last_tokens.find("[[")
⋮----
def finalize(self) -> StreamResponse | None
⋮----
"""Finalize the stream and flush any remaining buffered tokens.

        This should be called when the stream ends.
        It ensures no tokens are lost from the buffer and marks the final chunk appropriately.

        Returns:
            A StreamResponse with the remaining buffered tokens and is_last_chunk=True,
            or None if there are no buffered tokens or the stream hasn't started.
        """
⋮----
# Stream already ended or never started, nothing to finalize
⋮----
@property
    def _output_type(self) -> type | None
⋮----
"""Find the predictor for each stream listener.

    This is a utility function to automatically find the predictor for each stream listener. It is used when some
    listeners don't specify the predictor they want to listen to. If a listener's `signature_field_name` is not
    unique in the program, this function will raise an error.
    """
predictors = program.named_predictors()
⋮----
field_name_to_named_predictor = {}
⋮----
predict_id_to_listener = defaultdict(list)
</file>

<file path="dspy/teleprompt/gepa/__init__.py">
__all__ = ["GEPA"]
</file>

<file path="dspy/teleprompt/gepa/gepa_utils.py">
logger = logging.getLogger(__name__)
⋮----
class LoggerAdapter
⋮----
def __init__(self, logger: logging.Logger)
⋮----
def log(self, x: str)
⋮----
DSPyTrace = list[tuple[Any, dict[str, Any], Prediction]]
⋮----
ReflectiveExample = TypedDict(
⋮----
class ScoreWithFeedback(Prediction)
⋮----
score: float
feedback: str
⋮----
class PredictorFeedbackFn(Protocol)
⋮----
"""
        This function is used to provide feedback to a specific predictor.
        The function is called with the following arguments:
        - predictor_output: The output of the predictor.
        - predictor_inputs: The inputs to the predictor.
        - module_inputs: The inputs to the whole program --- `Example`.
        - module_outputs: The outputs of the whole program --- `Prediction`.
        - captured_trace: The trace of the module's execution.
        # Shape of trace is: [predictor_invocation_idx -> Tuple[Predictor, PredictorInputs, Prediction]]
        # Each trace is a tuple of (Predictor, PredictorInputs, Prediction)

        The function should return a `ScoreWithFeedback` object.
        The feedback is a string that is used to guide the evolution of the predictor.
        """
⋮----
class DspyAdapter(GEPAAdapter[Example, TraceData, Prediction])
⋮----
reflection_lm = self.reflection_lm or dspy.settings.lm
# If custom proposer provided, override everything with custom proposer
⋮----
results: dict[str, str] = {}
⋮----
base_instruction = candidate[name]
dataset_with_feedback = reflective_dataset[name]
⋮----
def build_program(self, candidate: dict[str, str])
⋮----
new_prog = self.student.deepcopy()
⋮----
def evaluate(self, batch, candidate, capture_traces=False)
⋮----
program = self.build_program(candidate)
callback_metadata = (
⋮----
# bootstrap_trace_data-like flow with trace capture
⋮----
trajs = bootstrap_trace_module.bootstrap_trace_data(
scores = []
outputs = []
⋮----
score = t["score"]
⋮----
score = score["score"]
⋮----
evaluator = Evaluate(
res = evaluator(program)
outputs = [r[1] for r in res.results]
scores = [r[2] for r in res.results]
scores = [s["score"] if hasattr(s, "score") else s for s in scores]
⋮----
ret_d: dict[str, list[ReflectiveExample]] = {}
⋮----
# Find the predictor object
module = None
⋮----
module = m
⋮----
# Create reflective examples from traces
items: list[ReflectiveExample] = []
⋮----
trace = data["trace"]
example = data["example"]
prediction = data["prediction"]
module_score = data["score"]
⋮----
module_score = module_score["score"]
⋮----
trace_instances = [t for t in trace if t[0].signature.equals(module.signature)]
⋮----
trace_instances = [t for t in trace_instances if not isinstance(t[2], FailedPrediction)]
⋮----
selected = None
⋮----
selected = t
⋮----
selected = self.rng.choice(trace_instances)
⋮----
inputs = selected[1]
outputs = selected[2]
⋮----
new_inputs = {}
new_outputs = {}
⋮----
contains_history = False
history_key_name = None
⋮----
contains_history = True
⋮----
history_key_name = input_key
⋮----
s = "```json\n"
⋮----
# Keep original object - will be properly formatted when sent to reflection LM
⋮----
s = "Couldn't parse the output as per the expected output format. The model's raw response was:\n"
⋮----
new_outputs = s
⋮----
d = {"Inputs": new_inputs, "Generated Outputs": new_outputs}
⋮----
adapter = ChatAdapter()
structure_instruction = ""
⋮----
# d['score'] = self.failure_score
⋮----
feedback_fn = self.feedback_map[pred_name]
fb = feedback_fn(
⋮----
# Always return strings from the LM outputs
# Even when it returns a dict with e.g., "text" and "reasoning" fields
def stripped_lm_call(self, x: str) -> list[str]
⋮----
raw_outputs = self.reflection_lm(x)
</file>

<file path="dspy/teleprompt/gepa/gepa.py">
logger = logging.getLogger(__name__)
⋮----
AUTO_RUN_SETTINGS = {
⋮----
@experimental(version="3.0.0")
class GEPAFeedbackMetric(Protocol)
⋮----
"""
        This function is called with the following arguments:
        - gold: The gold example.
        - pred: The predicted output.
        - trace: Optional. The trace of the program's execution.
        - pred_name: Optional. The name of the target predictor currently being optimized by GEPA, for which
            the feedback is being requested.
        - pred_trace: Optional. The trace of the target predictor's execution GEPA is seeking feedback for.

        Note the `pred_name` and `pred_trace` arguments. During optimization, GEPA will call the metric to obtain
        feedback for individual predictors being optimized. GEPA provides the name of the predictor in `pred_name`
        and the sub-trace (of the trace) corresponding to the predictor in `pred_trace`.
        If available at the predictor level, the metric should return dspy.Prediction(score: float, feedback: str)
        corresponding to the predictor.
        If not available at the predictor level, the metric can also return a text feedback at the program level
        (using just the gold, pred and trace).
        If no feedback is returned, GEPA will use a simple text feedback consisting of just the score:
        f"This trajectory got a score of {score}."
        """
⋮----
@experimental(version="3.0.0")
@dataclass(frozen=True)
class DspyGEPAResult
⋮----
"""
    Additional data related to the GEPA run.

    Fields:
    - candidates: list of proposed candidates (component_name -> component_text)
    - parents: lineage info; for each candidate i, parents[i] is a list of parent indices or None
    - val_aggregate_scores: per-candidate aggregate score on the validation set (higher is better)
    - val_subscores: per-candidate per-instance scores on the validation set (len == num_val_instances)
    - per_val_instance_best_candidates: for each val instance t, a set of candidate indices achieving the best score on t
    - discovery_eval_counts: Budget (number of metric calls / rollouts) consumed up to the discovery of each candidate

    - total_metric_calls: total number of metric calls made across the run
    - num_full_val_evals: number of full validation evaluations performed
    - log_dir: where artifacts were written (if any)
    - seed: RNG seed for reproducibility (if known)

    - best_idx: candidate index with the highest val_aggregate_scores
    - best_candidate: the program text mapping for best_idx
    """
⋮----
# Data about the proposed candidates
candidates: list[Module]
parents: list[list[int | None]]
val_aggregate_scores: list[float]
val_subscores: list[list[float]]
per_val_instance_best_candidates: list[set[int]]
discovery_eval_counts: list[int]
⋮----
# Optional data
best_outputs_valset: list[list[tuple[int, list[Prediction]]]] | None = None
⋮----
# Optimization metadata
total_metric_calls: int | None = None
num_full_val_evals: int | None = None
log_dir: str | None = None
seed: int | None = None
⋮----
@property
    def best_idx(self) -> int
⋮----
scores = self.val_aggregate_scores
⋮----
@property
    def best_candidate(self) -> dict[str, str]
⋮----
@property
    def highest_score_achieved_per_val_task(self) -> list[float]
⋮----
def to_dict(self) -> dict[str, Any]
⋮----
cands = [{k: v for k, v in cand.items()} for cand in self.candidates]
⋮----
@staticmethod
    def from_gepa_result(gepa_result: "GEPAResult", adapter: "DspyAdapter") -> "DspyGEPAResult"
⋮----
@experimental(version="3.0.0")
class GEPA(Teleprompter)
⋮----
"""
    GEPA is an evolutionary optimizer, which uses reflection to evolve text components
    of complex systems. GEPA is proposed in the paper [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457).
    The GEPA optimization engine is provided by the `gepa` package, available from [https://github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa).

    GEPA captures full traces of the DSPy module's execution, identifies the parts of the trace
    corresponding to a specific predictor, and reflects on the behaviour of the predictor to
    propose a new instruction for the predictor. GEPA allows users to provide textual feedback
    to the optimizer, which is used to guide the evolution of the predictor. The textual feedback
    can be provided at the granularity of individual predictors, or at the level of the entire system's
    execution.

    To provide feedback to the GEPA optimizer, implement a metric as follows:
    ```
    def metric(
        gold: Example,
        pred: Prediction,
        trace: Optional[DSPyTrace] = None,
        pred_name: Optional[str] = None,
        pred_trace: Optional[DSPyTrace] = None,
    ) -> float | ScoreWithFeedback:
        \"""
        This function is called with the following arguments:
        - gold: The gold example.
        - pred: The predicted output.
        - trace: Optional. The trace of the program's execution.
        - pred_name: Optional. The name of the target predictor currently being optimized by GEPA, for which
            the feedback is being requested.
        - pred_trace: Optional. The trace of the target predictor's execution GEPA is seeking feedback for.

        Note the `pred_name` and `pred_trace` arguments. During optimization, GEPA will call the metric to obtain
        feedback for individual predictors being optimized. GEPA provides the name of the predictor in `pred_name`
        and the sub-trace (of the trace) corresponding to the predictor in `pred_trace`.
        If available at the predictor level, the metric should return {'score': float, 'feedback': str} corresponding
        to the predictor.
        If not available at the predictor level, the metric can also return a text feedback at the program level
        (using just the gold, pred and trace).
        If no feedback is returned, GEPA will use a simple text feedback consisting of just the score:
        f"This trajectory got a score of {score}."
        \"""
        ...
    ```

    GEPA can also be used as a batch inference-time search strategy, by passing `valset=trainset, track_stats=True, track_best_outputs=True`, and using the
    `detailed_results` attribute of the optimized program (returned by `compile`) to get the Pareto frontier of the batch. `optimized_program.detailed_results.best_outputs_valset` will contain the best outputs for each task in the batch.

    Example:
    ```
    gepa = GEPA(metric=metric, track_stats=True)
    batch_of_tasks = [dspy.Example(...) for task in tasks]
    new_prog = gepa.compile(student, trainset=trainset, valset=batch_of_tasks)
    pareto_frontier = new_prog.detailed_results.val_aggregate_scores
    # pareto_frontier is a list of scores, one for each task in the batch.
    ```

    Args:
        metric: The metric function to use for feedback and evaluation.
        auto: The auto budget to use for the run. Options: "light", "medium", "heavy".
        max_full_evals: The maximum number of full evaluations to perform.
        max_metric_calls: The maximum number of metric calls to perform.
        reflection_minibatch_size: The number of examples to use for reflection in a single GEPA step. Default is 3.
        candidate_selection_strategy: The strategy to use for candidate selection. Default is "pareto",
            which stochastically selects candidates from the Pareto frontier of all validation scores.
            Options: "pareto", "current_best".
        reflection_lm: The language model to use for reflection. Required parameter. GEPA benefits from
            a strong reflection model. Consider using `dspy.LM(model='gpt-5', temperature=1.0, max_tokens=32000)`
            for optimal performance.
        skip_perfect_score: Whether to skip examples with perfect scores during reflection. Default is True.
        instruction_proposer: Optional custom instruction proposer implementing GEPA's ProposalFn protocol.
            **Default: None (recommended for most users)** - Uses GEPA's proven instruction proposer from
            the [GEPA library](https://github.com/gepa-ai/gepa), which implements the
            [`ProposalFn`](https://github.com/gepa-ai/gepa/blob/main/src/gepa/core/adapter.py). This default
            proposer is highly capable and was validated across diverse experiments reported in the GEPA
            paper and tutorials.

            See documentation on custom instruction proposers
            [here](https://dspy.ai/api/optimizers/GEPA/GEPA_Advanced/#custom-instruction-proposers).

            **Advanced Feature**: Only needed for specialized scenarios:
            - **Multi-modal handling**: Processing dspy.Image inputs alongside textual information
            - **Nuanced control over constraints**: Fine-grained control over instruction length, format,
              and structural requirements beyond standard feedback mechanisms
            - **Domain-specific knowledge injection**: Specialized terminology or context that cannot be
              provided through feedback_func alone
            - **Provider-specific prompting**: Optimizations for specific LLM providers (OpenAI, Anthropic)
              with unique formatting preferences
            - **Coupled component updates**: Coordinated updates of multiple components together rather
              than independent optimization
            - **External knowledge integration**: Runtime access to databases, APIs, or knowledge bases

            The default proposer handles the vast majority of use cases effectively. Use
            MultiModalInstructionProposer() from dspy.teleprompt.gepa.instruction_proposal for visual
            content or implement custom ProposalFn for highly specialized requirements.

            Note: When both instruction_proposer and reflection_lm are set, the instruction_proposer is called
            in the reflection_lm context. However, reflection_lm is optional when using a custom instruction_proposer.
            Custom instruction proposers can invoke their own LLMs if needed.
        component_selector: Custom component selector implementing the [ReflectionComponentSelector](https://github.com/gepa-ai/gepa/blob/main/src/gepa/proposer/reflective_mutation/base.py) protocol,
            or a string specifying a built-in selector strategy. Controls which components (predictors) are selected
            for optimization at each iteration. Defaults to 'round_robin' strategy which cycles through components
            one at a time. Available string options: 'round_robin' (cycles through components sequentially),
            'all' (selects all components for simultaneous optimization). Custom selectors can implement strategies
            using LLM-driven selection logic based on optimization state and trajectories.
            See [gepa component selectors](https://github.com/gepa-ai/gepa/blob/main/src/gepa/strategies/component_selector.py)
            for available built-in selectors and the ReflectionComponentSelector protocol for implementing custom selectors.
        add_format_failure_as_feedback: Whether to add format failures as feedback. Default is False.
        use_merge: Whether to use merge-based optimization. Default is True.
        max_merge_invocations: The maximum number of merge invocations to perform. Default is 5.
        num_threads: The number of threads to use for evaluation with `Evaluate`. Optional.
        failure_score: The score to assign to failed examples. Default is 0.0.
        perfect_score: The maximum score achievable by the metric. Default is 1.0. Used by GEPA
            to determine if all examples in a minibatch are perfect.
        log_dir: The directory to save the logs. GEPA saves elaborate logs, along with all candidate
            programs, in this directory. Running GEPA with the same `log_dir` will resume the run
            from the last checkpoint.
        track_stats: Whether to return detailed results and all proposed programs in the `detailed_results`
            attribute of the optimized program. Default is False.
        use_wandb: Whether to use wandb for logging. Default is False.
        wandb_api_key: The API key to use for wandb. If not provided, wandb will use the API key
            from the environment variable `WANDB_API_KEY`.
        wandb_init_kwargs: Additional keyword arguments to pass to `wandb.init`.
        track_best_outputs: Whether to track the best outputs on the validation set. track_stats must
            be True if track_best_outputs is True. The optimized program's `detailed_results.best_outputs_valset`
            will contain the best outputs for each task in the validation set.
        warn_on_score_mismatch: GEPA (currently) expects the metric to return the same module-level score when
            called with and without the pred_name. This flag (defaults to True) determines whether a warning is
            raised if a mismatch in module-level and predictor-level score is detected.
        seed: The random seed to use for reproducibility. Default is 0.
        gepa_kwargs: (Optional) Additional keyword arguments to pass directly to [gepa.optimize](https://github.com/gepa-ai/gepa/blob/main/src/gepa/api.py).
            Useful for accessing advanced GEPA features not directly exposed through DSPy's GEPA interface.

            Available parameters:
            - batch_sampler: Strategy for selecting training examples. Can be a [BatchSampler](https://github.com/gepa-ai/gepa/blob/main/src/gepa/strategies/batch_sampler.py) instance or a string
              ('epoch_shuffled'). Defaults to 'epoch_shuffled'. Only valid when reflection_minibatch_size is None.
            - merge_val_overlap_floor: Minimum number of shared validation ids required between parents before
              attempting a merge subsample. Only relevant when using `val_evaluation_policy` other than 'full_eval'.
              Default is 5.
            - stop_callbacks: Optional stopper(s) that return True when optimization should stop. Can be a single
              [StopperProtocol](https://github.com/gepa-ai/gepa/blob/main/src/gepa/utils/stop_condition.py) or a list of StopperProtocol instances.
              Examples: [FileStopper](https://github.com/gepa-ai/gepa/blob/main/src/gepa/utils/stop_condition.py),
              [TimeoutStopCondition](https://github.com/gepa-ai/gepa/blob/main/src/gepa/utils/stop_condition.py),
              [SignalStopper](https://github.com/gepa-ai/gepa/blob/main/src/gepa/utils/stop_condition.py),
              [NoImprovementStopper](https://github.com/gepa-ai/gepa/blob/main/src/gepa/utils/stop_condition.py),
              or custom stopping logic. Note: This overrides the default
              max_metric_calls stopping condition.
            - use_cloudpickle: Use cloudpickle instead of pickle for serialization. Can be helpful when the
              serialized state contains dynamically generated DSPy signatures. Default is False.
            - val_evaluation_policy: Strategy controlling which validation ids to score each iteration. Can be
              'full_eval' (evaluate every id each time) or an [EvaluationPolicy](https://github.com/gepa-ai/gepa/blob/main/src/gepa/strategies/eval_policy.py) instance. Default is 'full_eval'.
            - use_mlflow: If True, enables MLflow integration to log optimization progress.
              MLflow can be used alongside Weights & Biases (WandB).
            - mlflow_tracking_uri: The tracking URI to use for MLflow (when use_mlflow=True).
            - mlflow_experiment_name: The experiment name to use for MLflow (when use_mlflow=True).

            Note: Parameters already handled by DSPy's GEPA class will be overridden by the direct parameters
            and should not be passed through gepa_kwargs.

    Note:
        Budget Configuration: Exactly one of `auto`, `max_full_evals`, or `max_metric_calls` must be provided.
        The `auto` parameter provides preset configurations: "light" for quick experimentation, "medium" for
        balanced optimization, and "heavy" for thorough optimization.

        Reflection Configuration: The `reflection_lm` parameter is required and should be a strong language model.
        GEPA performs best with models like `dspy.LM(model='gpt-5', temperature=1.0, max_tokens=32000)`.
        The reflection process analyzes failed examples to generate feedback for program improvement.

        Merge Configuration: GEPA can merge successful program variants using `use_merge=True`.
        The `max_merge_invocations` parameter controls how many merge attempts are made during optimization.

        Evaluation Configuration: Use `num_threads` to parallelize evaluation. The `failure_score` and
        `perfect_score` parameters help GEPA understand your metric's range and optimize accordingly.

        Logging Configuration: Set `log_dir` to save detailed logs and enable checkpoint resuming.
        Use `track_stats=True` to access detailed optimization results via the `detailed_results` attribute.
        Enable `use_wandb=True` for experiment tracking and visualization.

        Reproducibility: Set `seed` to ensure consistent results across runs with the same configuration.
    """
⋮----
# Budget configuration
⋮----
# Reflection configuration
⋮----
# Merge-based configuration
⋮----
# Evaluation configuration
⋮----
# Logging
⋮----
# Reproducibility
⋮----
# GEPA passthrough kwargs
⋮----
# Evaluation Configuration
⋮----
# Logging configuration
⋮----
num_trials = int(max(2 * (num_preds * 2) * np.log2(num_candidates), 1.5 * num_candidates))
⋮----
V = valset_size
N = num_trials
M = minibatch_size
m = full_eval_steps
⋮----
# Initial full evaluation on the default program
total = V
⋮----
# Assume upto 5 trials for bootstrapping each candidate
⋮----
# N minibatch evaluations
⋮----
return total  # no periodic/full evals inside the loop
# Periodic full evals occur when trial_num % (m+1) == 0, where trial_num runs 2..N+1
periodic_fulls = (N + 1) // (m) + 1
# If 1 <= N < m, the code triggers one final full eval at the end
extra_final = 1 if N < m else 0
⋮----
"""
        GEPA uses the trainset to perform reflective updates to the prompt, but uses the valset for tracking Pareto scores.
        If no valset is provided, GEPA will use the trainset for both.

        Parameters:
        - student: The student module to optimize.
        - trainset: The training set to use for reflective updates.
        - valset: The validation set to use for tracking Pareto scores. If not provided, GEPA will use the trainset for both.
        """
⋮----
valset = valset or trainset
⋮----
rng = random.Random(self.seed)
⋮----
def feedback_fn_creator(pred_name: str, predictor) -> "PredictorFeedbackFn"
⋮----
trace_for_pred = [(predictor, predictor_inputs, predictor_output)]
o = self.metric_fn(
⋮----
feedback_map = {k: feedback_fn_creator(k, v) for k, v in student.named_predictors()}
⋮----
# Build the DSPy adapter that encapsulates evaluation, trace capture, feedback extraction, and instruction proposal
adapter = DspyAdapter(
⋮----
# Build the seed candidate: map each predictor name to its current instruction
seed_candidate = {name: pred.signature.instructions for name, pred in student.named_predictors()}
⋮----
gepa_result: GEPAResult = optimize(
⋮----
# Reflection-based configuration
⋮----
# Budget
⋮----
new_prog = adapter.build_program(gepa_result.best_candidate)
⋮----
dspy_gepa_result = DspyGEPAResult.from_gepa_result(gepa_result, adapter)
</file>

<file path="dspy/teleprompt/gepa/instruction_proposal.py">
logger = logging.getLogger(__name__)
⋮----
class GenerateEnhancedMultimodalInstructionFromFeedback(dspy.Signature)
⋮----
"""I provided an assistant with instructions to perform a task involving visual content, but the assistant's performance needs improvement based on the examples and feedback below.

    Your task is to write a better instruction for the assistant that addresses the specific issues identified in the feedback, with particular attention to how visual and textual information should be analyzed and integrated.

    ## Analysis Steps:
    1. **Read the inputs carefully** and identify both the visual and textual input formats, understanding how they work together
    2. **Read all the assistant responses and corresponding feedback** to understand what went wrong with visual analysis, text processing, or their integration
    3. **Identify visual analysis patterns** - what visual features, relationships, or details are important for this task
    4. **Identify domain-specific knowledge** about both visual and textual aspects, as this information may not be available to the assistant in the future
    5. **Look for successful visual-textual integration strategies** and include these patterns in the instruction
    6. **Address specific visual analysis issues** mentioned in the feedback

    ## Instruction Requirements:
    - **Clear task definition** explaining how to process both visual and textual inputs
    - **Visual analysis guidance** specific to this task (what to look for, how to describe, what features matter)
    - **Integration strategies** for combining visual observations with textual information
    - **Domain-specific knowledge** about visual concepts, terminology, or relationships
    - **Error prevention guidance** for common visual analysis mistakes shown in the feedback
    - **Precise, actionable language** for both visual and textual processing

    Focus on creating an instruction that helps the assistant properly analyze visual content, integrate it with textual information, and avoid the specific visual analysis mistakes shown in the examples."""
⋮----
current_instruction = dspy.InputField(
examples_with_feedback = dspy.InputField(
⋮----
improved_instruction = dspy.OutputField(
⋮----
class SingleComponentMultiModalProposer(dspy.Module)
⋮----
"""
    dspy.Module for proposing improved instructions based on feedback.
    """
⋮----
def __init__(self)
⋮----
def forward(self, current_instruction: str, reflective_dataset: list[ReflectiveExample]) -> str
⋮----
"""
        Generate an improved instruction based on current instruction and feedback examples.

        Args:
            current_instruction: The current instruction that needs improvement
            reflective_dataset: List of examples with inputs, outputs, and feedback
                               May contain dspy.Image objects in inputs

        Returns:
            str: Improved instruction text
        """
# Format examples with enhanced pattern recognition
⋮----
# Build kwargs for the prediction call
predict_kwargs = {
⋮----
# Create a rich multimodal examples_with_feedback that includes both text and images
⋮----
# Use current dspy LM settings (GEPA will pass reflection_lm via context)
result = self.propose_instruction(**predict_kwargs)
⋮----
"""
        Format examples with pattern analysis and feedback categorization.

        Returns:
            tuple: (formatted_text_with_patterns, image_map)
        """
# First, use the existing proven formatting approach
⋮----
# Enhanced analysis: categorize feedback patterns
feedback_analysis = self._analyze_feedback_patterns(reflective_dataset)
⋮----
# Add pattern analysis to the formatted examples
⋮----
pattern_summary = self._create_pattern_summary(feedback_analysis)
enhanced_examples = f"{pattern_summary}\n\n{formatted_examples}"
⋮----
def _analyze_feedback_patterns(self, reflective_dataset: list[ReflectiveExample]) -> dict[str, Any]
⋮----
"""
        Analyze feedback patterns to provide better context for instruction generation.

        Categorizes feedback into:
        - Error patterns: Common mistakes and their types
        - Success patterns: What worked well and should be preserved/emphasized
        - Domain knowledge gaps: Missing information that should be included
        - Task-specific guidance: Specific requirements or edge cases
        """
analysis = {
⋮----
# Simple pattern recognition - could be enhanced further
⋮----
feedback = example.get("Feedback", "").lower()
⋮----
# Identify error patterns
⋮----
# Identify success patterns
⋮----
# Identify domain knowledge needs
⋮----
# Create summary if patterns were found
⋮----
def _create_pattern_summary(self, feedback_analysis: dict[str, Any]) -> str
⋮----
"""Create a summary of feedback patterns to help guide instruction generation."""
⋮----
summary_parts = ["## Feedback Pattern Analysis\n"]
⋮----
"""
        Format examples using GEPA's markdown structure while preserving image objects.

        Returns:
            tuple: (formatted_text, image_map) where image_map maps example_index -> list[images]
        """
⋮----
def render_value_with_images(value, level=3, example_images=None)
⋮----
example_images = []
⋮----
image_idx = len(example_images) + 1
⋮----
s = ""
⋮----
def convert_sample_to_markdown_with_images(sample, example_num)
⋮----
s = f"# Example {example_num}\n"
⋮----
formatted_parts = []
image_map = {}
⋮----
formatted_text = "\n\n".join(formatted_parts)
⋮----
total_images = sum(len(imgs) for imgs in image_map.values())
formatted_text = (
⋮----
def _create_multimodal_examples(self, formatted_text: str, image_map: dict[int, list[Type]]) -> Any
⋮----
"""
        Create a multimodal input that contains both text and images for the reflection LM.

        Args:
            formatted_text: The formatted text with image placeholders
            image_map: Dictionary mapping example_index -> list[images] for structured access
        """
⋮----
# Collect all images from all examples
all_images = []
⋮----
multimodal_content = [formatted_text]
⋮----
class MultiModalInstructionProposer(ProposalFn)
⋮----
"""GEPA-compatible multimodal instruction proposer.

    This class handles multimodal inputs (like dspy.Image) during GEPA optimization by using
    a single-component proposer for each component that needs to be updated.
    """
⋮----
"""GEPA-compatible proposal function.

        Args:
            candidate: Current component name -> instruction mapping
            reflective_dataset: Component name -> list of reflective examples
            components_to_update: List of component names to update

        Returns:
            dict: Component name -> new instruction mapping
        """
updated_components = {}
⋮----
current_instruction = candidate[component_name]
component_reflective_data = reflective_dataset[component_name]
⋮----
# Call the single-instruction proposer.
#
# In the future, proposals could consider multiple components instructions,
# instead of just the current instruction, for more holistic instruction proposals.
new_instruction = self.single_proposer(
</file>

<file path="dspy/teleprompt/__init__.py">
__all__ = [
</file>

<file path="dspy/teleprompt/avatar_optimizer.py">
DEFAULT_MAX_EXAMPLES = 10
⋮----
class EvalResult(BaseModel)
⋮----
example: dict
score: float
actions: list[ActionOutput] | None = None
⋮----
class Comparator(dspy.Signature)
⋮----
"""After executing the given actions on user inputs using the given instruction, some inputs have yielded good, results, while others have not. I'll provide you the inputs along with their, corresponding evaluation metrics:

Task:
(1) Firstly, identify and contrast the patterns of inputs that have achieved good results with those that have not.
(2) Then, review the computational logic for any inconsistencies in the previous actions.
(3) Lastly, specify the modification in tools used that can lead to improved performance on the negative inputs."""
⋮----
instruction: str = dspy.InputField(
actions: list[str] = dspy.InputField(
pos_input_with_metrics: list[EvalResult] = dspy.InputField(
neg_input_with_metrics: list[EvalResult] = dspy.InputField(
feedback: str = dspy.OutputField(
⋮----
class FeedbackBasedInstruction(dspy.Signature)
⋮----
"""There is a task that needs to be completed for which one can use multiple tools to achieve the desired outcome. A group's performance was evaluated on a dataset of inputs, the inputs that did well are positive inputs, and the inputs that did not do well are negative inputs.

You received feedback on how they can better use the tools to improve your performance on the negative inputs. You have been provided with the previous instruction, that they followed to use tools to complete the task, and the feedback on your performance.

Your task is to incorporate the feedback and generate a detailed instruction for the group to follow to improve their performance on the task.

Make sure that the new instruction talks about how to use the tools effectively and should be no more than 3 paragraphs long. The previous instruction contains general guidelines that you must retain in the new instruction."""
⋮----
previous_instruction: str = dspy.InputField(
feedback: str = dspy.InputField(
new_instruction: str = dspy.OutputField(
⋮----
class AvatarOptimizer(Teleprompter)
⋮----
def process_example(self, actor, example, return_outputs)
⋮----
actor = deepcopy(actor)
⋮----
prediction = actor(**example.inputs().toDict())
score = self.metric(example, prediction)
⋮----
def thread_safe_evaluator(self, devset, actor, return_outputs=False, num_threads=None)
⋮----
total_score = 0
total_examples = len(devset)
results = []
num_threads = num_threads or dspy.settings.num_threads
⋮----
futures = [executor.submit(self.process_example, actor, example, return_outputs) for example in devset]
⋮----
result = future.result()
⋮----
avg_metric = total_score / total_examples
⋮----
pos_inputs = []
neg_inputs = []
⋮----
def compile(self, student, *, trainset)
⋮----
best_actor = deepcopy(student)
best_score = -999 if self.optimize_for == "max" else 999
⋮----
pos_inputs = sample(pos_inputs, self.max_positive_inputs)
⋮----
neg_inputs = sample(neg_inputs, self.max_negative_inputs)
⋮----
feedback = self.comparator(
⋮----
new_instruction = self.feedback_instruction(
⋮----
best_score = score
</file>

<file path="dspy/teleprompt/bettertogether.py">
logger = logging.getLogger(__name__)
⋮----
class BetterTogether(Teleprompter)
⋮----
STRAT_SEP = " -> "
⋮----
# TODO: Note that the BetterTogether optimizer is meaningful when
# BootstrapFinetune uses a metric to filter the training data before
# fine-tuning. However, one can also choose to run this optimizer with
# a BootstrapFinetune without a metric, say, if there aren't labels
# available for the training data. Should this be noted somewhere?
# TODO: We should re-consider if the metric should be required.
⋮----
is_supported_prompt = isinstance(self.prompt_optimizer, BootstrapFewShotWithRandomSearch)
is_supported_weight = isinstance(self.weight_optimizer, BootstrapFinetune)
⋮----
# TODO: We could record acc on a different valset to pick the best
# strategy within the provided strategy
⋮----
parsed_strategy = strategy.lower().split(self.STRAT_SEP)
⋮----
# TODO: Prepare student returns student.reset_copy(), which is what gets
# optimized. We should make this clear in the doc comments.
student = prepare_student(student)
⋮----
# Make a shallow copy of the trainset, so that we don't change the order
# of the examples in the original trainset
trainset = trainset[:]
⋮----
student = self._run_strategies(parsed_strategy, student, trainset, valset_ratio)
⋮----
def _run_strategies(self, parsed_strategy, student, trainset, valset_ratio) -> Module
⋮----
# Keep track of all the partial strategies/programs in parsed_strategy
# "" corresponds to the initial student program
candidate_programs = []
⋮----
launched_flag = False
⋮----
current_strategy = self.STRAT_SEP.join(parsed_strategy[:ind + 1])
⋮----
launched_flag = True
⋮----
# TODO: Should we reset or just deepcopy? How does resetting affect
# the predictor LMs?
student = student.deepcopy()
⋮----
student = self._compile_prompt_optimizer(student, trainset, valset_ratio)
⋮----
student = self._compile_weight_optimizer(student, trainset)
⋮----
# Record the program corresponding to the current strategy
⋮----
def _compile_prompt_optimizer(self, student, trainset, valset_ratio) -> Module
⋮----
# Sampling a validation set from the trainset for the prompt optimizer
# We drop the hints for prompt optimization
trainset = [x.with_inputs(*list(set(x.inputs().keys()) - {"hint"})) for x in trainset]
num_val = int(valset_ratio * len(trainset))
prompt_valset = trainset[:num_val]
prompt_trainset = trainset[num_val:]
⋮----
# TODO: To make this optimizer general, we need to ensure that all the
# prompt optimizers are accepting a valset or encode a way to check if
# a valset should be passed to an optimizer's compile method.
# TODO: We should ensure that the prompt optimizers in DSPy respect the
# predictor.lm attributes. In particular,
# BootstrapFewShotWithRandomSearch seems to be resetting these. We are
# manually re-setting the LMs here to circumvent this issue, but we
# should consider addressing it in BFRS.
⋮----
pred_lms = [pred.lm for pred in student.predictors()]
student = self.prompt_optimizer.compile(student, trainset=prompt_trainset, valset=prompt_valset)
⋮----
def _compile_weight_optimizer(self, student, trainset) -> Module
⋮----
# Saving the LMs before compiling the weight optimizer
original_lms = [pred.lm for pred in student.predictors()]
⋮----
# a valset should be passed to an optimizer's compile.
⋮----
student = self.weight_optimizer.compile(student, trainset=trainset)
⋮----
# Updating the train kwargs for the new LMs. This is needed because the
# train_kwargs of the optimizer is configured for the original LMs.
new_lms = [pred.lm for pred in student.predictors()]
⋮----
original_params = self.weight_optimizer.train_kwargs[original_lm]
</file>

<file path="dspy/teleprompt/bootstrap_finetune.py">
logger = logging.getLogger(__name__)
⋮----
class FinetuneTeleprompter(Teleprompter)
⋮----
@staticmethod
    def convert_to_lm_dict(arg) -> dict[LM, Any]
⋮----
non_empty_dict = arg and isinstance(arg, dict)
⋮----
# Default to using the same value for all LMs
⋮----
class BootstrapFinetune(FinetuneTeleprompter)
⋮----
# TODO(feature): Inputs train_kwargs (a dict with string keys) and
# adapter (Adapter) can depend on the LM they are used with. We are
# takingthese as parameters for the time being. However, they can be
# attached to LMs themselves -- an LM could know which adapter it should
# be used with along with the train_kwargs. This will lead the only
# required argument for LM.finetune() to be the train dataset.
⋮----
# TODO: Print statements can be converted to logger.info if we ensure
# that the default DSPy logger logs info level messages in notebook
# environments.
⋮----
trace_data = []
⋮----
teachers = teacher if isinstance(teacher, list) else [teacher]
teachers = [prepare_teacher(student, t) for t in teachers]
num_threads = self.num_threads or dspy.settings.num_threads
⋮----
key_to_data = {}
⋮----
data_pred_ind = None if self.multitask else pred_ind
⋮----
training_key = (pred.lm, data_pred_ind)
⋮----
finetune_kwargs = {
⋮----
# TODO(feature): We could run batches of fine-tuning jobs in sequence
# to avoid exceeding the number of threads.
⋮----
key_to_lm = self.finetune_lms(key_to_data)
⋮----
finetuned_lm = key_to_lm[training_key]
⋮----
# TODO: What should the correct behavior be here? Should
# BootstrapFinetune modify the prompt demos according to the
# train data?
⋮----
@staticmethod
    def finetune_lms(finetune_dict) -> dict[Any, LM]
⋮----
num_jobs = len(finetune_dict)
⋮----
# TODO(nit) Pass an identifier to the job so that we can tell the logs
# coming from different fine-tune threads.
⋮----
key_to_job = {}
⋮----
lm: LM = finetune_kwargs.pop("lm")
# TODO: The following line is a hack. We should re-think how to free
# up resources for fine-tuning. This might mean introducing a new
# provider method (e.g. prepare_for_finetune) that can be called
# before fine-tuning is started.
⋮----
key_to_lm = {}
⋮----
result = job.result()
⋮----
def _prepare_finetune_data(self, trace_data: list[dict[str, Any]], lm: LM, pred_ind: int | None = None)
⋮----
# TODO(nit) Log dataset details/size; make logs nicer
⋮----
trace_data = [d for d in trace_data if d["score"]]
⋮----
data = []
adapter = self.adapter[lm] or settings.adapter or ChatAdapter()
data_format = infer_data_format(adapter)
⋮----
include_data = pred_ind is None or pred_ind == pred_ind
⋮----
call_data = build_call_data_from_trace(
⋮----
# Note: Shared below are useful functions for preparing student/teacher programs
# Similar methods are implemented separately and used by other DSPy
# teleprompters. These can be moved to shared locations.
⋮----
# Find data that's relevant to the predictor
pred, inputs, outputs = trace[pred_ind]  # assuming that the order is kept
⋮----
demos = [] if exclude_demos else pred.demos
call_data = adapter.format_finetune_data(
⋮----
# # TODO(PR) check with team
# def bootstrap_trace_data_one_example(
#     example: Example,
#     program: Program,
#     metric: Optional[Callable] = None
# ) -> dict[str, Any]:
#     # Return a dict with the following keys:
#     #     example, prediction, trace, and score (if metric != None)
#     with dspy.context(trace=[]):
#         prediction = program(**example.inputs())
#         trace = dspy.settings.trace
#         score = metric(example, prediction, trace) if metric else None
⋮----
#     data_dict = dict(
#         example=example,
#         prediction=prediction,
#         trace=trace,
#     )
#     if metric:
#         data_dict["score"] = score
⋮----
#     return data_dict
⋮----
def all_predictors_have_lms(program: Module) -> bool
⋮----
"""Return True if all predictors in the program have an LM set."""
⋮----
def copy_program_with_lms(program: Module) -> Module
⋮----
pred_lms = [pred.lm for pred in program.predictors()]
program = program.deepcopy()
⋮----
def prepare_student(student: Module) -> Module
⋮----
# TODO: Should we use reset_copy here? How would it affect the student
# program's predictor LMs, if they are set?
⋮----
# TODO: Should there be a deepcopy here?
# student = student.deepcopy()
⋮----
def prepare_teacher(student: Module, teacher: Module | None = None) -> Module
⋮----
# Ensuring that the student and teacher are are structurally equivalent
⋮----
# Ensuring that the student and teacher programs do not share predictors
⋮----
def assert_structural_equivalency(program1: object, program2: object)
⋮----
num1 = len(program1.predictors())
num2 = len(program2.predictors())
err = f"Structurally equivalent programs must have the the number of predictors. The number of predictors for the two modules do not match: {num1} != {num2}"
⋮----
pzip = zip(program1.named_predictors(), program2.named_predictors(), strict=False)
⋮----
err = f"Program predictor names must match at  corresponding indices for structural equivalency. The predictor names for the programs do not match at index {ind}: '{name1}' != '{name2}'"
⋮----
def assert_no_shared_predictor(program1: Module, program2: Module)
⋮----
id_to_name1 = {id(p): n for n, p in program1.named_predictors()}
id_to_name2 = {id(p): n for n, p in program2.named_predictors()}
shared_ids = set(id_to_name1.keys()) & set(id_to_name2.keys())
⋮----
pred_names = ", ".join(id_to_name1[id] for id in shared_ids)
err = f"The programs share the following predictor(s) with each other: {pred_names}"
⋮----
def get_unique_lms(program: Module) -> list[LM]
⋮----
lms = [pred.lm for pred in program.predictors()]
⋮----
def launch_lms(program: Module)
⋮----
lms = get_unique_lms(program)
⋮----
def kill_lms(program: Module)
</file>

<file path="dspy/teleprompt/bootstrap_trace.py">
logger = logging.getLogger(__name__)
⋮----
@dataclass
class FailedPrediction
⋮----
completion_text: str
format_reward: float | None = None
⋮----
class TraceData(TypedDict)
⋮----
example_ind: int
example: Example
prediction: Prediction
trace: list[tuple[Any, dict[str, Any], Prediction]]
score: float | None
⋮----
# Return a list of dicts with the following keys: example_ind, example, prediction, trace, and score
# (if metric != None)
evaluator = Evaluate(
⋮----
provide_traceback=False,  # TODO(check with team)
max_errors=len(dataset) * 10,  # TODO(check with team)
⋮----
def wrapped_metric(example, prediction, trace=None)
⋮----
# Use `object.__getattribute__` to bypass the custom hook `Module.__getattribute__` so that we avoid
# the warning that `forward` is not accessed through `__call__`.
original_forward = object.__getattribute__(program, "forward")
⋮----
def patched_forward(program_to_use: Module, **kwargs)
⋮----
completion_str = e.lm_response
parsed_result = e.parsed_result
failed_signature = e.signature
failed_inputs = kwargs
⋮----
present = list(parsed_result.keys()) if parsed_result else None
expected = list(failed_signature.output_fields.keys())
⋮----
found_pred = None
⋮----
found_pred = pred
⋮----
trace = dspy.settings.trace.copy()
# Trace is Tuple[signature, inputs, prediction outputs]
⋮----
failed_pred = FailedPrediction(
⋮----
failed_pred = FailedPrediction(completion_text=completion_str, format_reward=format_failure_score)
⋮----
results = evaluator(
⋮----
data = []
⋮----
# TODO(GRPO Team): Often during GRPO bootstrapping, the LLM response does not follow dspy formatting. This
# leads to a value error. To reproduce this issue, try Qwen/Qwen2.5-Coder-0.5B-Instruct with MATH dataset.
# Proposal(Lakshya): We should capture the incorrectly-formatted LLM response, and store it in the trace,
# and pass it to in the GRPO group with a high-negative user-configurable score.
⋮----
data_dict = {"example": example, "prediction": prediction, "trace": trace, "example_ind": example_ind}
</file>

<file path="dspy/teleprompt/bootstrap.py">
# TODO: metrics should return an object with __bool__ basically, but fine if they're more complex.
# They can also be sortable.
⋮----
# TODO: Switch here from dspy.dsp.Example to dspy.Example. Right now, it's okay because it's internal only (predictors).
# NOTE: Notice the places where we don't shuffle examples. I do like that this one doesn't shuffle.
# Other ones that consider options may want to use both unshuffled and then shuffle a few times, when
# considering candidates.
⋮----
# TODO: the max_rounds via branch_idx to get past the cache, not just temperature.
# In principle, we can also sample multiple outputs from the final generation step
# (or even each step, in case the validation function just wants *one* thing that works, but nah)
# and try them all. Having a pretty solid guess on the "final step" of each example isn't hard by the second round,
# in the sense that we have the trace from the first round. (Yes it may change but that's an edge case that
# won't hurt our "best effort" guarantees.)
⋮----
# TODO: When this bootstraps for another teleprompter like finetune, we want all demos we gather.
# But when it's for direct use we may want to sample ONE demo per predictor--example pair.
# This is important for "multi-use" modules.
⋮----
# TODO: Add baselines=[...]
⋮----
logger = logging.getLogger(__name__)
⋮----
class BootstrapFewShot(Teleprompter)
⋮----
"""A Teleprompter class that composes a set of demos/examples to go into a predictor's prompt.
        These demos come from a combination of labeled examples in the training set, and bootstrapped demos.

        Each bootstrap round copies the LM with a new ``rollout_id`` at ``temperature=1.0`` to
        bypass caches and gather diverse traces.

        Args:
            metric (Callable): A function that compares an expected value and predicted value,
                outputting the result of that comparison.
            metric_threshold (float, optional): If the metric yields a numerical value, then check it
                against this threshold when deciding whether or not to accept a bootstrap example.
                Defaults to None.
            teacher_settings (dict, optional): Settings for the `teacher` model.
                Defaults to None.
            max_bootstrapped_demos (int): Maximum number of bootstrapped demonstrations to include.
                Defaults to 4.
            max_labeled_demos (int): Maximum number of labeled demonstrations to include.
                Defaults to 16.
            max_rounds (int): Number of iterations to attempt generating the required bootstrap
                examples. If unsuccessful after `max_rounds`, the program ends. Defaults to 1.
            max_errors (Optional[int]): Maximum number of errors until program ends.
                If ``None``, inherits from ``dspy.settings.max_errors``.
        """
⋮----
def compile(self, student, *, teacher=None, trainset)
⋮----
def _prepare_student_and_teacher(self, student, teacher)
⋮----
# NOTE: behavior change on Oct 28, 2024. Deep copy instead of reset copy for the student-as-teacher.
⋮----
teleprompter = LabeledFewShot(k=self.max_labeled_demos)
⋮----
def _prepare_predictor_mappings(self)
⋮----
# fallback in case if .equals is not implemented (e.g. dsp.Prompt)
⋮----
name2predictor[name1] = None  # dict(student=predictor1, teacher=predictor2)
⋮----
# FIXME(shangyint): This is an ugly hack to bind traces of
# retry.module to retry
# if isinstance(predictor1, Retry):
#     predictor2name[id(predictor1.module)] = name1
⋮----
def _bootstrap(self, *, max_bootstraps=None)
⋮----
max_bootstraps = max_bootstraps or self.max_bootstrapped_demos
bootstrap_attempts = 0
⋮----
bootstrapped = {}
⋮----
# Unbootstrapped training examples
⋮----
# NOTE: Can't yet use evaluate because we need to trace *per example*
# evaluate = Evaluate(program=self.teacher, metric=self.metric, num_threads=12)
# score = evaluate(self.metric, display_table=False, display_progress=True)
⋮----
def _bootstrap_one_example(self, example, round_idx=0)
⋮----
name2traces = {}
teacher = self.teacher
predictor_cache = {}
⋮----
lm = dspy.settings.lm
# Use a fresh rollout with temperature=1.0 to bypass caches.
lm = lm.copy(rollout_id=round_idx, temperature=1.0) if round_idx > 0 else lm
new_settings = {"lm": lm} if round_idx > 0 else {}
⋮----
prediction = teacher(**example.inputs())
trace = dspy.settings.trace
⋮----
metric_val = self.metric(example, prediction, trace)
⋮----
success = metric_val >= self.metric_threshold
⋮----
success = metric_val
⋮----
success = True
⋮----
success = False
⋮----
current_error_count = self.error_count
effective_max_errors = self.max_errors if self.max_errors is not None else dspy.settings.max_errors
⋮----
demo = dspy.Example(augmented=True, **inputs, **outputs)
⋮----
predictor_name = self.predictor2name[id(predictor)]
⋮----
continue  # FIXME: !
⋮----
# # TODO: Look closer into this. It's a bit tricky to reproduce.
# print(f"Failed to find predictor {predictor} in {self.predictor2name}.")
# print(
#     "Are you doing this in a notebook (Jupyter)? This might be caused by redefining values by rerunning cells.",
# )
# print("Try restarting the notebook, or open an issue.")
# raise KeyError(
#     f"Failed to find predictor {id(predictor)} {predictor} in {self.predictor2name}.",
# ) from e
⋮----
# Update the traces
⋮----
# If there are multiple traces for the same predictor in the sample example,
# sample 50/50 from the first N-1 traces or the last trace.
⋮----
rng = random.Random(Hasher.hash(tuple(demos)))
demos = [rng.choice(demos[:-1]) if rng.random() < 0.5 else demos[-1]]
⋮----
def _train(self)
⋮----
rng = random.Random(0)
raw_demos = self.validation
⋮----
augmented_demos = self.name2traces[name][: self.max_bootstrapped_demos]
⋮----
sample_size = min(self.max_labeled_demos - len(augmented_demos), len(raw_demos))
sample_size = max(0, sample_size)
⋮----
raw_demos = rng.sample(raw_demos, sample_size)
</file>

<file path="dspy/teleprompt/copro_optimizer.py">
logger = logging.getLogger(__name__)
⋮----
"""
USAGE SUGGESTIONS:

The following code can be used to compile a optimized signature teleprompter, and evaluate it on an end task:

teleprompter = COPRO(prompt_model=prompt_model, metric=metric, breadth=BREADTH, depth=DEPTH, init_temperature=INIT_TEMPERATURE)
kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)
compiled_prompt_opt = teleprompter.compile(program.deepcopy(), trainset=trainset[:DEV_NUM], eval_kwargs=kwargs)
eval_score = evaluate(compiled_prompt_opt, devset=evalset[:EVAL_NUM], **kwargs)

Note that this teleprompter takes in the following parameters:

* prompt_model: The model used for prompt generation. When unspecified, defaults to the model set in settings (ie. dspy.configure(lm=task_model)).
* metric: The task metric used for optimization.
* breadth: The number of new prompts to generate at each iteration. Default=10.
* depth: The number of times we should ask our prompt model to generate new prompts, with the history of the past prompts as input. Default=3.
* init_temperature: The temperature used to generate new prompts. Higher roughly equals more creative. Default=1.4.
* track_stats: Tells the method whether or not to track statistics about the optimization process.
                If True, the method will track the following statistics:
                    * results_best: The min,max,avg,stddev of top 10 scores for each predictor at each depth.
                    * results_latest: The min,max,avg,stddev of newest prompt scores for each predictor at each depth.
                    * total_calls: The total number of calls to the task metric.
                These statistics will be returned as attributes of the best program.
"""
⋮----
class BasicGenerateInstruction(Signature)
⋮----
"""You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don't be afraid to be creative."""
⋮----
basic_instruction = dspy.InputField(desc="The initial instructions before optimization")
proposed_instruction = dspy.OutputField(desc="The improved instructions for the language model")
proposed_prefix_for_output_field = dspy.OutputField(
⋮----
class GenerateInstructionGivenAttempts(dspy.Signature)
⋮----
"""You are an instruction optimizer for large language models. I will give some task instructions I've tried, along with their corresponding validation scores. The instructions are arranged in increasing order based on their scores, where higher scores indicate better quality.

    Your task is to propose a new instruction that will lead a good language model to perform the task even better. Don't be afraid to be creative."""
⋮----
attempted_instructions = dspy.InputField()
⋮----
class COPRO(Teleprompter)
⋮----
def _check_candidates_equal(self, candidate1, candidate2)
⋮----
def _drop_duplicates(self, candidates)
⋮----
final_candidates = []
last_batch = []
last_batch_score = -1
⋮----
repeat = False
⋮----
repeat = True
⋮----
last_batch = [c]
last_batch_score = c["score"]
⋮----
def _print_signature(self, predictor)
⋮----
signature = self._get_signature(predictor)
⋮----
def _get_signature(self, predictor)
⋮----
def _set_signature(self, predictor, updated_signature)
⋮----
def compile(self, student, *, trainset, eval_kwargs)
⋮----
"""
        optimizes `signature` of `student` program - note that it may be zero-shot or already pre-optimized (demos already chosen - `demos != []`)

        parameters:
        student: program to optimize and left modified.
        trainset: iterable of `Example`s
        eval_kwargs: optional, dict
           Additional keywords to go into `Evaluate` for the metric.

        Returns optimized version of `student`.
        """
module = student.deepcopy()
evaluate = Evaluate(devset=trainset, metric=self.metric, **eval_kwargs)
total_calls = 0
results_best = {
results_latest = {
⋮----
candidates = {}
evaluated_candidates = defaultdict(dict)
⋮----
# Seed the prompt optimizer zero shot with just the instruction, generate BREADTH new prompts
⋮----
basic_instruction = None
basic_prefix = None
⋮----
basic_instruction = self._get_signature(predictor).instructions
basic_prefix = self._get_signature(predictor).fields[last_key].json_schema_extra["prefix"]
⋮----
instruct = dspy.Predict(
⋮----
# Add in our initial prompt as a candidate as well
⋮----
latest_candidates = candidates
all_candidates = candidates
⋮----
module_clone = module.deepcopy()
⋮----
# For each iteration in depth...
⋮----
):  # TODO: fix this so that we eval the new batch of predictors with the new best following predictors
⋮----
latest_scores = []
⋮----
# Go through our module's predictors
⋮----
candidates_ = latest_candidates[id(p_old)]  # Use the most recently generated candidates for evaluation
⋮----
# Unless our program has multiple predictors, in which case we need to reevaluate all prompts with
# the new prompt(s) for the other predictor(s).
candidates_ = all_candidates[
⋮----
# For each candidate
⋮----
# Get the candidate instruction and prefix
⋮----
# Set this new module with our instruction / prefix
⋮----
updated_signature = (
⋮----
# Score the instruction / prefix
⋮----
score = evaluate(module_clone, devset=trainset, **eval_kwargs).score
⋮----
replace_entry = True
⋮----
replace_entry = False
⋮----
# Add it to our evaluated candidates list
⋮----
# Now that we've evaluated the candidates, set this predictor to the best performing version
# to ensure the next round of scores reflect the best possible version
best_candidate = max(evaluated_candidates[id(p_old)].values(), key=lambda candidate: candidate["score"])
⋮----
new_candidates = {}
⋮----
# Build Few-Shot Example of Optimized Prompts
attempts = []
shortest_len = self.breadth
shortest_len = min(len(evaluated_candidates[id(p_base)]), shortest_len)
best_predictors = list(evaluated_candidates[id(p_base)].values())
⋮----
# best_predictors = evaluated_candidates[id(p_base)].values()[:]
⋮----
scores = [x["score"] for x in best_predictors][:10]
⋮----
# breakpoint()
⋮----
# Generate next batch of potential prompts to optimize, with previous attempts as input
⋮----
instr = dspy.Predict(
⋮----
# Get candidates for each predictor
⋮----
latest_candidates = new_candidates
⋮----
candidates = []
⋮----
best_predictors = list(evaluated_candidates[id(predictor)].values())
⋮----
candidates = self._drop_duplicates(candidates)
⋮----
best_program = candidates[0]["program"]
</file>

<file path="dspy/teleprompt/ensemble.py">
"""
TODO: The EnsembledProgram should actually imitate the structure of the individual programs (IF they are all compatible). This allows compiling with an ensemble program as a (singular) teacher. Basically the top majority-compatible trace will end up being used, if dspy.majority is the reduce_fn.
"""
⋮----
class Ensemble(Teleprompter)
⋮----
def __init__(self, *, reduce_fn=None, size=None, deterministic=False)
⋮----
"""A common reduce_fn is dspy.majority."""
⋮----
def compile(self, programs)
⋮----
size = self.size
reduce_fn = self.reduce_fn
⋮----
class EnsembledProgram(dspy.Module)
⋮----
def __init__(self)
⋮----
def forward(self, *args, **kwargs)
⋮----
programs = random.sample(self.programs, size) if size else self.programs
outputs = [prog(*args, **kwargs) for prog in programs]
</file>

<file path="dspy/teleprompt/grpo.py">
logger = logging.getLogger(__name__)
⋮----
class GRPO(FinetuneTeleprompter)
⋮----
# The backend will be called with a batch of (num_dspy_examples_per_grpo_step * num_rollouts_per_grpo_step * num_predictors) per training set if multitask is True
# If multitask is False, the backend will be called with a batch of (num_dspy_examples_per_grpo_step * num_rollouts_per_grpo_step) per training job
⋮----
# At this point, trace_data: list[example_idx -> list[teacher_idx -> [num_samples_per_input * Dict(example, prediction, trace, example_ind, score)]]]
# Shape of trace is: [dspy_module_invocation_idx -> Tuple[Predictor, PredictorInputs, Prediction]]
⋮----
# TODO(GRPO Team): Ideally, once the dspy format issue is fixed, this change should be reverted back to being a normal assert.
⋮----
def report_validation_metrics(self, student, trainset, valset, logger, step_idx=-1)
⋮----
# Validation set provided by user
⋮----
valset_evaluator = Evaluate(
⋮----
provide_traceback=False,  # TODO(check with team)
max_errors=len(valset)*10,  # TODO(check with team)
⋮----
valset_evaluation = valset_evaluator(student, metric=self.metric)
trainset_scores = [r[-1] for r in valset_evaluation.results[len(valset):]]
valset_scores = [r[-1] for r in valset_evaluation.results[:len(valset)]]
trainset_agg = sum(trainset_scores) / len(trainset_scores)
valset_agg = sum(valset_scores) / len(valset_scores)
⋮----
# No validation set provided by user
⋮----
max_errors=len(trainset)*10,  # TODO(check with team)
⋮----
# No valset provided, and not using train as val
⋮----
def update_shuffled_trainset(self, original_trainset)
⋮----
num_to_pad = self.num_dspy_examples_per_grpo_step - (len(original_trainset) % self.num_dspy_examples_per_grpo_step)
⋮----
# Select ids based on least frequent ids
⋮----
selected_id = self.id_freqs.most_common()[::-1][0][0]
⋮----
base_idx = train_step_idx * self.num_dspy_examples_per_grpo_step
⋮----
curr_epoch = 0
⋮----
curr_epoch = base_idx // len(self.shuffled_trainset_ids)
⋮----
base_idx = base_idx % len(self.shuffled_trainset_ids)
end_idx = base_idx + self.num_dspy_examples_per_grpo_step
⋮----
selected_ids = self.shuffled_trainset_ids[base_idx:end_idx]
selected_trainset = [original_trainset[i] for i in selected_ids]
⋮----
multiplier = (self.num_dspy_examples_per_grpo_step + len(trainset) - 1) // len(trainset)
⋮----
trainset = trainset * multiplier
⋮----
# TODO(GRPO Team): Following checks are for unimplemented features.
# Consider if we want to eventually implement them or remove. We don't
# yet support:
# * multitask == False
# * student program with multiple predictor LMs
# The main reason for these is that we update the LMs in place. If these
# LMs are shared between the different predictors of the student
# program and we have multitask == False, we need to decide which steps
# will use new LM copies and we need to ensure our decision is
# consistent with any teacher LMs that share the same LMs.
# TODO(GRPO Team): We want to make it possible to continue GRPO runs in
# the future by saving the state of the GRPO run in the event of a
# process failure.
⋮----
student_lms = {id(pred.lm) for pred in student.predictors()}
⋮----
# Our regular input validation starts here
⋮----
pred_signature_hash_to_ind = {hash(pred.signature): ind for ind, pred in enumerate(student.predictors())}
num_student_predictors = len(student.predictors())
⋮----
teacher = student
teachers = teacher if isinstance(teacher, list) else [teacher]
⋮----
# Ensure that the teachers list contain the student program
⋮----
num_samples_per_input = self.num_rollouts_per_grpo_step // len(teachers)
⋮----
# We will disable the LM cache for all programs (student and teachers)
# These will be reverted to their original state at the end of the
# training
lm_cache_dict = {}
⋮----
# Update train_kwargs
⋮----
train_kwargs = self.train_kwargs[pred.lm]
train_kwargs = {} if train_kwargs is None else train_kwargs
⋮----
# We need to have a separate job for each unique LM x the data
# collection strategy. This properly handles all combinations of
# multitask and predictor LMs
⋮----
grpo_training_jobs = {}
⋮----
data_key = None if self.multitask else pred_ind
job_key = (pred.lm, data_key)
⋮----
job = pred.lm.reinforce(train_kwargs=train_kwargs)
⋮----
# Queue of GRPO groups per training job; key is (LM, data_key)
group_queues = {}
⋮----
subsample_training_dataset = self.select_training_sample_and_update_shuffled_trainset(
def _any_available_for_step()
⋮----
grpo_status: GRPOStatus = job.get_status()
pending_batch_ids = grpo_status["pending_batch_ids"]
available = set(pending_batch_ids) - set(self.fulfilled_batch_ids)
⋮----
trace_data = [[[] for _ in range(len(teachers))] for _ in range(len(subsample_training_dataset))]
⋮----
subsample_training_dataset_repeated = [example for _ in range(num_samples_per_input) for example in subsample_training_dataset]
round_data = bootstrap_trace_data(
⋮----
raise_on_error=False, # TODO(GRPO Team): This should be True, once the dspy format issue is fixed
⋮----
example_ind_in_subsample = data_dict["example_ind"] % len(subsample_training_dataset)
⋮----
# The trace_data for examples with FailedPrediction cases will have the signature at index 0, instead of the predictor
# We need to replace the signature with the predictor
⋮----
# Now, we need to prepare batches of data to be sent for training
# Shape of train_batch_per_predictor: list[num_student_predictors -> list[ ]]
train_batch_per_predictor: list[list[GRPOGroup]] = [[] for _ in range(num_student_predictors)]
⋮----
# Each example_data is a list of teacher_idx -> [num_samples_per_input * Dict(example, prediction, trace, example_ind, score)]
# We need to flatten this list and create a batch for each predictor
⋮----
# TODO(Lakshya, Omar, Noah): Discuss what to do with the same module being invoked multiple times within a single dspy.Example
predictor_example_invocations: list[list[tuple]] = []
⋮----
# Each sample is a Dict(example, prediction, trace, example_ind, score)
# sample['prediction'] is module_level prediction
⋮----
trace_instances_for_current_pred = [(*t, sample["score"]) for t in sample["trace"] if hash(t[0].signature) == hash(student.predictors()[pred_id].signature)]
⋮----
min_len = min([len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))])
max_len = max([len(predictor_example_invocations[i]) for i in range(len(predictor_example_invocations))])
⋮----
predictor_example_invocations = [invocation[:min_len] for invocation in predictor_example_invocations]
⋮----
selector = lambda l: self.rng.choice(l) # noqa: E731, E741
⋮----
selector = lambda l: l[-1] # noqa: E731, E741
predictor_example_invocations = [
⋮----
example_training_data: list[GRPOGroup] = [[] for _ in range(max_len)]
⋮----
trace_instance = predictor_example_invocations[rollout_idx][group_idx]
score = trace_instance[3]
# for module_invocation_idx, trace_instance in enumerate(trace_instances_for_current_pred):
# Each trace is a tuple of (Predictor, PredictorInputs, Prediction)
trace_pred_id = pred_signature_hash_to_ind.get(hash(trace_instance[0].signature))
⋮----
predictor = trace_instance[0]
pred_lm = predictor.lm
adapter = self.adapter[pred_lm] or settings.adapter or XMLAdapter()
⋮----
# TODO(Lakshya): Currently we exclude demos from the training data
# TODO(GRPO Team): Use build_call_data_from_trace (from bootstrap_finetune) instead of
# dealing with the message formatting ourselves.
inp_messages = adapter.format(
⋮----
demos=[] # TODO: Add support for demos
⋮----
score = trace_instance[2].format_reward or self.format_failure_score
⋮----
all_messages = adapter.format_finetune_data(
⋮----
# TODO(GRPO Team): How can we avoid this warning?
⋮----
# We now run the GRPO step. Notes:
# * The job here has a reference to a particular M that's attached
#   to the student program. We update the .model field of this LM
#   inside the job, which also updates the LM in the student program
#   since these point to the same reference (along with any teacher
#   program that shares the same LM).
# * TODO(GRPO Team): This is inconsistent with how
#   BootstrapFinetune works, which creates new LM instances post
#   training. We should decide whether the LMs should be updated in
#   place or new LMs should be created, and standardize our approach
#   for both. If we decide to create new LMs, we should find a way
#   to update self.adapter and self.train_kwargs accordingly, in
#   addition to updating any teacher programs that share the same
#   LM.
⋮----
train_data: list[GRPOGroup] = sum(train_batch_per_predictor, []) if data_key is None else train_batch_per_predictor[data_key] #noqa: RUF017
⋮----
# TODO(GRPO Team): This is very undesirable. This occurs only because in some of the generations, the model does not follow the correct dspy format.
# The ideal solution is to identify the full response string in that predictor's group, and then assign a high-negative (user-configurable) reward to that group.
# Pad the group to the expected number of generations by repeating the whole group, might require multiple iterations
⋮----
# Determine available batch IDs for this specific job
⋮----
available_batch_ids = list(set(pending_batch_ids) - set(self.fulfilled_batch_ids))
⋮----
# Initialize and (re)fill the queue for this job as needed
job_key = (lm_for_job, data_key)
q = group_queues.setdefault(job_key, deque())
⋮----
# Refill strategy: add randomized copies of current train_data until we can satisfy all batch_ids
⋮----
need = len(available_batch_ids) - len(q)
⋮----
# Shuffle by sampling without replacement
shuffled = self.rng.sample(train_data, k=len(train_data))
⋮----
# Build GRPOGroup items by popping from the queue; fallback to random selection if needed
final_train_data: list[GRPOGroup] = []
⋮----
grp = q.popleft()
⋮----
# Fallback: choose randomly from current train_data (or flattened pool) if queue underflows
fallback_pool = train_data if len(train_data) > 0 else sum(train_batch_per_predictor, [])
⋮----
# Nothing to send for this job
⋮----
grp = self.rng.choice(fallback_pool)
⋮----
# Track fulfilled IDs to avoid reuse
⋮----
# Revert cache states to their initial values
⋮----
def disable_lm_cache(program: Module, lm_cache_dict: dict)
⋮----
"""Disable the LM cache for all predictors in the program."""
⋮----
if pred.lm not in lm_cache_dict:  # Check to avoid overwriting the cache
⋮----
def recover_lm_cache(program: Module, lm_cache_dict: dict)
⋮----
"""Recover the LM caches for all predictors in the program to their original state."""
⋮----
# We do not expect this branch to execute at all since all the LMs
# are modified in place and no new LMs are created during training.
# However, we do not complain if this happens since this is a
# relatively minor feature. We default the LM cache to True.
</file>

<file path="dspy/teleprompt/infer_rules.py">
logger = logging.getLogger(__name__)
⋮----
class InferRules(BootstrapFewShot)
⋮----
def __init__(self, num_candidates=10, num_rules=10, num_threads=None, teacher_settings=None, **kwargs)
⋮----
def compile(self, student, *, teacher=None, trainset, valset=None)
⋮----
train_size = int(0.5 * len(trainset))
⋮----
original_program = self.student.deepcopy()
all_predictors = [p for p in original_program.predictors() if hasattr(p, "signature")]
instructions_list = [p.signature.instructions for p in all_predictors]
⋮----
best_score = -np.inf
best_program = None
⋮----
candidate_program = original_program.deepcopy()
candidate_predictors = [p for p in candidate_program.predictors() if hasattr(p, "signature")]
⋮----
rules = self.induce_natural_language_rules(predictor, trainset)
⋮----
score = self.evaluate_program(candidate_program, valset)
⋮----
best_score = score
best_program = candidate_program
⋮----
def induce_natural_language_rules(self, predictor, trainset)
⋮----
demos = self.get_predictor_demos(trainset, predictor)
signature = predictor.signature
⋮----
examples_text = self.format_examples(demos, signature)
⋮----
demos = demos[:-1]
⋮----
def update_program_instructions(self, predictor, natural_language_rules)
⋮----
def format_examples(self, demos, signature)
⋮----
examples_text = ""
⋮----
input_fields = {k: v for k, v in demo.items() if k in signature.input_fields}
output_fields = {k: v for k, v in demo.items() if k in signature.output_fields}
input_text = "\n".join(f"{k}: {v}" for k, v in input_fields.items())
output_text = "\n".join(f"{k}: {v}" for k, v in output_fields.items())
⋮----
def get_predictor_demos(self, trainset, predictor)
⋮----
# TODO: Consider how this handled "incomplete" demos.
⋮----
def evaluate_program(self, program, dataset)
⋮----
effective_max_errors = (
evaluate = Evaluate(
score = evaluate(program, metric=self.metric).score
⋮----
class RulesInductionProgram(dspy.Module)
⋮----
def __init__(self, num_rules, teacher_settings=None)
⋮----
class CustomRulesInduction(dspy.Signature)
⋮----
__doc__ = (
examples_text = dspy.InputField(desc="Text containing examples")
natural_language_rules = dspy.OutputField(desc="Induced natural language rules")
⋮----
def forward(self, examples_text)
⋮----
# Generate rules with a fresh rollout and non-zero temperature.
lm = dspy.settings.lm.copy(
⋮----
rules = self.rules_induction(examples_text=examples_text).natural_language_rules
</file>

<file path="dspy/teleprompt/knn_fewshot.py">
class KNNFewShot(Teleprompter)
⋮----
def __init__(self, k: int, trainset: list[Example], vectorizer: Embedder, **few_shot_bootstrap_args: dict[str, Any])
⋮----
"""
        KNNFewShot is an optimizer that uses an in-memory KNN retriever to find the k nearest neighbors
        in a trainset at test time. For each input example in a forward call, it identifies the k most
        similar examples from the trainset and attaches them as demonstrations to the student module.

        Args:
            k: The number of nearest neighbors to attach to the student model.
            trainset: The training set to use for few-shot prompting.
            vectorizer: The `Embedder` to use for vectorization
            **few_shot_bootstrap_args: Additional arguments for the `BootstrapFewShot` optimizer.

        Example:
            ```python
            import dspy
            from sentence_transformers import SentenceTransformer

            # Define a QA module with chain of thought
            qa = dspy.ChainOfThought("question -> answer")

            # Create a training dataset with examples
            trainset = [
                dspy.Example(question="What is the capital of France?", answer="Paris").with_inputs("question"),
                # ... more examples ...
            ]

            # Initialize KNNFewShot with a sentence transformer model
            knn_few_shot = KNNFewShot(
                k=3,
                trainset=trainset,
                vectorizer=dspy.Embedder(SentenceTransformer("all-MiniLM-L6-v2").encode)
            )

            # Compile the QA module with few-shot learning
            compiled_qa = knn_few_shot.compile(qa)

            # Use the compiled module
            result = compiled_qa("What is the capital of Belgium?")
            ```
        """
⋮----
def compile(self, student, *, teacher=None)
⋮----
student_copy = student.reset_copy()
⋮----
def forward_pass(_, **kwargs)
⋮----
knn_trainset = self.KNN(**kwargs)
few_shot_bootstrap = BootstrapFewShot(**self.few_shot_bootstrap_args)
compiled_program = few_shot_bootstrap.compile(
</file>

<file path="dspy/teleprompt/mipro_optimizer_v2.py">
logger = logging.getLogger(__name__)
⋮----
# Constants
BOOTSTRAPPED_FEWSHOT_EXAMPLES_IN_CONTEXT = 3
LABELED_FEWSHOT_EXAMPLES_IN_CONTEXT = 0
MIN_MINIBATCH_SIZE = 50
⋮----
AUTO_RUN_SETTINGS = {
⋮----
# ANSI escape codes for colors
YELLOW = "\033[93m"
GREEN = "\033[92m"
BLUE = "\033[94m"
BOLD = "\033[1m"
ENDC = "\033[0m"  # Resets the color to default
⋮----
class MIPROv2(Teleprompter)
⋮----
# Validate 'auto' parameter
allowed_modes = {None, "light", "medium", "heavy"}
⋮----
requires_permission_to_run: bool | None = None, # deprecated
⋮----
effective_max_errors = (
⋮----
effective_max_bootstrapped_demos = (
effective_max_labeled_demos = (
⋮----
zeroshot_opt = (effective_max_bootstrapped_demos == 0) and (effective_max_labeled_demos == 0)
⋮----
# If auto is None, and num_trials is not provided (but num_candidates is), raise an error that suggests a good num_trials value
⋮----
# If auto is None, and num_candidates or num_trials is None, raise an error
⋮----
# If auto is provided, and either num_candidates or num_trials is not None, raise an error
⋮----
# Set random seeds
seed = seed or self.seed
⋮----
# Set training & validation sets
⋮----
num_instruct_candidates = (
num_fewshot_candidates = (
⋮----
# Set hyperparameters based on run mode (if set)
⋮----
# Initialize program and evaluator
program = student.deepcopy()
evaluate = Evaluate(
⋮----
# Step 1: Bootstrap few-shot examples
demo_candidates = self._bootstrap_fewshot_examples(
⋮----
# Step 2: Propose instruction candidates
instruction_candidates = self._propose_instructions(
⋮----
# If zero-shot, discard demos
⋮----
demo_candidates = None
⋮----
# Step 3: Find optimal prompt parameters
best_program = self._optimize_prompt_parameters(
⋮----
def _set_random_seeds(self, seed)
⋮----
def _set_num_trials_from_num_candidates(self, program, zeroshot_opt, num_candidates)
⋮----
num_vars = len(program.predictors())
⋮----
num_vars *= 2  # Account for few-shot examples + instruction variables
# Trials = MAX(c*M*log(N), c=2, 3/2*N)
num_trials = int(max(2 * num_vars * np.log2(num_candidates), 1.5 * num_candidates))
⋮----
auto_settings = AUTO_RUN_SETTINGS[self.auto]
⋮----
valset = create_minibatch(valset, batch_size=auto_settings["val_size"], rng=self.rng)
minibatch = len(valset) > MIN_MINIBATCH_SIZE
⋮----
# Set num instruct candidates to 1/2 of N if optimizing with few-shot examples, otherwise set to N
# This is because we've found that it's generally better to spend optimization budget on few-shot examples
# When they are allowed.
num_instruct_candidates = auto_settings["n"] if zeroshot_opt else int(auto_settings["n"] * 0.5)
num_fewshot_candidates = auto_settings["n"]
⋮----
num_trials = self._set_num_trials_from_num_candidates(program, zeroshot_opt, auto_settings["n"])
⋮----
def _set_and_validate_datasets(self, trainset: list, valset: list | None)
⋮----
valset_size = min(1000, max(1, int(len(trainset) * 0.80)))
cutoff = len(trainset) - valset_size
valset = trainset[cutoff:]
trainset = trainset[:cutoff]
⋮----
num_predictors = len(program.predictors())
⋮----
# Estimate prompt model calls
estimated_prompt_model_calls = (
⋮----
10  # Data summarizer calls
+ num_instruct_candidates * num_predictors  # Candidate generation
+ (num_predictors + 1 if program_aware_proposer else 0)  # Program-aware proposer
⋮----
prompt_model_line = (
⋮----
# Estimate task model calls
⋮----
estimated_task_model_calls = len(valset) * num_trials
task_model_line = (
⋮----
full_eval_steps = num_trials // minibatch_full_eval_steps + 1
estimated_task_model_calls = minibatch_size * num_trials + len(valset) * full_eval_steps
⋮----
zeroshot = max_bootstrapped_demos == 0 and max_labeled_demos == 0
⋮----
max_errors = dspy.settings.max_errors
⋮----
demo_candidates = create_n_fewshot_demo_sets(
# NOTE: Bootstrapping is essential to MIPRO!
# Failing silently here makes the rest of the optimization far weaker as a result!
# except Exception as e:
#     logger.info(f"!!!!\n\n\n\n\nError generating few-shot examples: {e}")
#     logger.info("Running without few-shot examples.!!!!\n\n\n\n\n")
#     demo_candidates = None
⋮----
proposer = GroundedProposer(
⋮----
instruction_candidates = proposer.propose_instructions_for_program(
⋮----
# Run optimization
⋮----
# Compute the adjusted total trials that we will run (including full evals)
run_additional_full_eval_at_end = 1 if num_trials % minibatch_full_eval_steps != 0 else 0
adjusted_num_trials = int(
⋮----
default_score = eval_candidate_program(len(valset), valset, program, evaluate, self.rng).score
⋮----
trial_logs = {}
⋮----
# Initialize optimization variables
best_score = default_score
best_program = program.deepcopy()
total_eval_calls = len(valset)
score_data = [{"score": best_score, "program": program.deepcopy(), "full_eval": True}]
param_score_dict = defaultdict(list)
fully_evaled_param_combos = {}
⋮----
# Define the objective function
def objective(trial)
⋮----
trial_num = trial.number + 1
⋮----
# Create a new candidate program
candidate_program = program.deepcopy()
⋮----
# Choose instructions and demos, insert them into the program
⋮----
# Log assembled program
⋮----
# Evaluate the candidate program (on minibatch if minibatch=True)
batch_size = minibatch_size if minibatch else len(valset)
score = eval_candidate_program(batch_size, valset, candidate_program, evaluate, self.rng).score
⋮----
# Update best score and program
⋮----
best_score = score
best_program = candidate_program.deepcopy()
⋮----
# Log evaluation results
⋮----
)  # score, prog, full_eval
⋮----
categorical_key = ",".join(map(str, chosen_params))
⋮----
# If minibatch, perform full evaluation at intervals (and at the very end)
⋮----
sampler = optuna.samplers.TPESampler(seed=seed, multivariate=True)
study = optuna.create_study(direction="maximize", sampler=sampler)
⋮----
default_params = {f"{i}_predictor_instruction": 0 for i in range(len(program.predictors()))}
⋮----
# Add default run as a baseline in optuna (TODO: figure out how to weight this by # of samples evaluated on)
trial = optuna.trial.create_trial(
⋮----
# Attach logs to best program
⋮----
sorted_candidate_programs = sorted(score_data, key=lambda x: x["score"], reverse=True)
# Attach all minibatch programs
⋮----
# Attach all programs that were evaluated on the full trainset, in descending order of score
⋮----
minibatch_scores = ", ".join([f"{s['score']}" for s in score_data if not s["full_eval"]])
⋮----
full_eval_scores = ", ".join([f"{s['score']}" for s in score_data if s["full_eval"]])
trajectory = "[" + full_eval_scores + "]"
⋮----
chosen_params = []
raw_chosen_params = {}
⋮----
# Select instruction
instruction_idx = trial.suggest_categorical(
selected_instruction = instruction_candidates[i][instruction_idx]
updated_signature = get_signature(predictor).with_instructions(selected_instruction)
⋮----
# Select demos if available
⋮----
demos_idx = trial.suggest_categorical(f"{i}_predictor_demos", range(len(demo_candidates[i])))
⋮----
def _get_param_distributions(self, program, instruction_candidates, demo_candidates)
⋮----
param_distributions = {}
⋮----
# Identify best program to evaluate fully
⋮----
full_eval_score = eval_candidate_program(len(valset), valset, highest_mean_program, evaluate, self.rng).score
⋮----
# Log full eval as a trial so that optuna can learn from the new results
⋮----
# Log full evaluation results
⋮----
# Update best score and program if necessary
⋮----
best_score = full_eval_score
best_program = highest_mean_program.deepcopy()
</file>

<file path="dspy/teleprompt/random_search.py">
# TODO: Don't forget dealing with the raw demos.
# TODO: Deal with the (pretty common) case of having a metric for filtering and a separate metric for eval.
# The metric itself may tell though by the presence of trace.
⋮----
# TODO: This function should take a max_budget and max_teacher_budget. That's in the number of program calls.
# In this case, max_student_budget is max_budget - max_teacher_budget.
# For max_teacher_budget, this will just limit the total number of things we bootstrap.
# This can end up implicitly defining the number of candidate programs (i.e., stop when runs out). Cap at 16.
# For max_student_budget, this will be a more upfront calculation.
# Right now, it can also just induce the number of candidate programs. Later, it could be used more interestingly
# for selective early stopping.
# Progressive elimination sounds about right: after 50 examples, drop bottom third, after 100, another third, etc.
# until only 3--5 are left for the end. Could also be systematic and add (earlier) stopping based on error bounds.
# In general, though, the early filtering is just saying: either there are some really bad ones, or some really really
# good ones, or most things are pretty close. In all of these cases, dropping the bottom third is not going to hurt.
⋮----
class BootstrapFewShotWithRandomSearch(Teleprompter)
⋮----
def compile(self, student, *, teacher=None, trainset, valset=None, restrict=None, labeled_sample=True)
⋮----
self.valset = valset or trainset  # TODO: FIXME: Note this choice.
⋮----
effective_max_errors = self.max_errors if self.max_errors is not None else dspy.settings.max_errors
⋮----
scores = []
all_subscores = []
score_data = []
⋮----
trainset_copy = list(self.trainset)
⋮----
# zero-shot
program = student.reset_copy()
⋮----
# labels only
teleprompter = LabeledFewShot(k=self.max_labeled_demos)
program = teleprompter.compile(student, trainset=trainset_copy, sample=labeled_sample)
⋮----
# unshuffled few-shot
optimizer = BootstrapFewShot(
program = optimizer.compile(student, teacher=teacher, trainset=trainset_copy)
⋮----
size = random.Random(seed).randint(self.min_num_samples, self.max_num_samples)
⋮----
evaluate = Evaluate(
⋮----
result = evaluate(program)
⋮----
best_program = program
⋮----
# To best program, attach all program candidates in decreasing average score
⋮----
# sample between 4 and 10 examples from traces
# TODO: FIXME: The max number of demos should be determined in part by the LM's tokenizer + max_length.
# This does require executing the program, or at least the predictor.
# # # # # # (Actually we can just combine the token counts of the traces, when formatted via signature/adapter).
# Alternatively, we can keep track of the (zero-shot) number of tokens when we bootstrap.
# As another option, we can just try a wide range and handle failures as penalties on the score.
# The number "24" of traces to collect can also be affected. If we only need 3x10, some overlap is ok.
# We can also consider having short_demos and long_demos.
</file>

<file path="dspy/teleprompt/signature_opt.py">
"""
===============================================================
DEPRECATED!!!
PLEASE USE COPRO INSTEAD.
===============================================================

USAGE SUGGESTIONS:

The following code can be used to compile a optimized signature teleprompter, and evaluate it on an end task:

teleprompter = SignatureOptimizer(prompt_model=prompt_model, metric=metric, breadth=BREADTH, depth=DEPTH, init_temperature=INIT_TEMPERATURE)
kwargs = dict(num_threads=NUM_THREADS, display_progress=True, display_table=0)
compiled_prompt_opt = teleprompter.compile(program.deepcopy(), devset=devset[:DEV_NUM], eval_kwargs=kwargs)
eval_score = evaluate(compiled_prompt_opt, devset=evalset[:EVAL_NUM], **kwargs)

Note that this teleprompter takes in the following parameters:

* prompt_model: The model used for prompt generation. When unspecified, defaults to the model set in settings (ie. dspy.configure(lm=task_model)).
* metric: The task metric used for optimization.
* breadth: The number of new prompts to generate at each iteration. Default=10.
* depth: The number of times we should ask our prompt model to generate new prompts, with the history of the past prompts as input. Default=3.
* init_temperature: The temperature used to generate new prompts. Higher roughly equals more creative. Default=1.4.
* verbose: Tells the method whether or not to print intermediate steps.
* track_stats: Tells the method whether or not to track statistics about the optimization process.
                If True, the method will track the following statistics:
                    * results_best: The min,max,avg,stddev of top 10 scores for each predictor at each depth.
                    * results_latest: The min,max,avg,stddev of newest prompt scores for each predictor at each depth.
                    * total_calls: The total number of calls to the task metric.
                These statistics will be returned as attributes of the best program.
"""
⋮----
class SignatureOptimizer(COPRO)
⋮----
def compile(self, student, *, devset, eval_kwargs)
</file>

<file path="dspy/teleprompt/simba_utils.py">
logger = logging.getLogger(__name__)
⋮----
def prepare_models_for_resampling(program: dspy.Module, n: int, teacher_settings: dict | None = None)
⋮----
lm = program.get_lm() or dspy.settings.lm
⋮----
start_rollout_id = lm.kwargs.get("rollout_id", 0)
rollout_ids = [start_rollout_id + i for i in range(n)]
⋮----
# If we have a teacher model, use this as the first model
⋮----
teacher_lm = teacher_settings.get("lm") or lm
⋮----
# The rest of the models are just copies of the base model
⋮----
def wrap_program(program: dspy.Module, metric: Callable)
⋮----
def wrapped_program(example)
⋮----
prediction = program(**example.inputs())
⋮----
trace = dspy.settings.trace.copy()
⋮----
output = None
score = 0.0
output_metadata = {}
⋮----
output = metric(example, prediction)
⋮----
score = output
⋮----
score = output.score
# Extract fields from the output dspy.Prediction, excluding `score``
output_metadata = {
⋮----
def append_a_demo(demo_input_field_maxlen)
⋮----
def append_a_demo_(bucket, system, **kwargs)
⋮----
batch_10p_score = kwargs["batch_10p_score"]
⋮----
good = bucket[0]
trace = good["trace"]
name2demo = {}
⋮----
demo = dspy.Example(augmented=True, **_inputs, **_outputs)
name = predictor2name[id(predictor)]
name2demo[name] = demo  # keep the last demo for each predictor
⋮----
predictor = name2predictor[name]
⋮----
def append_a_rule(bucket, system, **kwargs)
⋮----
predictor2name = kwargs["predictor2name"]
⋮----
prompt_model = kwargs["prompt_model"] or dspy.settings.lm
⋮----
module_names = [name for name, _ in system.named_predictors()]
⋮----
example = good["example"]
⋮----
better_trajectory = [
worse_trajectory = [
⋮----
kwargs = {
⋮----
kwargs = {k: v if isinstance(v, str) else orjson.dumps(recursive_mask(v), option=orjson.OPT_INDENT_2).decode()
⋮----
advice_program = dspy.Predict(OfferFeedback)
advice = advice_program(**kwargs).module_advice
⋮----
instructions = predictor.signature.instructions + "\n\n" + advice[name]
⋮----
class OfferFeedback(dspy.Signature)
⋮----
"""
    You will be given two trajectories of an LLM-driven program's execution. Your goal is to help the program's modules
    build up experience on how to maximize the reward value assigned to the program's outputs if it were to receive
    similar inputs in the future.

    The module won't see its own history. It will rely on your advice balancing being concrete and being generalizable.

    In your advice:
    - Avoid boilerplate. Offer advice that would change the module's behavior for the better in the future.
    - Ensure that advice offered to a module M is specific to that M's specific sub-task, not the overall program.
    - Rely on contrasting the behavior of the worse trajectory against the better trajectory in making recommendations.
    - Ensure each unique module name appears exactly once as a key in the advice dictionary.
    """
⋮----
program_code: str = InputField(desc="The code of the program that we are analyzing")
modules_defn: str = InputField(desc="The definition of each module in the program, including its I/O")
program_inputs: str = InputField(desc="The inputs to the program that we are analyzing")
oracle_metadata: str = InputField(desc="Any (hidden) metadata about the training set instance we're analyzing")
worse_program_trajectory: str = InputField(
worse_program_outputs: str = InputField(desc="The outputs of the program that we are analyzing")
worse_reward_value: float = InputField(desc="The reward value assigned to the program's outputs")
worse_reward_info: str = InputField(desc="Additional information that might be helpful to understanding the assigned reward value.")
better_program_trajectory: str = InputField(
better_program_outputs: str = InputField(desc="The outputs of the program that we are analyzing")
better_reward_value: float = InputField(desc="The reward value assigned to the program's outputs")
better_reward_info: str = InputField(desc="Additional information that might be helpful to understanding the assigned reward value.")
module_names: list[str] = InputField(desc="The names of the modules in the program, for which we seek advice")
discussion: str = OutputField(desc="Discussing blame of where each module went wrong, if it did")
module_advice: dict[str, str] = OutputField(
⋮----
def inspect_modules(program)
⋮----
separator = "-" * 80
output = [separator]
⋮----
signature = predictor.signature
instructions = textwrap.dedent(signature.instructions)
instructions = ("\n" + "\t" * 2).join([""] + instructions.splitlines())
⋮----
def recursive_mask(o)
⋮----
# If the object is already serializable, return it.
⋮----
# If it's a dictionary, apply recursively to its values.
⋮----
# If it's a list, apply recursively.
⋮----
# If it's a tuple, apply recursively.
⋮----
# Otherwise, replace it with a placeholder string (or use repr(o)).
</file>

<file path="dspy/teleprompt/simba.py">
logger = logging.getLogger(__name__)
⋮----
class SIMBA(Teleprompter)
⋮----
"""
    SIMBA (Stochastic Introspective Mini-Batch Ascent) optimizer for DSPy.
    
    SIMBA is a DSPy optimizer that uses the LLM to analyze its own performance and 
    generate improvement rules. It samples mini-batches, identifies challenging examples 
    with high output variability, then either creates self-reflective rules or adds 
    successful examples as demonstrations.
    
    For more details, see: https://dspy.ai/api/optimizers/SIMBA/
    """
⋮----
"""
        Initializes SIMBA.

        Args:
            metric: A function that takes an Example and a prediction_dict
                as input and returns a float.
            bsize: Mini-batch size. Defaults to 32.
            num_candidates: Number of new candidate programs to produce
                per iteration. Defaults to 6.
            max_steps: Number of optimization steps to run. Defaults to 8.
            max_demos: Maximum number of demos a predictor can hold
                before dropping some. Defaults to 4.
            prompt_model: The model to use to evolve the program. When `prompt_model is None`, the globally configured
                lm is used.
            teacher_settings: Settings for the teacher model. Defaults to None.
            demo_input_field_maxlen: Maximum number of characters to keep
                in an input field when building a new demo. Defaults to 100,000.
            num_threads: Number of threads for parallel execution.
                Defaults to None.
            temperature_for_sampling: Temperature used for picking
                programs during the trajectory-sampling step. Defaults to 0.2.
            temperature_for_candidates: Temperature used for picking
                the source program for building new candidates. Defaults to 0.2.
        """
⋮----
"""
        Compile and optimize the student module using SIMBA.
        
        Args:
            student: The module to optimize
            trainset: Training examples for optimization
            seed: Random seed for reproducibility
            
        Returns:
            The optimized module with candidate_programs and trial_logs attached
        """
# Basic checks
⋮----
# Initialize RNG
rng = random.Random(seed)
rng_np = np.random.default_rng(seed)
⋮----
programs = []
program_scores = {}
next_program_idx = 0
⋮----
# Helper functions
def calc_average_score(prog_idx: int) -> float
⋮----
scores = program_scores.get(prog_idx, [])
⋮----
def top_k_plus_baseline(k: int) -> list[int]
⋮----
# Sort all programs by descending average score
scored_programs = sorted(programs, key=lambda p: calc_average_score(p.simba_idx), reverse=True)
top_k = [p.simba_idx for p in scored_programs[:k]]
# Ensure baseline=0 is in there:
⋮----
def softmax_sample(rng_obj: random.Random, program_idxs: list[int], temperature: float) -> int
⋮----
# Unnormalized weights
scores = [calc_average_score(idx) for idx in program_idxs]
exps = [np.exp(s / temperature) for s in scores]
sum_exps = sum(exps)
⋮----
# Fallback: uniform if all exps are zero
⋮----
# Weighted random choice
probs = [val / sum_exps for val in exps]
⋮----
def register_new_program(prog: dspy.Module, score_list: list[float]) -> None
⋮----
new_idx = next_program_idx
⋮----
# Initialize the baseline program: index=0
student = student.deepcopy()
⋮----
winning_programs = [student]
⋮----
# Data shuffling
data_indices = list(range(len(trainset)))
⋮----
instance_idx = 0
⋮----
# Parallel runner
run_parallel = dspy.Parallel(access_examples=False, num_threads=self.num_threads)
⋮----
trial_logs = {}
⋮----
# STEP 1: Get next batch
⋮----
batch_indices = data_indices[instance_idx : instance_idx + self.bsize]
batch = [trainset[i] for i in batch_indices]
⋮----
# We'll generate (program, model) pairs for the trajectory sampling.
# Prepare distinct LMs (with different temperatures, etc.) from the baseline=programs[0].
models = prepare_models_for_resampling(programs[0], self.num_candidates, self.teacher_settings)
top_programs = top_k_plus_baseline(self.num_candidates)
⋮----
exec_pairs = []
predictor2name = {}
⋮----
# For each model, for each example, pick a program from the pool via softmax
⋮----
chosen_prog_idx = softmax_sample(rng, top_programs, self.temperature_for_sampling)
candidate_system = programs[chosen_prog_idx].deepcopy()
⋮----
# Use the special wrap that includes the 'example' in the output
wrapped_candidate_system = wrap_program(candidate_system, self.metric)
⋮----
# STEP 2: Execute
⋮----
outputs = run_parallel(exec_pairs)
⋮----
# STEP 3: Sort the training buckets by (max-to-min gap, max score, and max-to-avg gap).
buckets = []
largest_max_to_avg_gap = float("-inf")
batch_10th_percentile_score = np.percentile([float(o["score"]) for o in outputs], 10)
batch_90th_percentile_score = np.percentile([float(o["score"]) for o in outputs], 90)
⋮----
# We'll chunk `outputs` by example index, each chunk has length = num_candidates
⋮----
# gather all results for this example
bucket = [outputs[i] for i in range(idx, len(outputs), self.bsize)]
⋮----
max_score = float(bucket[0]["score"])
min_score = float(bucket[-1]["score"])
avg_score = sum(x["score"] for x in bucket) / len(bucket)
max_to_min_gap = max_score - min_score
max_to_avg_gap = max_score - avg_score
⋮----
largest_max_to_avg_gap = max_to_avg_gap
⋮----
# sort the buckets
⋮----
# Baseline for the batch is just the average of all runs
all_scores_in_this_batch = [o["score"] for o in outputs]
baseline_score = sum(all_scores_in_this_batch) / len(all_scores_in_this_batch)
⋮----
# STEP 4: Build new candidate programs by applying a strategy to some top buckets.
system_candidates = []
⋮----
# pick source program
src_prog_idx = softmax_sample(
system_candidate = programs[src_prog_idx].deepcopy()
⋮----
# Drop some demos from each predictor
name2predictor = {}
num_demos_list = []
⋮----
max_demos_tmp = self.max_demos if self.max_demos > 0 else 3
⋮----
num_demos = max(num_demos_list) if num_demos_list else 0
num_demos_to_drop = max(rng_np.poisson(num_demos / max_demos_tmp), int(num_demos >= max_demos_tmp))
num_demos_to_drop = min(num_demos_to_drop, num_demos)
demos_to_drop = [rng.randrange(num_demos) for _ in range(num_demos_to_drop)]
⋮----
# Pick a strategy
strategy = rng.choice(self.strategies)
⋮----
# STEP 5: Evaluate these new system_candidates on the same mini-batch
⋮----
exec_pairs = [(wrap_program(sys, self.metric), ex) for sys in system_candidates for ex in batch]
⋮----
# STEP 6: Compute average mini-batch scores for each new candidate
candidate_scores = []
⋮----
start = idx_cand * self.bsize
end = (idx_cand + 1) * self.bsize
sys_scores = [outputs[i]["score"] for i in range(start, end)]
avg_sys_score = sum(sys_scores) / len(sys_scores)
⋮----
# STEP 7: Select the best among these new ones for "winning" record
⋮----
best_idx_among_candidates = candidate_scores.index(max(candidate_scores))
best_program = system_candidates[best_idx_among_candidates]
⋮----
# STEP 8: Register all new candidate systems in our global pool
⋮----
M = len(winning_programs) - 1  # noqa: N806
N = self.num_candidates + 1  # noqa: N806
⋮----
program_idxs = [0] * N
⋮----
program_idxs = [round(i * M / (N - 1)) for i in range(N)]
⋮----
program_idxs = list(dict.fromkeys(program_idxs))
⋮----
candidate_programs = [winning_programs[i].deepcopy() for i in program_idxs]
⋮----
exec_pairs = [(wrap_program(sys, self.metric), ex) for sys in candidate_programs for ex in trainset]
⋮----
scores = []
⋮----
start = idx_prog * len(trainset)
end = (idx_prog + 1) * len(trainset)
⋮----
avg_score = sum(sys_scores) / len(sys_scores) if sys_scores else 0.0
⋮----
# Build sorted list of {"score", "program"} dicts
⋮----
candidate_data = [{"score": s, "program": p} for s, p in zip(scores, candidate_programs, strict=False)]
⋮----
best_idx = scores.index(max(scores)) if scores else 0
best_program = candidate_programs[best_idx].deepcopy()
⋮----
# Attach sorted, scored candidates & logs
</file>

<file path="dspy/teleprompt/teleprompt_optuna.py">
class BootstrapFewShotWithOptuna(Teleprompter)
⋮----
# self.max_num_traces = 1 + int(max_bootstrapped_demos / 2.0 * self.num_candidate_sets)
⋮----
# Semi-hacky way to get the parent class's _bootstrap function to stop early.
# self.max_bootstrapped_demos = self.max_num_traces
⋮----
# print("Going to sample", self.max_num_traces, "traces in total.")
⋮----
def objective(self, trial)
⋮----
program2 = self.student.reset_copy()
⋮----
all_demos = compiled_predictor.demos
demo_index = trial.suggest_int(f"demo_index_for_{name}", 0, len(all_demos) - 1)
selected_demo = dict(all_demos[demo_index])
⋮----
evaluate = Evaluate(
result = evaluate(program2)
⋮----
def compile(self, student, *, teacher=None, max_demos, trainset, valset=None)
⋮----
teleprompter_optimize = BootstrapFewShot(
⋮----
study = optuna.create_study(direction="maximize")
⋮----
best_program = study.trials[study.best_trial.number].user_attrs["program"]
</file>

<file path="dspy/teleprompt/teleprompt.py">
class Teleprompter
⋮----
def __init__(self)
⋮----
def compile(self, student: Module, *, trainset: list[Example], teacher: Module | None = None, valset: list[Example] | None = None, **kwargs) -> Module
⋮----
"""
        Optimize the student program.

        Args:
            student: The student program to optimize.
            trainset: The training set to use for optimization.
            teacher: The teacher program to use for optimization.
            valset: The validation set to use for optimization.

        Returns:
            The optimized student program.
        """
⋮----
def get_params(self) -> dict[str, Any]
⋮----
"""
        Get the parameters of the teleprompter.

        Returns:
            The parameters of the teleprompter.
        """
</file>

<file path="dspy/teleprompt/utils.py">
# Won't be able to read code from jupyter notebooks
extract_symbols = None
⋮----
"""
This file consists of helper functions for our variety of optimizers.
"""
⋮----
### OPTIMIZER TRAINING UTILS ###
⋮----
logger = logging.getLogger(__name__)
⋮----
def create_minibatch(trainset, batch_size=50, rng=None)
⋮----
"""Create a minibatch from the trainset."""
⋮----
# Ensure batch_size isn't larger than the size of the dataset
batch_size = min(batch_size, len(trainset))
⋮----
# If no RNG is provided, fall back to the global random instance
rng = rng or random
⋮----
# Randomly sample indices for the mini-batch using the provided rng
sampled_indices = rng.sample(range(len(trainset)), batch_size)
⋮----
# Create the mini-batch using the sampled indices
minibatch = [trainset[i] for i in sampled_indices]
⋮----
def eval_candidate_program(batch_size, trainset, candidate_program, evaluate, rng=None)
⋮----
"""Evaluate a candidate program on the trainset, using the specified batch size."""
⋮----
# Evaluate on the full trainset
⋮----
# Or evaluate on a minibatch
⋮----
# TODO: Handle this better, as -ve scores are possible
⋮----
"""Evaluation of candidate_program with pruning implemented"""
⋮----
# Evaluate with the new prompts
total_score = 0
num_batches = math.ceil(len(trainset) / batch_size)
total_eval_size = 0
⋮----
start_index = i * batch_size
end_index = min((i + 1) * batch_size, len(trainset))
split_trainset = trainset[start_index:end_index]
split_score = evaluate(
⋮----
curr_weighted_avg_score = total_score / min((i + 1) * batch_size, len(trainset))
⋮----
# Handle pruning based on the intermediate value.
⋮----
score = curr_weighted_avg_score
⋮----
def get_program_with_highest_avg_score(param_score_dict, fully_evaled_param_combos)
⋮----
"""Used as a helper function for bayesian + minibatching optimizers. Returns the program with the highest average score from the batches evaluated so far."""
⋮----
# Calculate the mean for each combination of categorical parameters, based on past trials
results = []
⋮----
scores = np.array([v[0] for v in values])
mean = np.average(scores)
program = values[0][1]
params = values[0][2]
⋮----
# Sort results by the mean
sorted_results = sorted(results, key=lambda x: x[1], reverse=True)
⋮----
# Find the combination with the highest mean, skip fully evaluated ones
⋮----
# If no valid program is found, we return the last valid one that we found
⋮----
"""
    Calculate the average and best quality of the last n programs proposed. This is useful for seeing if our proposals
    are actually 'improving' overtime or not.
    """
# Get the trials from the last n keys in trial logs
last_n_trial_nums = list(trial_logs.keys())[-n:]
⋮----
# Calculate the average and best score of these trials
# if num_eval_calls in the trial is less than the trainset, throw a not-implemented error for now
total_train_score = 0
best_train_score = 0
total_dev_score = 0
best_dev_score = 0
⋮----
full_eval = trial_logs[trial_num]["full_eval"]
⋮----
train_score = trial_logs[trial_num]["score"]
program = base_program.deepcopy()
⋮----
dev_score = evaluate(program, devset=devset)
⋮----
best_train_score = train_score
best_dev_score = dev_score
⋮----
### LOGGING UTILS ###
⋮----
"""Get a full trace of the task model's history for a given candidate program."""
_ = evaluate(candidate_program, devset=devset[:1])
_ = task_model.inspect_history(n=len(candidate_program.predictors()))
⋮----
def print_full_program(program)
⋮----
"""Print out the program's instructions & prefixes for each module."""
⋮----
def save_candidate_program(program, log_dir, trial_num, note=None)
⋮----
"""Save the candidate program to the log directory."""
⋮----
# Ensure the directory exists
eval_programs_dir = os.path.join(log_dir, "evaluated_programs")
⋮----
# Define the save path for the program
⋮----
save_path = os.path.join(eval_programs_dir, f"program_{trial_num}_{note}.json")
⋮----
save_path = os.path.join(eval_programs_dir, f"program_{trial_num}.json")
⋮----
# Save the program
⋮----
def save_file_to_log_dir(source_file_path, log_dir)
⋮----
"""Save a file to our log directory"""
⋮----
destination_file_path = os.path.join(log_dir, os.path.basename(source_file_path))
⋮----
# Copy the file
⋮----
def setup_logging(log_dir)
⋮----
"""Setup logger, which will log our print statements to a txt file at our log_dir for later viewing"""
⋮----
# Create a logger
logger = logging.getLogger()
⋮----
# Create a file handler that logs debug and higher level messages
file_handler = logging.FileHandler(f"{log_dir}/logs.txt")
⋮----
file_formatter = logging.Formatter("%(asctime)s - %(message)s")
⋮----
# Create a console handler with a higher log level
console_handler = logging.StreamHandler()
⋮----
console_formatter = logging.Formatter("%(message)s")
⋮----
def get_token_usage(model) -> tuple[int, int]
⋮----
"""
    Extract total input tokens and output tokens from a model's interaction history.
    Returns (total_input_tokens, total_output_tokens).
    """
⋮----
input_tokens = []
output_tokens = []
⋮----
usage = interaction.get("usage", {})
_input_tokens = usage.get("prompt_tokens", 0)
_output_tokens = usage.get("completion_tokens", 0)
⋮----
total_input_tokens = int(np.sum(input_tokens))
total_output_tokens = int(np.sum(output_tokens))
⋮----
def log_token_usage(trial_logs, trial_num, model_dict)
⋮----
"""
    Extract total input and output tokens used by each model and log to trial_logs[trial_num]["token_usage"].
    """
⋮----
token_usage_dict = {}
⋮----
# Store token usage info in trial logs
⋮----
### OTHER UTILS ###
⋮----
def get_prompt_model(prompt_model)
⋮----
def get_signature(predictor)
⋮----
def set_signature(predictor, updated_signature)
⋮----
"""
    This function is copied from random_search.py, and creates fewshot examples in the same way that random search does.
    This allows us to take advantage of using the same fewshot examples when we use the same random seed in our optimizers.
    """
max_errors = dspy.settings.max_errors if max_errors is None else max_errors
demo_candidates = {}
⋮----
# Account for confusing way this is set up, where we add in 3 more candidate sets to the N specified
⋮----
# Initialize demo_candidates dictionary
⋮----
rng = rng or random.Random(seed)
⋮----
# Go through and create each candidate set
⋮----
trainset_copy = list(trainset)
⋮----
# zero-shot
program2 = student.reset_copy()
⋮----
# labels only
teleprompter = LabeledFewShot(k=max_labeled_demos)
program2 = teleprompter.compile(
⋮----
# unshuffled few-shot
program = BootstrapFewShot(
program2 = program.compile(student, teacher=teacher, trainset=trainset_copy)
⋮----
# shuffled few-shot
⋮----
size = rng.randint(min_num_samples, max_bootstrapped_demos)
⋮----
teleprompter = BootstrapFewShot(
⋮----
def old_getfile(object)
⋮----
"""Work out which source or compiled file an object was defined in."""
⋮----
module = sys.modules.get(object.__module__)
⋮----
object = object.__func__
⋮----
object = object.__code__
⋮----
object = object.tb_frame
⋮----
object = object.f_code
⋮----
def new_getfile(object)
⋮----
# Lookup by parent module (as in current inspect)
⋮----
object_ = sys.modules.get(object.__module__)
⋮----
# If parent module is __main__, lookup by methods (NEW)
</file>

<file path="dspy/teleprompt/vanilla.py">
class LabeledFewShot(Teleprompter)
⋮----
def __init__(self, k=16)
⋮----
def compile(self, student, *, trainset, sample=True)
⋮----
rng = random.Random(0)
⋮----
# NOTE: I believe templatev2 keeps rdemos as long as they have the last field.
# This may change later, especially with the introduction of required vs optional fields.
# NOTE: Since we're relying on downstream code to handle the demos, this sampling may be sub-sampled.
</file>

<file path="dspy/utils/__init__.py">
def download(url)
⋮----
filename = os.path.basename(url)
remote_size = int(requests.head(url, allow_redirects=True).headers.get("Content-Length", 0))
local_size = os.path.getsize(filename) if os.path.exists(filename) else 0
⋮----
__all__ = [
</file>

<file path="dspy/utils/annotation.py">
P = ParamSpec("P")
R = TypeVar("R")
⋮----
@overload
def experimental(f: Callable[P, R], version: str | None = None) -> Callable[P, R]: ...
⋮----
@overload
def experimental(f: None = None, version: str | None = None) -> Callable[[Callable[P, R]], Callable[P, R]]: ...
⋮----
"""Decorator / decorator creator for marking APIs experimental in the docstring.

    Args:
        f: The function to be decorated.
        version: The version in which the API was introduced as experimental.
            The version is used to determine whether the API should be considered
            as stable or not when releasing a new version of DSPy.

    Returns:
        A decorator that adds a note to the docstring of the decorated API.
    """
⋮----
def decorator(f: Callable[P, R]) -> Callable[P, R]
⋮----
def _experimental(api: Callable[P, R], version: str | None = None) -> Callable[P, R]
⋮----
"""Add experimental notice to the API's docstring."""
⋮----
api_type = "class"
⋮----
api_type = "function"
⋮----
api_type = "property"
⋮----
api_type = "method"
⋮----
api_type = str(type(api))
⋮----
indent = _get_min_indent_of_docstring(api.__doc__) if api.__doc__ else ""
⋮----
version_text = f" (introduced in v{version})" if version else ""
notice = (
⋮----
def _get_min_indent_of_docstring(docstring_str: str) -> str
⋮----
"""
    Get the minimum indentation string of a docstring, based on the assumption
    that the closing triple quote for multiline comments must be on a new line.
    Note that based on ruff rule D209, the closing triple quote for multiline
    comments must be on a new line.

    Args:
        docstring_str: string with docstring

    Returns:
        Whitespace corresponding to the indent of a docstring.
    """
⋮----
match = re.match(r"^\s*", docstring_str.rsplit("\n", 1)[-1])
</file>

<file path="dspy/utils/asyncify.py">
_limiter = None
⋮----
def get_async_max_workers()
⋮----
def get_limiter()
⋮----
async_max_workers = get_async_max_workers()
⋮----
_limiter = CapacityLimiter(async_max_workers)
⋮----
def asyncify(program: "Module") -> Callable[[Any, Any], Awaitable[Any]]
⋮----
"""
    Wraps a DSPy program so that it can be called asynchronously. This is useful for running a
    program in parallel with another task (e.g., another DSPy program).

    This implementation propagates the current thread's configuration context to the worker thread.

    Args:
        program: The DSPy program to be wrapped for asynchronous execution.

    Returns:
        An async function: An async function that, when awaited, runs the program in a worker thread.
            The current thread's configuration context is inherited for each call.
    """
⋮----
async def async_program(*args, **kwargs) -> Any
⋮----
# Capture the current overrides at call-time.
⋮----
parent_overrides = thread_local_overrides.get().copy()
⋮----
def wrapped_program(*a, **kw)
⋮----
original_overrides = thread_local_overrides.get()
token = thread_local_overrides.set({**original_overrides, **parent_overrides.copy()})
⋮----
# Create a fresh asyncified callable each time, ensuring the latest context is used.
call_async = asyncer.asyncify(wrapped_program, abandon_on_cancel=True, limiter=get_limiter())
</file>

<file path="dspy/utils/caching.py">
_DEFAULT_CACHE_DIR = os.path.join(Path.home(), ".dspy_cache")
DSPY_CACHEDIR = os.environ.get("DSPY_CACHEDIR") or _DEFAULT_CACHE_DIR
⋮----
def create_subdir_in_cachedir(subdir: str) -> str
⋮----
"""Create a subdirectory in the DSPy cache directory."""
subdir = os.path.join(DSPY_CACHEDIR, subdir)
subdir = os.path.abspath(subdir)
</file>

<file path="dspy/utils/callback.py">
ACTIVE_CALL_ID = ContextVar("active_call_id", default=None)
⋮----
logger = logging.getLogger(__name__)
⋮----
class BaseCallback
⋮----
"""A base class for defining callback handlers for DSPy components.

    To use a callback, subclass this class and implement the desired handlers. Each handler
    will be called at the appropriate time before/after the execution of the corresponding component.  For example, if
    you want to print a message before and after an LM is called, implement `the on_llm_start` and `on_lm_end` handler.
    Users can set the callback globally using `dspy.configure` or locally by passing it to the component
    constructor.


    Example 1: Set a global callback using `dspy.configure`.

    ```
    import dspy
    from dspy.utils.callback import BaseCallback

    class LoggingCallback(BaseCallback):

        def on_lm_start(self, call_id, instance, inputs):
            print(f"LM is called with inputs: {inputs}")

        def on_lm_end(self, call_id, outputs, exception):
            print(f"LM is finished with outputs: {outputs}")

    dspy.configure(
        callbacks=[LoggingCallback()]
    )

    cot = dspy.ChainOfThought("question -> answer")
    cot(question="What is the meaning of life?")

    # > LM is called with inputs: {'question': 'What is the meaning of life?'}
    # > LM is finished with outputs: {'answer': '42'}
    ```

    Example 2: Set a local callback by passing it to the component constructor.

    ```
    lm_1 = dspy.LM("gpt-3.5-turbo", callbacks=[LoggingCallback()])
    lm_1(question="What is the meaning of life?")

    # > LM is called with inputs: {'question': 'What is the meaning of life?'}
    # > LM is finished with outputs: {'answer': '42'}

    lm_2 = dspy.LM("gpt-3.5-turbo")
    lm_2(question="What is the meaning of life?")
    # No logging here because only `lm_1` has the callback set.
    ```
    """
⋮----
"""A handler triggered when forward() method of a module (subclass of dspy.Module) is called.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            instance: The Module instance.
            inputs: The inputs to the module's forward() method. Each arguments is stored as
                a key-value pair in a dictionary.
        """
⋮----
"""A handler triggered after forward() method of a module (subclass of dspy.Module) is executed.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            outputs: The outputs of the module's forward() method. If the method is interrupted by
                an exception, this will be None.
            exception: If an exception is raised during the execution, it will be stored here.
        """
⋮----
"""A handler triggered when __call__ method of dspy.LM instance is called.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            instance: The LM instance.
            inputs: The inputs to the LM's __call__ method. Each arguments is stored as
                a key-value pair in a dictionary.
        """
⋮----
"""A handler triggered after __call__ method of dspy.LM instance is executed.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            outputs: The outputs of the LM's __call__ method. If the method is interrupted by
                an exception, this will be None.
            exception: If an exception is raised during the execution, it will be stored here.
        """
⋮----
"""A handler triggered when format() method of an adapter (subclass of dspy.Adapter) is called.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            instance: The Adapter instance.
            inputs: The inputs to the Adapter's format() method. Each arguments is stored as
                a key-value pair in a dictionary.
        """
⋮----
"""A handler triggered after format() method of an adapter (subclass of dspy.Adapter) is called..

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            outputs: The outputs of the Adapter's format() method. If the method is interrupted
                by an exception, this will be None.
            exception: If an exception is raised during the execution, it will be stored here.
        """
⋮----
"""A handler triggered when parse() method of an adapter (subclass of dspy.Adapter) is called.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            instance: The Adapter instance.
            inputs: The inputs to the Adapter's parse() method. Each arguments is stored as
                a key-value pair in a dictionary.
        """
⋮----
"""A handler triggered after parse() method of an adapter (subclass of dspy.Adapter) is called.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            outputs: The outputs of the Adapter's parse() method. If the method is interrupted
                by an exception, this will be None.
            exception: If an exception is raised during the execution, it will be stored here.
        """
⋮----
"""A handler triggered when a tool is called.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            instance: The Tool instance.
            inputs: The inputs to the Tool's __call__ method. Each arguments is stored as
                a key-value pair in a dictionary.
        """
⋮----
"""A handler triggered after a tool is executed.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            outputs: The outputs of the Tool's __call__ method. If the method is interrupted by
                an exception, this will be None.
            exception: If an exception is raised during the execution, it will be stored here.
        """
⋮----
"""A handler triggered when evaluation is started.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            instance: The Evaluate instance.
            inputs: The inputs to the Evaluate's __call__ method. Each arguments is stored as
                a key-value pair in a dictionary.
        """
⋮----
"""A handler triggered after evaluation is executed.

        Args:
            call_id: A unique identifier for the call. Can be used to connect start/end handlers.
            outputs: The outputs of the Evaluate's __call__ method. If the method is interrupted by
                an exception, this will be None.
            exception: If an exception is raised during the execution, it will be stored here.
        """
⋮----
def with_callbacks(fn)
⋮----
"""Decorator to add callback functionality to instance methods."""
⋮----
def _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)
⋮----
"""Execute all start callbacks for a function call."""
inputs = inspect.getcallargs(fn, instance, *args, **kwargs)
⋮----
def _execute_end_callbacks(instance, fn, call_id, results, exception, callbacks)
⋮----
"""Execute all end callbacks for a function call."""
⋮----
def _get_active_callbacks(instance)
⋮----
"""Get combined global and instance-level callbacks."""
⋮----
@functools.wraps(fn)
        async def async_wrapper(instance, *args, **kwargs)
⋮----
callbacks = _get_active_callbacks(instance)
⋮----
call_id = uuid.uuid4().hex
⋮----
# Active ID must be set right before the function is called, not before calling the callbacks.
parent_call_id = ACTIVE_CALL_ID.get()
⋮----
results = None
exception = None
⋮----
results = await fn(instance, *args, **kwargs)
⋮----
exception = e
⋮----
@functools.wraps(fn)
        def sync_wrapper(instance, *args, **kwargs)
⋮----
results = fn(instance, *args, **kwargs)
⋮----
def _get_on_start_handler(callback: BaseCallback, instance: Any, fn: Callable) -> Callable
⋮----
"""Selects the appropriate on_start handler of the callback based on the instance and function name."""
⋮----
# We treat everything else as a module.
⋮----
def _get_on_end_handler(callback: BaseCallback, instance: Any, fn: Callable) -> Callable
⋮----
"""Selects the appropriate on_end handler of the callback based on the instance and function name."""
</file>

<file path="dspy/utils/dummies.py">
class DummyLM(LM)
⋮----
"""Dummy language model for unit testing purposes.

    Three modes of operation:

    Mode 1: List of dictionaries

    If a list of dictionaries is provided, the dummy model will return the next dictionary
    in the list for each request, formatted according to the `format_field_with_value` function.

    Example:

    ```
    lm = DummyLM([{"answer": "red"}, {"answer": "blue"}])
    dspy.configure(lm=lm)
    predictor("What color is the sky?")
    # Output:
    # [[## answer ##]]
    # red
    predictor("What color is the sky?")
    # Output:
    # [[## answer ##]]
    # blue
    ```

    Mode 2: Dictionary of dictionaries

    If a dictionary of dictionaries is provided, the dummy model will return the value
    corresponding to the key which is contained with the final message of the prompt,
    formatted according to the `format_field_with_value` function from the chat adapter.

    ```
    lm = DummyLM({"What color is the sky?": {"answer": "blue"}})
    dspy.configure(lm=lm)
    predictor("What color is the sky?")
    # Output:
    # [[## answer ##]]
    # blue
    ```

    Mode 3: Follow examples

    If `follow_examples` is set to True, and the prompt contains an example input exactly equal to the prompt,
    the dummy model will return the output from that example.

    ```
    lm = DummyLM([{"answer": "red"}], follow_examples=True)
    dspy.configure(lm=lm)
    predictor("What color is the sky?, demos=dspy.Example(input="What color is the sky?", output="blue"))
    # Output:
    # [[## answer ##]]
    # blue
    ```

    """
⋮----
# Set adapter, defaulting to ChatAdapter
⋮----
adapter = ChatAdapter()
⋮----
def _use_example(self, messages)
⋮----
# find all field names
fields = defaultdict(int)
⋮----
# find the fields which are missing from the final turns
max_count = max(fields.values())
output_fields = [field for field, count in fields.items() if count != max_count]
⋮----
# get the output from the last turn that has the output fields as headers
final_input = messages[-1]["content"].split("\n\n")[0]
⋮----
@with_callbacks
    def __call__(self, prompt=None, messages=None, **kwargs)
⋮----
def format_answer_fields(field_names_and_values: dict[str, Any])
⋮----
fields_with_values = {
# The reason why DummyLM needs an adapter is because it needs to know which output format to mimic.
# Normally LMs should not have any knowledge of an adapter, because the output format is defined in the prompt.
adapter = self.adapter
⋮----
# Try to use role="assistant" if the adapter supports it (like JSONAdapter)
⋮----
# Fallback for adapters that don't support role parameter (like ChatAdapter)
⋮----
# Build the request.
outputs = []
⋮----
messages = messages or [{"role": "user", "content": prompt}]
kwargs = {**self.kwargs, **kwargs}
⋮----
current_output = self._use_example(messages)
⋮----
current_output = next(
⋮----
current_output = format_answer_fields(next(self.answers, {"answer": "No more responses"}))
⋮----
# Mock reasoning
⋮----
current_output = {
# Store the output
⋮----
# Logging, with removed api key & where `cost` is None on cache hit.
kwargs = {k: v for k, v in kwargs.items() if not k.startswith("api_")}
entry = {"prompt": prompt, "messages": messages, "kwargs": kwargs}
entry = {**entry, "outputs": outputs, "usage": 0}
entry = {**entry, "cost": 0}
⋮----
async def acall(self, prompt=None, messages=None, **kwargs)
⋮----
def get_convo(self, index)
⋮----
"""Get the prompt + answer from the ith message."""
⋮----
def dummy_rm(passages=()) -> callable
⋮----
def inner(query: str, *, k: int, **kwargs)
⋮----
max_length = max(map(len, passages)) + 100
vectorizer = DummyVectorizer(max_length)
passage_vecs = vectorizer(passages)
⋮----
query_vec = vectorizer([query])[0]
scores = passage_vecs @ query_vec
largest_idx = (-scores).argsort()[:k]
⋮----
class DummyVectorizer
⋮----
"""Simple vectorizer based on n-grams."""
⋮----
def __init__(self, max_length=100, n_gram=2)
⋮----
self.P = 10**9 + 7  # A large prime number
⋮----
def _hash(self, gram)
⋮----
"""Hashes a string using a polynomial hash function."""
h = 1
⋮----
h = h * coeff + ord(c)
⋮----
def __call__(self, texts: list[str]) -> np.ndarray
⋮----
vecs = []
⋮----
grams = [text[i : i + self.n_gram] for i in range(len(text) - self.n_gram + 1)]
vec = [0] * self.max_length
⋮----
vecs = np.array(vecs, dtype=np.float32)
⋮----
vecs /= np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-10  # Added epsilon to avoid division by zero
</file>

<file path="dspy/utils/exceptions.py">
class AdapterParseError(Exception)
⋮----
"""Exception raised when adapter cannot parse the LM response."""
⋮----
message = f"{message}\n\n" if message else ""
message = (
</file>

<file path="dspy/utils/hasher.py">
"""
The following class was pulled from the `datasets` package from Hugging Face.
The reason for vendoring this code is to avoid a hard dependency on `datasets`,
which is a large package that is not needed for the majority of use cases.

License: Apache License 2.0
Author: Hugging Face Inc.
URL: https://github.com/huggingface/datasets/blob/fa73ab472eecf9136a3daf7a0fbff16a3dffa7a6/src/datasets/fingerprint.py#L170
Changes: 2025-08-10 - Ran ruff to format the code to DSPy styles.
"""
class Hasher
⋮----
"""Hasher that accepts python objects as inputs."""
⋮----
dispatch: dict = {}
⋮----
def __init__(self)
⋮----
@classmethod
    def hash_bytes(cls, value: bytes | list[bytes]) -> str
⋮----
value = [value] if isinstance(value, bytes) else value
m = xxhash.xxh64()
⋮----
@classmethod
    def hash(cls, value: Any) -> str
⋮----
def update(self, value: Any) -> None
⋮----
header_for_update = f"=={type(value)}=="
value_for_update = self.hash(value)
⋮----
def hexdigest(self) -> str
</file>

<file path="dspy/utils/inspect_history.py">
def _green(text: str, end: str = "\n")
⋮----
def _red(text: str, end: str = "\n")
⋮----
def _blue(text: str, end: str = "\n")
⋮----
def pretty_print_history(history, n: int = 1)
⋮----
"""Prints the last n prompts and their completions."""
⋮----
messages = item["messages"] or [{"role": "user", "content": item["prompt"]}]
outputs = item["outputs"]
timestamp = item.get("timestamp", "Unknown time")
⋮----
image_str = ""
⋮----
len_base64 = len(c["image_url"]["url"].split("base64,")[1])
image_str = (
⋮----
image_str = f"<image_url: {c['image_url']['url']}>"
⋮----
audio_format = c["input_audio"]["format"]
len_audio = len(c["input_audio"]["data"])
audio_str = f"<audio format='{audio_format}' base64-encoded, length={len_audio}>"
⋮----
file = c.get("file", c.get("input_file", {}))
filename = file.get("filename", "")
file_id = file.get("file_id", "")
file_data = file.get("file_data", "")
file_str = f"<file: name:{filename}, id:{file_id}, data_length:{len(file_data)}>"
⋮----
choices_text = f" \t (and {len(outputs) - 1} other completions)"
</file>

<file path="dspy/utils/langchain_tool.py">
def convert_langchain_tool(tool: "BaseTool") -> Tool
⋮----
"""Build a DSPy tool from a LangChain tool.
    
    This function converts a LangChain tool (either created with @tool decorator
    or by subclassing BaseTool) into a DSPy Tool.

    Args:
        tool: The LangChain tool to convert.

    Returns:
        A DSPy Tool object.
    """
async def func(**kwargs)
⋮----
result = await tool.ainvoke(kwargs)
⋮----
# Get args_schema from the tool
# https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.BaseTool.html#langchain_core.tools.base.BaseTool.args_schema
args_schema = tool.args_schema
⋮----
# The args_schema of Langchain tool is a pydantic model, so we can get the type hints from the model fields
arg_types = {
</file>

<file path="dspy/utils/logging_utils.py">
LOGGING_LINE_FORMAT = "%(asctime)s %(levelname)s %(name)s: %(message)s"
LOGGING_DATETIME_FORMAT = "%Y/%m/%d %H:%M:%S"
⋮----
class DSPyLoggingStream
⋮----
"""
    A Python stream for use with event logging APIs throughout DSPy (`eprint()`,
    `logger.info()`, etc.). This stream wraps `sys.stderr`, forwarding `write()` and
    `flush()` calls to the stream referred to by `sys.stderr` at the time of the call.
    It also provides capabilities for disabling the stream to silence event logs.
    """
⋮----
def __init__(self)
⋮----
def write(self, text)
⋮----
def flush(self)
⋮----
@property
    def enabled(self)
⋮----
@enabled.setter
    def enabled(self, value)
⋮----
DSPY_LOGGING_STREAM = DSPyLoggingStream()
⋮----
def disable_logging()
⋮----
"""
    Disables the `DSPyLoggingStream` used by event logging APIs throughout DSPy
    (`eprint()`, `logger.info()`, etc), silencing all subsequent event logs.
    """
⋮----
def enable_logging()
⋮----
"""
    Enables the `DSPyLoggingStream` used by event logging APIs throughout DSPy
    (`eprint()`, `logger.info()`, etc), emitting all subsequent event logs. This
    reverses the effects of `disable_logging()`.
    """
⋮----
def configure_dspy_loggers(root_module_name)
⋮----
formatter = logging.Formatter(fmt=LOGGING_LINE_FORMAT, datefmt=LOGGING_DATETIME_FORMAT)
⋮----
dspy_handler_name = "dspy_handler"
handler = logging.StreamHandler(stream=DSPY_LOGGING_STREAM)
⋮----
logger = logging.getLogger(root_module_name)
</file>

<file path="dspy/utils/magicattr.py">
"""
Compatibility layer for magicattr that works with Python 3.14+

This module provides a patched version of magicattr's functionality
that is compatible with Python 3.14's removal of ast.Num and ast.Str.

Based on magicattr 0.1.6 by Jairus Martin (MIT License)
https://github.com/frmdstryr/magicattr
"""
⋮----
_AST_TYPES = (ast.Name, ast.Attribute, ast.Subscript, ast.Call)
_STRING_TYPE = str
⋮----
def get(obj, attr, **kwargs)
⋮----
"""A getattr that supports nested lookups on objects, dicts, lists, and
    any combination in between.
    """
⋮----
obj = _lookup(obj, chunk)
⋮----
def set(obj, attr, val)
⋮----
"""A setattr that supports nested lookups on objects, dicts, lists, and
    any combination in between.
    """
⋮----
def delete(obj, attr)
⋮----
"""A delattr that supports deletion of a nested lookups on objects,
    dicts, lists, and any combination in between.
    """
⋮----
def lookup(obj, attr)
⋮----
"""Like get but instead of returning the final value it returns the
    object and action that will be done.
    """
nodes = tuple(_parse(attr))
⋮----
obj = reduce(_lookup, nodes[:-1], obj)
node = nodes[-1]
⋮----
node = nodes[0]
⋮----
def _parse(attr)
⋮----
"""Parse and validate an attr string"""
⋮----
nodes = ast.parse(attr).body
⋮----
def _lookup_subscript_value(node)
⋮----
"""Lookup the value of ast node on the object.

    Compatible with Python 3.14+ which removed ast.Num and ast.Str
    """
⋮----
node = node.value
⋮----
# Python 3.14+ uses ast.Constant for all constants
⋮----
# Fallback for older Python versions
⋮----
# Handle numeric indexes
⋮----
# Handle string keys
⋮----
# Handle negative indexes
⋮----
operand = node.operand
⋮----
# Fallback for older Python
⋮----
def _lookup(obj, node)
⋮----
"""Lookup the given ast node on the object."""
</file>

<file path="dspy/utils/mcp.py">
def _convert_mcp_tool_result(call_tool_result: "mcp.types.CallToolResult") -> str | list[Any]
⋮----
text_contents: list[TextContent] = []
non_text_contents = []
⋮----
tool_content = [content.text for content in text_contents]
⋮----
tool_content = tool_content[0]
⋮----
def convert_mcp_tool(session: "mcp.ClientSession", tool: "mcp.types.Tool") -> Tool
⋮----
"""Build a DSPy tool from an MCP tool.

    Args:
        session: The MCP session to use.
        tool: The MCP tool to convert.

    Returns:
        A dspy Tool object.
    """
⋮----
# Convert the MCP tool and Session to a single async method
async def func(*args, **kwargs)
⋮----
result = await session.call_tool(tool.name, arguments=kwargs)
</file>

<file path="dspy/utils/parallelizer.py">
logger = logging.getLogger(__name__)
⋮----
class ParallelExecutor
⋮----
"""
        Offers isolation between the tasks (dspy.settings) irrespective of whether num_threads == 1 or > 1.
        Handles also straggler timeouts.
        """
⋮----
def execute(self, function, data)
⋮----
wrapped = self._wrap_function(function)
⋮----
def _wrap_function(self, user_function)
⋮----
def safe_func(item)
⋮----
def _execute_parallel(self, function, data)
⋮----
results = [None] * len(data)
job_cancelled = "cancelled"
⋮----
# We resubmit at most once per item.
start_time_map = {}
start_time_lock = threading.Lock()
resubmitted = set()
⋮----
# This is the worker function each thread will run.
def worker(parent_overrides, submission_id, index, item)
⋮----
# Record actual start time
⋮----
# Apply parent's thread-local overrides
⋮----
original = thread_local_overrides.get()
new_overrides = {**original, **parent_overrides.copy()}
⋮----
# Usage tracker needs to be deep copied across threads so that each thread tracks its own usage
⋮----
token = thread_local_overrides.set(new_overrides)
⋮----
# Handle Ctrl-C in the main thread
⋮----
@contextlib.contextmanager
        def interrupt_manager()
⋮----
orig_handler = signal.getsignal(signal.SIGINT)
⋮----
def handler(sig, frame)
⋮----
executor = ThreadPoolExecutor(max_workers=self.num_threads)
⋮----
parent_overrides = thread_local_overrides.get().copy()
⋮----
futures_map = {}
futures_set = set()
submission_counter = 0
⋮----
f = executor.submit(worker, parent_overrides, submission_counter, idx, item)
⋮----
pbar = tqdm.tqdm(
⋮----
def all_done()
⋮----
# Check if this is an exception
⋮----
results[index] = None  # Keep None for failed examples
⋮----
# Update progress
⋮----
vals = [r[-1] for r in results if r is not None]
⋮----
# Check stragglers if few remain
⋮----
now = time.time()
⋮----
st = start_time_map.get(sid, None)
⋮----
nf = executor.submit(
⋮----
# Avoid waiting on leftover tasks that no longer matter
⋮----
def _update_progress(self, pbar, nresults, ntotal)
⋮----
pct = round(100 * nresults / ntotal, 1) if ntotal else 0
</file>

<file path="dspy/utils/saving.py">
logger = logging.getLogger(__name__)
⋮----
def get_dependency_versions()
⋮----
cloudpickle_version = ".".join(cloudpickle.__version__.split(".")[:2])
⋮----
def load(path: str, allow_pickle: bool = False) -> "Module"
⋮----
"""Load saved DSPy model.

    This method is used to load a saved DSPy model with `save_program=True`, i.e., the model is saved with cloudpickle.

    Args:
        path (str): Path to the saved model.
        allow_pickle (bool): Whether to allow loading the model with pickle. This is dangerous and should only be used if you are sure you trust the source of the model.

    Returns:
        The loaded model, a `dspy.Module` instance.
    """
⋮----
path = Path(path)
⋮----
metadata = orjson.loads(f.read())
⋮----
dependency_versions = get_dependency_versions()
saved_dependency_versions = metadata["dependency_versions"]
</file>

<file path="dspy/utils/syncify.py">
def run_async(coro)
⋮----
"""Run an async coroutine from a synchronous context."""
⋮----
loop = asyncio.get_running_loop()
⋮----
loop = None
⋮----
# If we're in a running event loop (e.g., Jupyter), use asyncio.create_task and run until done
⋮----
def syncify(program: "Module", in_place: bool = True) -> "Module"
⋮----
"""Convert an async DSPy module to a sync program.

    There are two modes of this function:

    - `in_place=True` (recommended): Modify the module in place. But this may not work if you already have a `forward`
        method which does different things from `aforward`.
    - `in_place=False`: Return a wrapper module. This changes the module's architecture, but it's more robust.

    Args:
        program: The async program to convert, must have an `aforward` method implemented.
        in_place: If True, modify the module in place. Otherwise, return a wrapper module.

    Returns:
        The sync program, which has a `forward` method that can be called from a synchronous context.
    """
⋮----
def forward(self, *args, **kwargs)
⋮----
# Create the `forward` method in place.
⋮----
class SyncWrapper(Module)
⋮----
def __init__(self, program: "Module")
</file>

<file path="dspy/utils/unbatchify.py">
class Unbatchify
⋮----
"""
        Initializes the Unbatchify.

        Args:
            batch_fn: The batch-processing function that accepts a list of inputs and returns a list of outputs.
            max_batch_size: The maximum number of items to include in a batch.
            max_wait_time: The maximum time (in seconds) to wait for batch to fill before processing.
        """
⋮----
self.worker_thread.daemon = True  # Ensures thread exits when main program exits
⋮----
def __call__(self, input_item: Any) -> Any
⋮----
"""
        Thread-safe function that accepts a single input and returns the corresponding output.

        Args:
            input_item: The single input item to process.

        Returns:
            The output corresponding to the input_item after processing through batch_fn.
        """
future = Future()
⋮----
result = future.result()
⋮----
def _worker(self)
⋮----
"""
        Worker thread that batches inputs and processes them using batch_fn.
        """
⋮----
batch = []
futures = []
start_time = time.time()
⋮----
outputs = self.batch_fn(batch)
⋮----
# Clean up remaining items when stopping
⋮----
def close(self)
⋮----
"""
        Stops the worker thread and cleans up resources.
        """
⋮----
def __enter__(self)
⋮----
"""
        Enables use as a context manager.
        """
⋮----
def __exit__(self, exc_type, exc_value, traceback)
⋮----
"""
        Ensures resources are cleaned up when exiting context.
        """
⋮----
def __del__(self)
⋮----
"""
        Ensures the worker thread is terminated when the object is garbage collected.
        """
</file>

<file path="dspy/utils/usage_tracker.py">
"""Usage tracking utilities for DSPy."""
⋮----
class UsageTracker
⋮----
"""Tracks LM usage data within a context."""
⋮----
def __init__(self)
⋮----
# Map of LM name to list of usage entries. For example:
# {
#     "openai/gpt-4o-mini": [
#         {"prompt_tokens": 100, "completion_tokens": 200},
#         {"prompt_tokens": 300, "completion_tokens": 400},
#     ],
# }
⋮----
def _flatten_usage_entry(self, usage_entry: dict[str, Any]) -> dict[str, Any]
⋮----
result = {}
⋮----
# Convert Pydantic models to dicts, like `PromptTokensDetailsWrapper` from litellm.
⋮----
result = dict(usage_entry2)
⋮----
current_v = result.get(k)
⋮----
def add_usage(self, lm: str, usage_entry: dict[str, Any]) -> None
⋮----
"""Add a usage entry to the tracker."""
⋮----
def get_total_tokens(self) -> dict[str, dict[str, Any]]
⋮----
"""Calculate total tokens from all tracked usage."""
total_usage_by_lm = {}
⋮----
total_usage = {}
⋮----
total_usage = self._merge_usage_entries(total_usage, usage_entry)
⋮----
@contextmanager
def track_usage() -> Generator[UsageTracker, None, None]
⋮----
"""Context manager for tracking LM usage."""
tracker = UsageTracker()
</file>

<file path="dspy/__init__.py">
from dspy.evaluate import Evaluate  # isort: skip
from dspy.clients import *  # isort: skip
from dspy.adapters import Adapter, ChatAdapter, JSONAdapter, XMLAdapter, TwoStepAdapter, Image, Audio, File, History, Type, Tool, ToolCalls, Code, Reasoning  # isort: skip
⋮----
# Singleton definitions and aliasing
configure = settings.configure
load_settings = settings.load
context = settings.context
⋮----
BootstrapRS = BootstrapFewShotWithRandomSearch
⋮----
cache = DSPY_CACHE
</file>

<file path="dspy/__metadata__.py">
#replace_package_name_marker
__name__="dspy"
#replace_package_version_marker
__version__="3.1.3"
__description__="DSPy"
__url__="https://github.com/stanfordnlp/dspy"
__author__="Omar Khattab"
__author_email__="okhattab@stanford.edu"
</file>

<file path="tests/adapters/test_adapter_utils.py">
# ruff: noqa: UP007
⋮----
class Profile(BaseModel)
⋮----
name: str
age: int
⋮----
def test_parse_value_str_annotation()
⋮----
# Test basic string conversion
⋮----
def test_parse_value_pydantic_types()
⋮----
# Test with pydantic BaseModel - JSON string input
json_str = '{"name": "John", "age": 30}'
result = parse_value(json_str, Profile)
⋮----
# Test with pydantic BaseModel - dict input
dict_input = {"name": "Jane", "age": 25}
result = parse_value(dict_input, Profile)
⋮----
# Test with invalid pydantic data
⋮----
parse_value('{"name": "John"}', Profile)  # missing required age field
⋮----
def test_parse_value_basic_types()
⋮----
# Test int
⋮----
# Test float
⋮----
# Test bool
⋮----
# Test list
⋮----
def test_parse_value_literal()
⋮----
# Test Literal type
⋮----
# Test Literal with quotes and prefixes
⋮----
# Test invalid literal
⋮----
def test_parse_value_union()
⋮----
# Test Union with None (Optional)
⋮----
# Test Union fallback to str
⋮----
def test_parse_value_json_repair()
⋮----
# Test cases where json_repair is needed
⋮----
# Test ast.literal_eval fallback
⋮----
# Test fallback to original value when parsing fails
malformed = "not json or literal"
</file>

<file path="tests/adapters/test_audio.py">
# Case 1: Standard format (no change)
⋮----
# Case 2: The 'x-' prefix
⋮----
# Case 3: The edge case
⋮----
# Case 4: Empty string and edge cases
⋮----
def test_normalize_audio_format(input_format, expected_format)
⋮----
"""
    Tests that the _normalize_audio_format helper correctly removes 'x-' prefixes.
    This single test covers the logic for from_url, from_file, and encode_audio.
    """
</file>

<file path="tests/adapters/test_baml_adapter.py">
# Test fixtures - Pydantic models for testing
class PatientAddress(pydantic.BaseModel)
⋮----
"""Patient Address model docstring"""
street: str
city: str
country: Literal["US", "CA"]
⋮----
class PatientDetails(pydantic.BaseModel)
⋮----
"""
    Patient Details model docstring
    Multiline docstring support test
    """
name: str = pydantic.Field(description="Full name of the patient")
age: int
address: PatientAddress | None = None
⋮----
class ComplexNestedModel(pydantic.BaseModel)
⋮----
"""Complex model docstring"""
id: int = pydantic.Field(description="Unique identifier")
details: PatientDetails
tags: list[str] = pydantic.Field(default_factory=list)
metadata: dict[str, str] = pydantic.Field(default_factory=dict)
⋮----
class ModelWithLists(pydantic.BaseModel)
⋮----
items: list[PatientAddress] = pydantic.Field(description="List of patient addresses")
scores: list[float]
⋮----
class ImageWrapper(pydantic.BaseModel)
⋮----
images: list[dspy.Image]
tag: list[str]
⋮----
class CircularModel(pydantic.BaseModel)
⋮----
name: str
field: "CircularModel"
⋮----
def test_baml_adapter_basic_schema_generation()
⋮----
"""Test that BAMLAdapter generates simplified schemas for Pydantic models."""
⋮----
class TestSignature(dspy.Signature)
⋮----
question: str = dspy.InputField()
patient: PatientDetails = dspy.OutputField()
⋮----
adapter = BAMLAdapter()
schema = adapter.format_field_structure(TestSignature)
⋮----
# Should contain simplified schema with comments
⋮----
def test_baml_adapter_handles_optional_fields()
⋮----
"""Test optional field rendering with 'or null' syntax."""
⋮----
input: str = dspy.InputField()
⋮----
# Optional address field should show 'or null'
⋮----
def test_baml_adapter_handles_primitive_types()
⋮----
"""Test rendering of basic primitive types."""
⋮----
class SimpleModel(pydantic.BaseModel)
⋮----
text: str
number: int
decimal: float
flag: bool
⋮----
output: SimpleModel = dspy.OutputField()
⋮----
def test_baml_adapter_handles_lists_with_bracket_notation()
⋮----
"""Test that lists of Pydantic models use proper bracket notation."""
⋮----
addresses: ModelWithLists = dspy.OutputField()
⋮----
# Should use bracket notation for lists and include comments
⋮----
def test_baml_adapter_handles_complex_nested_models()
⋮----
"""Test deeply nested Pydantic model schema generation."""
⋮----
complex: ComplexNestedModel = dspy.OutputField()
⋮----
expected_patient_details = "\n".join([
⋮----
# Should include nested structure with comments
⋮----
def test_baml_adapter_raise_error_on_circular_references()
⋮----
"""Test that circular references are handled gracefully."""
⋮----
circular: CircularModel = dspy.OutputField()
⋮----
def test_baml_adapter_formats_pydantic_inputs_as_clean_json()
⋮----
"""Test that Pydantic input instances are formatted as clean JSON."""
⋮----
patient: PatientDetails = dspy.InputField()
⋮----
answer: str = dspy.OutputField()
⋮----
patient = PatientDetails(
⋮----
messages = adapter.format(TestSignature, [], {"patient": patient, "question": "What is the diagnosis?"})
⋮----
# Should have clean, indented JSON for Pydantic input
user_message = messages[-1]["content"]
⋮----
def test_baml_adapter_handles_mixed_input_types()
⋮----
"""Test formatting of mixed Pydantic and primitive inputs."""
⋮----
priority: int = dspy.InputField()
notes: str = dspy.InputField()
result: str = dspy.OutputField()
⋮----
patient = PatientDetails(name="Jane Doe", age=30)
⋮----
messages = adapter.format(TestSignature, [], {"patient": patient, "priority": 1, "notes": "Urgent case"})
⋮----
# Pydantic should be JSON formatted
⋮----
# Primitives should be formatted normally
⋮----
def test_baml_adapter_handles_schema_generation_errors_gracefully()
⋮----
"""Test graceful handling of schema generation errors."""
⋮----
class ProblematicModel(pydantic.BaseModel)
⋮----
# This might cause issues in schema generation
field: object
⋮----
output: ProblematicModel = dspy.OutputField()
⋮----
# Should not raise an exception
⋮----
# If no exception, schema should at least contain some basic structure
⋮----
# If exception occurs, test passes as we're testing graceful handling
⋮----
def test_baml_adapter_raises_on_missing_fields()
⋮----
"""Test that missing required fields raise appropriate errors."""
⋮----
summary: str = dspy.OutputField()
⋮----
# Missing 'summary' field
completion = '{"patient": {"name": "John", "age": 30}}'
⋮----
assert e.value.adapter_name == "JSONAdapter"  # BAMLAdapter inherits from JSONAdapter
⋮----
def test_baml_adapter_handles_type_casting_errors()
⋮----
"""Test graceful handling of type casting errors."""
⋮----
# Invalid age type
completion = '{"patient": {"name": "John", "age": "not_a_number"}}'
⋮----
# Should raise ValidationError from Pydantic (which is the expected behavior)
⋮----
def test_baml_adapter_with_images()
⋮----
"""Test BAMLAdapter integration with dspy.Image objects."""
⋮----
image_data: ImageWrapper = dspy.InputField()
description: str = dspy.OutputField()
⋮----
image_wrapper = ImageWrapper(
⋮----
messages = adapter.format(TestSignature, [], {"image_data": image_wrapper})
⋮----
# Should contain image URLs in the message content
⋮----
image_contents = [
⋮----
def test_baml_adapter_with_tools()
⋮----
"""Test BAMLAdapter integration with dspy.Tool objects."""
⋮----
tools: list[dspy.Tool] = dspy.InputField()
⋮----
def get_patient_info(patient_id: int) -> str
⋮----
"""Get patient information by ID"""
⋮----
def schedule_appointment(patient_name: str, date: str) -> str
⋮----
"""Schedule an appointment for a patient"""
⋮----
tools = [dspy.Tool(get_patient_info), dspy.Tool(schedule_appointment)]
⋮----
messages = adapter.format(TestSignature, [], {"question": "Schedule an appointment for John", "tools": tools})
⋮----
def test_baml_adapter_with_code()
⋮----
"""Test BAMLAdapter integration with dspy.Code objects."""
⋮----
# Test with code as input field
class CodeAnalysisSignature(dspy.Signature)
⋮----
code: dspy.Code = dspy.InputField()
analysis: str = dspy.OutputField()
⋮----
messages = adapter.format(CodeAnalysisSignature, [], {"code": "def hello():\n    print('Hello, world!')"})
⋮----
# Test with code as output field
class CodeGenSignature(dspy.Signature)
⋮----
task: str = dspy.InputField()
code: dspy.Code = dspy.OutputField()
⋮----
result = adapter(
⋮----
def test_baml_adapter_with_conversation_history()
⋮----
"""Test BAMLAdapter integration with dspy.History objects."""
⋮----
history: dspy.History = dspy.InputField()
⋮----
history = dspy.History(
⋮----
messages = adapter.format(TestSignature, [], {"history": history, "question": "What medications should we avoid?"})
⋮----
# Should format history as separate messages
assert len(messages) == 6  # system + 2 history pairs + user
⋮----
# Comparison tests with JSONAdapter
def test_baml_vs_json_adapter_token_efficiency()
⋮----
"""Test that BAMLAdapter generates more token-efficient schemas."""
⋮----
baml_adapter = BAMLAdapter()
json_adapter = dspy.JSONAdapter()
⋮----
baml_schema = baml_adapter.format_field_structure(TestSignature)
json_schema = json_adapter.format_field_structure(TestSignature)
⋮----
# Simple character count as proxy for token efficiency
# BAMLAdapter should always produce shorter schemas
⋮----
def test_baml_vs_json_adapter_functional_compatibility()
⋮----
"""Test that both adapters parse identical outputs to the same results."""
⋮----
completion = """{"patient": {
⋮----
baml_result = baml_adapter.parse(TestSignature, completion)
json_result = json_adapter.parse(TestSignature, completion)
⋮----
# Results should be functionally equivalent
⋮----
@pytest.mark.asyncio
async def test_baml_adapter_async_functionality()
⋮----
"""Test BAMLAdapter async operations."""
⋮----
result = await adapter.acall(
⋮----
def test_baml_adapter_with_field_aliases()
⋮----
"""Test BAMLAdapter with Pydantic field aliases."""
⋮----
class ModelWithAliases(pydantic.BaseModel)
⋮----
full_name: str = pydantic.Field(alias="name")
patient_age: int = pydantic.Field(alias="age")
⋮----
data: ModelWithAliases = dspy.OutputField()
⋮----
# Schema should show aliases in the output structure
⋮----
assert "name:" in schema  # Should use alias, not field name
assert "age:" in schema  # Should use alias, not field name
⋮----
def test_baml_adapter_field_alias_without_description()
⋮----
"""Test BAMLAdapter with field alias present but description absent."""
⋮----
class ModelWithAliasNoDescription(pydantic.BaseModel)
⋮----
internal_field: str = pydantic.Field(alias="public_name")
regular_field: int
field_with_description: str = pydantic.Field(description="This field has a description", alias="desc_field")
⋮----
data: ModelWithAliasNoDescription = dspy.OutputField()
⋮----
# Should show alias as comment when description is absent
⋮----
# Should show description comment when present
⋮----
# Regular field (without alias) should appear in schema but without alias comment
⋮----
# Check that regular_field section doesn't have an alias comment
regular_field_section = schema.split("regular_field: int,")[0].split("\n")[-1]
⋮----
def test_baml_adapter_multiple_pydantic_input_fields()
⋮----
"""Test that multiple InputField() with Pydantic models are rendered correctly."""
⋮----
class UserProfile(pydantic.BaseModel)
⋮----
name: str = pydantic.Field(description="User's full name")
email: str
⋮----
class SystemConfig(pydantic.BaseModel)
⋮----
timeout: int = pydantic.Field(description="Timeout in seconds")
debug: bool
endpoints: list[str]
⋮----
input_1: UserProfile = dspy.InputField(desc="User profile information")
input_2: SystemConfig = dspy.InputField(desc="System configuration settings")
result: str = dspy.OutputField(desc="Resulting output after processing")
⋮----
# Test schema generation includes headers for ALL input fields
⋮----
assert "[[ ## input_1 ## ]]" in schema  # Should include first input field header
assert "[[ ## input_2 ## ]]" in schema  # Should include second input field header
assert "[[ ## result ## ]]" in schema  # Should include output field header
assert "[[ ## completed ## ]]" in schema  # Should include completed section
⋮----
# Test field descriptions are in the correct method
field_desc = adapter.format_field_description(TestSignature)
⋮----
# Test message formatting with actual Pydantic instances
user_profile = UserProfile(name="John Doe", email="john@example.com", age=30)
system_config = SystemConfig(timeout=300, debug=True, endpoints=["api1", "api2"])
⋮----
messages = adapter.format(TestSignature, [], {"input_1": user_profile, "input_2": system_config})
⋮----
# Verify both inputs are rendered with the correct bracket notation
⋮----
# Verify JSON content for both inputs
⋮----
# Endpoints array is formatted with indentation, so check for individual elements
</file>

<file path="tests/adapters/test_base_type.py">
def test_basic_extract_custom_type_from_annotation()
⋮----
class Event(dspy.Type)
⋮----
event_name: str
start_date_time: str
end_date_time: str | None
location: str | None
⋮----
class ExtractEvent(dspy.Signature)
⋮----
"""Extract all events from the email content."""
⋮----
email: str = dspy.InputField()
event: Event = dspy.OutputField()
⋮----
class ExtractEvents(dspy.Signature)
⋮----
events: list[Event] = dspy.OutputField()
⋮----
def test_extract_custom_type_from_annotation_with_nested_type()
⋮----
class EventIdentifier(dspy.Type)
⋮----
model_config = pydantic.ConfigDict(frozen=True)  # Make it hashable
event_id: str
⋮----
events: list[dict[EventIdentifier, Event]] = dspy.OutputField()
</file>

<file path="tests/adapters/test_chat_adapter.py">
# Scenario 1: double quotes escaped within strings
⋮----
# Scenario 2: Single quotes inside strings
⋮----
# Scenario 3: Strings containing both single and double quotes
⋮----
# Scenario 4: No quotes at all (check the default)
⋮----
# Scenario 5: Mixed types
⋮----
"""
    This test verifies that when we declare Literal fields with various mixes
    of single/double quotes, the generated content string includes those
    Literals exactly as we want them to appear (like IPython does).
    """
⋮----
class TestSignature(dspy.Signature)
⋮----
input_text: input_literal = dspy.InputField()
output_text: output_literal = dspy.OutputField()
⋮----
program = dspy.Predict(TestSignature)
⋮----
content = call_kwargs["messages"][0]["content"]
⋮----
def test_chat_adapter_sync_call()
⋮----
signature = dspy.make_signature("question->answer")
adapter = dspy.ChatAdapter()
lm = dspy.utils.DummyLM([{"answer": "Paris"}])
result = adapter(lm, {}, signature, [], {"question": "What is the capital of France?"})
⋮----
@pytest.mark.asyncio
async def test_chat_adapter_async_call()
⋮----
result = await adapter.acall(lm, {}, signature, [], {"question": "What is the capital of France?"})
⋮----
def test_chat_adapter_with_pydantic_models()
⋮----
"""
    This test verifies that ChatAdapter can handle different input and output field types, both basic and nested.
    """
⋮----
class DogClass(pydantic.BaseModel)
⋮----
dog_breeds: list[str] = pydantic.Field(description="List of the breeds of dogs")
num_dogs: int = pydantic.Field(description="Number of dogs the owner has", ge=0, le=10)
⋮----
class PetOwner(pydantic.BaseModel)
⋮----
name: str = pydantic.Field(description="Name of the owner")
num_pets: int = pydantic.Field(description="Amount of pets the owner has", ge=0, le=100)
dogs: DogClass = pydantic.Field(description="Nested Pydantic class with dog specific information ")
⋮----
class Answer(pydantic.BaseModel)
⋮----
result: str
analysis: str
⋮----
owner: PetOwner = dspy.InputField()
question: str = dspy.InputField()
output: Answer = dspy.OutputField()
⋮----
system_content = call_kwargs["messages"][0]["content"]
user_content = call_kwargs["messages"][1]["content"]
⋮----
def test_chat_adapter_signature_information()
⋮----
"""
    This test ensures that the signature information sent to the LM follows an expected format.
    """
⋮----
input1: str = dspy.InputField(desc="String Input")
input2: int = dspy.InputField(desc="Integer Input")
output: str = dspy.OutputField(desc="String Output")
⋮----
def test_chat_adapter_exception_raised_on_failure()
⋮----
"""
    This test ensures that on an error, ChatAdapter raises an explicit exception.
    """
⋮----
invalid_completion = "{'output':'mismatched value'}"
⋮----
def test_chat_adapter_formats_image()
⋮----
# Test basic image formatting
image = dspy.Image(url="https://example.com/image.jpg")
⋮----
class MySignature(dspy.Signature)
⋮----
image: dspy.Image = dspy.InputField()
text: str = dspy.OutputField()
⋮----
messages = adapter.format(MySignature, [], {"image": image})
⋮----
user_message_content = messages[1]["content"]
⋮----
# The message should have 3 chunks of types: text, image_url, text
⋮----
# Assert that the image is formatted correctly
expected_image_content = {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
⋮----
def test_chat_adapter_formats_image_with_few_shot_examples()
⋮----
demos = [
messages = adapter.format(MySignature, demos, {"image": dspy.Image(url="https://example.com/image3.jpg")})
⋮----
# 1 system message, 2 few shot examples (1 user and assistant message for each example), 1 user message
⋮----
def test_chat_adapter_formats_image_with_nested_images()
⋮----
class ImageWrapper(pydantic.BaseModel)
⋮----
images: list[dspy.Image]
tag: list[str]
⋮----
image: ImageWrapper = dspy.InputField()
⋮----
image1 = dspy.Image(url="https://example.com/image1.jpg")
image2 = dspy.Image(url="https://example.com/image2.jpg")
image3 = dspy.Image(url="https://example.com/image3.jpg")
⋮----
image_wrapper = ImageWrapper(images=[image1, image2, image3], tag=["test", "example"])
⋮----
messages = adapter.format(MySignature, [], {"image": image_wrapper})
⋮----
expected_image1_content = {"type": "image_url", "image_url": {"url": "https://example.com/image1.jpg"}}
expected_image2_content = {"type": "image_url", "image_url": {"url": "https://example.com/image2.jpg"}}
expected_image3_content = {"type": "image_url", "image_url": {"url": "https://example.com/image3.jpg"}}
⋮----
def test_chat_adapter_formats_image_with_few_shot_examples_with_nested_images()
⋮----
image_wrapper_2 = ImageWrapper(images=[dspy.Image(url="https://example.com/image4.jpg")], tag=["test", "example"])
⋮----
messages = adapter.format(MySignature, demos, {"image": image_wrapper_2})
⋮----
# Image information in the few-shot example's user message
⋮----
# The query image is formatted in the last user message
⋮----
def test_chat_adapter_with_tool()
⋮----
"""Answer question with the help of the tools"""
⋮----
tools: list[dspy.Tool] = dspy.InputField()
answer: str = dspy.OutputField()
tool_calls: dspy.ToolCalls = dspy.OutputField()
⋮----
def get_weather(city: str) -> str
⋮----
"""Get the weather for a city"""
⋮----
def get_population(country: str, year: int) -> str
⋮----
"""Get the population for a country"""
⋮----
tools = [dspy.Tool(get_weather), dspy.Tool(get_population)]
⋮----
messages = adapter.format(MySignature, [], {"question": "What is the weather in Tokyo?", "tools": tools})
⋮----
# The output field type description should be included in the system message even if the output field is nested
⋮----
# The user message should include the question and the tools
⋮----
# Tool arguments format should be included in the user message
⋮----
def test_chat_adapter_with_code()
⋮----
# Test with code as input field
class CodeAnalysis(dspy.Signature)
⋮----
"""Analyze the time complexity of the code"""
⋮----
code: dspy.Code = dspy.InputField()
result: str = dspy.OutputField()
⋮----
messages = adapter.format(CodeAnalysis, [], {"code": "print('Hello, world!')"})
⋮----
# Test with code as output field
class CodeGeneration(dspy.Signature)
⋮----
"""Generate code to answer the question"""
⋮----
code: dspy.Code = dspy.OutputField()
⋮----
result = adapter(
⋮----
def test_chat_adapter_formats_conversation_history()
⋮----
history: dspy.History = dspy.InputField()
⋮----
history = dspy.History(
⋮----
messages = adapter.format(MySignature, [], {"question": "What is the capital of France?", "history": history})
⋮----
def test_chat_adapter_fallback_to_json_adapter_on_exception()
⋮----
# Mock returning a response compatible with JSONAdapter but not ChatAdapter
⋮----
lm = dspy.LM("openai/gpt-4o-mini", cache=False)
⋮----
# The parse should succeed
⋮----
def test_chat_adapter_respects_use_json_adapter_fallback_flag()
⋮----
adapter = dspy.ChatAdapter(use_json_adapter_fallback=False)
⋮----
@pytest.mark.asyncio
async def test_chat_adapter_fallback_to_json_adapter_on_exception_async()
⋮----
def test_chat_adapter_toolcalls_native_function_calling()
⋮----
tools = [dspy.Tool(get_weather)]
⋮----
adapter = dspy.JSONAdapter(use_native_function_calling=True)
⋮----
# Case 1: Tool calls are present in the response, while content is None.
⋮----
# `answer` is not present, so we set it to None
⋮----
# Case 2: Tool calls are not present in the response, while content is present.
⋮----
def test_chat_adapter_toolcalls_vague_match()
⋮----
# Case 1: tool_calls field is a list of dicts
⋮----
# Case 2: tool_calls field is a single dict with "name" and "args" keys
⋮----
def test_chat_adapter_native_reasoning()
⋮----
reasoning: dspy.Reasoning = dspy.OutputField()
⋮----
modified_signature = adapter._call_preprocess(
⋮----
def test_chat_adapter_parses_float_with_underscores()
⋮----
"""
    This test verifies that ChatAdapter can parse float numbers with underscores.
    After json-repair version 0.54.1, floats like "123_456.789" are treated as normal float numbers.
    """
⋮----
class Score(pydantic.BaseModel)
⋮----
score: float
⋮----
score: Score = dspy.OutputField()
⋮----
# Simulate a response with a float number containing underscores
⋮----
result = adapter(lm, {}, MySignature, [], {"question": "What is the score?"})
⋮----
# The underscore-separated float should be parsed as a normal float
⋮----
def test_format_system_message()
⋮----
"""Answer the question with multiple answers and scores"""
⋮----
answers: list[str] = dspy.OutputField()
scores: list[float] = dspy.OutputField()
⋮----
system_message = adapter.format_system_message(MySignature)
expected_system_message = """Your input fields are:
</file>

<file path="tests/adapters/test_citation.py">
def test_citation_validate_input()
⋮----
citation = Citations.Citation(
⋮----
def test_citations_in_nested_type()
⋮----
class Wrapper(pydantic.BaseModel)
⋮----
citations: Citations
⋮----
citations = Citations(citations=[citation])
wrapper = Wrapper(citations=citations)
⋮----
def test_citation_with_all_fields()
⋮----
def test_citation_format()
⋮----
formatted = citation.format()
⋮----
def test_citations_format()
⋮----
citations = Citations(citations=[
⋮----
formatted = citations.format()
⋮----
def test_citations_from_dict_list()
⋮----
citations_data = [
⋮----
citations = Citations.from_dict_list(citations_data)
⋮----
def test_citations_postprocessing()
⋮----
class CitationSignature(Signature)
⋮----
"""Test signature with citations."""
question: str = dspy.InputField()
answer: str = dspy.OutputField()
citations: Citations = dspy.OutputField()
⋮----
adapter = ChatAdapter(native_response_types=[Citations])
⋮----
outputs = [{
⋮----
result = adapter._call_postprocess(
⋮----
def test_citation_extraction_from_lm_response()
⋮----
mock_choice = MagicMock(message=MagicMock(provider_specific_fields={"citations": [[
⋮----
lm = dspy.LM(model="test")
citations = lm._extract_citations_from_response(mock_choice)
</file>

<file path="tests/adapters/test_code.py">
def test_code_validate_input()
⋮----
# Create a `dspy.Code` instance with valid code.
code = dspy.Code["python"](code="print('Hello, world!')")
⋮----
# Try to create a `dspy.Code` instance with invalid type.
⋮----
def foo(x)
⋮----
code_source = inspect.getsource(foo)
code = dspy.Code["python"](code=code_source)
⋮----
def test_code_in_nested_type()
⋮----
class Wrapper(pydantic.BaseModel)
⋮----
code: dspy.Code
⋮----
code = dspy.Code(code="print('Hello, world!')")
wrapper = Wrapper(code=code)
⋮----
def test_code_with_language()
⋮----
java_code = dspy.Code["java"](code="System.out.println('Hello, world!');")
⋮----
cpp_code = dspy.Code["cpp"](code="std::cout << 'Hello, world!' << std::endl;")
⋮----
def test_code_parses_from_dirty_code()
⋮----
dirty_code = "```python\nprint('Hello, world!')```"
code = dspy.Code(code=dirty_code)
⋮----
dirty_code_with_reasoning = """
code = dspy.Code(code=dirty_code_with_reasoning)
</file>

<file path="tests/adapters/test_document.py">
def test_document_validate_input()
⋮----
# Create a `Document` instance with valid data.
doc = Document(data="The Earth orbits the Sun.")
⋮----
# Try to create a `Document` instance with invalid type.
⋮----
def test_document_in_nested_type()
⋮----
class Wrapper(pydantic.BaseModel)
⋮----
document: Document
⋮----
doc = Document(data="Hello, world!")
wrapper = Wrapper(document=doc)
⋮----
def test_document_with_all_fields()
⋮----
doc = Document(
⋮----
def test_document_format()
⋮----
formatted = doc.format()
⋮----
doc_block = formatted[0]
</file>

<file path="tests/adapters/test_json_adapter.py">
def test_json_adapter_passes_structured_output_when_supported_by_model()
⋮----
class OutputField3(pydantic.BaseModel)
⋮----
subfield1: int = pydantic.Field(description="Int subfield 1", ge=0, le=10)
subfield2: float = pydantic.Field(description="Float subfield 2")
⋮----
class TestSignature(dspy.Signature)
⋮----
input1: str = dspy.InputField()
output1: str = dspy.OutputField()  # Description intentionally left blank
output2: bool = dspy.OutputField(desc="Boolean output field")
output3: OutputField3 = dspy.OutputField(desc="Nested output field")
output4_unannotated = dspy.OutputField(desc="Unannotated output field")
⋮----
program = dspy.Predict(TestSignature)
⋮----
# Configure DSPy to use an OpenAI LM that supports structured outputs
⋮----
def clean_schema_extra(field_name, field_info)
⋮----
attrs = dict(field_info.__repr_args__())
⋮----
response_format = call_kwargs.get("response_format")
⋮----
def test_json_adapter_not_using_structured_outputs_when_not_supported_by_model()
⋮----
output1: str = dspy.OutputField()
output2: bool = dspy.OutputField()
⋮----
# Configure DSPy to use a model from a fake provider that doesn't support structured outputs
⋮----
def test_json_adapter_falls_back_when_structured_outputs_fails()
⋮----
output1: str = dspy.OutputField(desc="String output field")
⋮----
def test_json_adapter_with_structured_outputs_does_not_mutate_original_signature()
⋮----
subfield1: int = pydantic.Field(description="Int subfield 1")
⋮----
def test_json_adapter_sync_call()
⋮----
signature = dspy.make_signature("question->answer")
adapter = dspy.JSONAdapter()
lm = dspy.utils.DummyLM([{"answer": "Paris"}], adapter=adapter)
⋮----
result = adapter(lm, {}, signature, [], {"question": "What is the capital of France?"})
⋮----
@pytest.mark.asyncio
async def test_json_adapter_async_call()
⋮----
result = await adapter.acall(lm, {}, signature, [], {"question": "What is the capital of France?"})
⋮----
def test_json_adapter_on_pydantic_model()
⋮----
class User(pydantic.BaseModel)
⋮----
id: int
name: str
email: str
⋮----
class Answer(pydantic.BaseModel)
⋮----
analysis: str
result: str
⋮----
user: User = dspy.InputField(desc="The user who asks the question")
question: str = dspy.InputField(desc="Question the user asks")
answer: Answer = dspy.OutputField(desc="Answer to this question")
⋮----
result = program(
⋮----
# Check that litellm.completion was called exactly once
⋮----
# Assert that there are exactly 2 messages (system + user)
⋮----
content = call_kwargs["messages"][0]["content"]
⋮----
# Assert that system prompt includes correct input field descriptions
expected_input_fields = (
⋮----
# Assert that system prompt includes correct output field description
expected_output_fields = "1. `answer` (Answer): Answer to this question\n"
⋮----
# Assert that system prompt includes input formatting structure
expected_input_structure = "[[ ## user ## ]]\n{user}\n\n[[ ## question ## ]]\n{question}\n\n"
⋮----
# Assert that system prompt includes output formatting structure
expected_output_structure = (
⋮----
user_message_content = call_kwargs["messages"][1]["content"]
⋮----
# Assert that the user input data is formatted correctly
expected_input_data = (
⋮----
# Assert that the adapter output has expected fields and values
⋮----
def test_json_adapter_parse_raise_error_on_mismatch_fields()
⋮----
lm = dspy.LM(model="openai/gpt-4o-mini")
⋮----
def test_json_adapter_formats_image()
⋮----
# Test basic image formatting
image = dspy.Image(url="https://example.com/image.jpg")
⋮----
class MySignature(dspy.Signature)
⋮----
image: dspy.Image = dspy.InputField()
text: str = dspy.OutputField()
⋮----
messages = adapter.format(MySignature, [], {"image": image})
⋮----
user_message_content = messages[1]["content"]
⋮----
# The message should have 3 chunks of types: text, image_url, text
⋮----
# Assert that the image is formatted correctly
expected_image_content = {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
⋮----
def test_json_adapter_formats_image_with_few_shot_examples()
⋮----
demos = [
messages = adapter.format(MySignature, demos, {"image": dspy.Image(url="https://example.com/image3.jpg")})
⋮----
# 1 system message, 2 few shot examples (1 user and assistant message for each example), 1 user message
⋮----
def test_json_adapter_formats_image_with_nested_images()
⋮----
class ImageWrapper(pydantic.BaseModel)
⋮----
images: list[dspy.Image]
tag: list[str]
⋮----
image: ImageWrapper = dspy.InputField()
⋮----
image1 = dspy.Image(url="https://example.com/image1.jpg")
image2 = dspy.Image(url="https://example.com/image2.jpg")
image3 = dspy.Image(url="https://example.com/image3.jpg")
⋮----
image_wrapper = ImageWrapper(images=[image1, image2, image3], tag=["test", "example"])
⋮----
messages = adapter.format(MySignature, [], {"image": image_wrapper})
⋮----
expected_image1_content = {"type": "image_url", "image_url": {"url": "https://example.com/image1.jpg"}}
expected_image2_content = {"type": "image_url", "image_url": {"url": "https://example.com/image2.jpg"}}
expected_image3_content = {"type": "image_url", "image_url": {"url": "https://example.com/image3.jpg"}}
⋮----
def test_json_adapter_formats_with_nested_documents()
⋮----
class DocumentWrapper(pydantic.BaseModel)
⋮----
documents: list[dspy.experimental.Document]
⋮----
document: DocumentWrapper = dspy.InputField()
⋮----
doc1 = dspy.experimental.Document(data="Hello, world!")
doc2 = dspy.experimental.Document(data="Hello, world 2!")
⋮----
document_wrapper = DocumentWrapper(documents=[doc1, doc2])
⋮----
messages = adapter.format(MySignature, [], {"document": document_wrapper})
⋮----
expected_doc1_content = {
expected_doc2_content = {
⋮----
def test_json_adapter_formats_image_with_few_shot_examples_with_nested_images()
⋮----
image_wrapper_2 = ImageWrapper(images=[dspy.Image(url="https://example.com/image4.jpg")], tag=["test", "example"])
⋮----
messages = adapter.format(MySignature, demos, {"image": image_wrapper_2})
⋮----
# Image information in the few-shot example's user message
⋮----
# The query image is formatted in the last user message
⋮----
def test_json_adapter_with_tool()
⋮----
"""Answer question with the help of the tools"""
⋮----
question: str = dspy.InputField()
tools: list[dspy.Tool] = dspy.InputField()
answer: str = dspy.OutputField()
tool_calls: dspy.ToolCalls = dspy.OutputField()
⋮----
def get_weather(city: str) -> str
⋮----
"""Get the weather for a city"""
⋮----
def get_population(country: str, year: int) -> str
⋮----
"""Get the population for a country"""
⋮----
tools = [dspy.Tool(get_weather), dspy.Tool(get_population)]
⋮----
messages = adapter.format(MySignature, [], {"question": "What is the weather in Tokyo?", "tools": tools})
⋮----
# The output field type description should be included in the system message even if the output field is nested
⋮----
# The user message should include the question and the tools
⋮----
# Tool arguments format should be included in the user message
⋮----
# Assert tool calls are included in the `tools` arg
⋮----
def test_json_adapter_with_code()
⋮----
# Test with code as input field
class CodeAnalysis(dspy.Signature)
⋮----
"""Analyze the time complexity of the code"""
⋮----
code: dspy.Code = dspy.InputField()
result: str = dspy.OutputField()
⋮----
messages = adapter.format(CodeAnalysis, [], {"code": "print('Hello, world!')"})
⋮----
# Test with code as output field
class CodeGeneration(dspy.Signature)
⋮----
"""Generate code to answer the question"""
⋮----
code: dspy.Code = dspy.OutputField()
⋮----
result = adapter(
⋮----
def test_json_adapter_formats_conversation_history()
⋮----
history: dspy.History = dspy.InputField()
⋮----
history = dspy.History(
⋮----
messages = adapter.format(MySignature, [], {"question": "What is the capital of France?", "history": history})
⋮----
@pytest.mark.asyncio
async def test_json_adapter_on_pydantic_model_async()
⋮----
result = await program.acall(
⋮----
# Check that litellm.acompletion was called exactly once
⋮----
def test_json_adapter_fallback_to_json_mode_on_structured_output_failure()
⋮----
answer: str = dspy.OutputField(desc="String output field")
⋮----
# First call raises error to simulate structured output failure, second call returns a valid response
⋮----
result = program(question="Dummy question!")
# The parse should succeed on the second call
⋮----
# The first call should have tried structured output
⋮----
# The second call should have used JSON mode
⋮----
def test_json_adapter_json_mode_no_structured_outputs()
⋮----
# Call a model that allows json but not structured outputs
⋮----
@pytest.mark.asyncio
async def test_json_adapter_json_mode_no_structured_outputs_async()
⋮----
result = await program.acall(question="Dummy question!")
⋮----
@pytest.mark.asyncio
async def test_json_adapter_fallback_to_json_mode_on_structured_output_failure_async()
⋮----
def test_error_message_on_json_adapter_failure()
⋮----
@pytest.mark.asyncio
async def test_error_message_on_json_adapter_failure_async()
⋮----
def test_json_adapter_toolcalls_native_function_calling()
⋮----
tools = [dspy.Tool(get_weather)]
⋮----
adapter = dspy.JSONAdapter(use_native_function_calling=True)
⋮----
# Case 1: Tool calls are present in the response, while content is None.
⋮----
# `answer` is not present, so we set it to None
⋮----
# Case 2: Tool calls are not present in the response, while content is present.
⋮----
def test_json_adapter_toolcalls_no_native_function_calling()
⋮----
# Patch _get_structured_outputs_response_format to track calls
⋮----
# Patch litellm.completion to return a dummy response
⋮----
adapter = dspy.JSONAdapter(use_native_function_calling=False)
lm = dspy.LM(model="openai/gpt-4o-mini", cache=False)
⋮----
# _get_structured_outputs_response_format is not called because without using native function calling,
# JSONAdapter falls back to json mode for stable quality.
⋮----
def test_json_adapter_native_reasoning()
⋮----
reasoning: dspy.Reasoning = dspy.OutputField()
⋮----
modified_signature = adapter._call_preprocess(
⋮----
def test_json_adapter_with_responses_api()
⋮----
api_response = ResponsesAPIResponse(
⋮----
lm = dspy.LM(model="openai/gpt-4o", model_type="responses", cache=False)
⋮----
result = program(question="What is the capital of the USA?")
⋮----
# Verify that response_format was converted to text.format
call_kwargs = mock_responses.call_args.kwargs
⋮----
def test_format_system_message()
⋮----
"""Answer the question with multiple answers and scores"""
⋮----
answers: list[str] = dspy.OutputField()
scores: list[float] = dspy.OutputField()
⋮----
system_message = adapter.format_system_message(MySignature)
expected_system_message = """Your input fields are:
</file>

<file path="tests/adapters/test_reasoning.py">
def test_reasoning_basic_operations()
⋮----
reasoning = dspy.Reasoning(content="Hello World")
⋮----
# Test str conversion
⋮----
# Test equality
⋮----
# Test len
⋮----
# Test indexing
⋮----
# Test in operator
⋮----
# Test iteration
chars = [c for c in reasoning]
⋮----
def test_reasoning_concatenation()
⋮----
reasoning = dspy.Reasoning(content="Hello")
⋮----
# Test + operator
result1 = reasoning + " World"
⋮----
# Test reverse + operator
result2 = "Prefix: " + reasoning
⋮----
# Test Reasoning + Reasoning
reasoning2 = dspy.Reasoning(content=" World")
result3 = reasoning + reasoning2
⋮----
def test_reasoning_string_methods()
⋮----
reasoning = dspy.Reasoning(content="  Hello World  ")
⋮----
# Test strip
⋮----
# Test lower/upper
⋮----
# Test split
⋮----
# Test replace
⋮----
# Test startswith/endswith
⋮----
# Test find
⋮----
# Test count
⋮----
# Test join
⋮----
def test_reasoning_with_chain_of_thought()
⋮----
lm = DummyLM([{"reasoning": "Let me think step by step", "answer": "42"}])
⋮----
cot = dspy.ChainOfThought("question -> answer")
result = cot(question="What is the answer?")
⋮----
# Test that we can use string methods on result.reasoning
⋮----
def test_reasoning_error_message()
</file>

<file path="tests/adapters/test_tool.py">
# Test fixtures
def dummy_function(x: int, y: str = "hello") -> str
⋮----
"""A dummy function for testing.

    Args:
        x: An integer parameter
        y: A string parameter
    """
⋮----
class DummyModel(BaseModel)
⋮----
field1: str = "hello"
field2: int
⋮----
def dummy_with_pydantic(model: DummyModel) -> str
⋮----
"""A dummy function that accepts a Pydantic model."""
⋮----
class Address(BaseModel)
⋮----
street: str
city: str
zip_code: str
is_primary: bool = False
⋮----
class ContactInfo(BaseModel)
⋮----
email: str
phone: str | None = None
addresses: list[Address]
⋮----
class UserProfile(BaseModel)
⋮----
user_id: int
name: str
age: int | None = None
contact: ContactInfo
tags: list[str] = []
⋮----
class Note(BaseModel)
⋮----
content: str
author: str
⋮----
def complex_dummy_function(profile: UserProfile, priority: int, notes: list[Note] | None = None) -> dict[str, Any]
⋮----
"""Process user profile with complex nested structure.

    Args:
        profile: User profile containing nested contact and address information
        priority: Priority level of the processing
        notes: Optional processing notes
    """
primary_address = next(
⋮----
async def async_dummy_function(x: int, y: str = "hello") -> str
⋮----
"""An async dummy function for testing.

    Args:
        x: An integer parameter
        y: A string parameter
    """
await asyncio.sleep(0.1)  # Simulate some async work
⋮----
async def async_dummy_with_pydantic(model: DummyModel) -> str
⋮----
"""An async dummy function that accepts a Pydantic model."""
⋮----
"""Process user profile with complex nested structure asynchronously.

    Args:
        profile: User profile containing nested contact and address information
        priority: Priority level of the processing
        notes: Optional processing notes
    """
# Simulate some async processing work
⋮----
# Simulate more async work after finding primary address
⋮----
def test_basic_initialization()
⋮----
tool = Tool(name="test_tool", desc="A test tool", args={"param1": {"type": "string"}}, func=lambda x: x)
⋮----
def test_tool_from_function()
⋮----
tool = Tool(dummy_function)
⋮----
def test_tool_from_class()
⋮----
class Foo
⋮----
def __init__(self, user_id: str)
⋮----
def __call__(self, a: int, b: int) -> int
⋮----
"""Add two numbers."""
⋮----
tool = Tool(Foo("123"))
⋮----
def test_tool_from_function_with_pydantic()
⋮----
tool = Tool(dummy_with_pydantic)
⋮----
def test_tool_from_function_with_pydantic_nesting()
⋮----
tool = Tool(complex_dummy_function)
⋮----
# Reference should be resolved for nested pydantic models
⋮----
def test_tool_callable()
⋮----
result = tool(x=42, y="hello")
⋮----
def test_tool_with_pydantic_callable()
⋮----
model = DummyModel(field1="test", field2=123)
result = tool(model=model)
⋮----
def test_invalid_function_call()
⋮----
def test_parameter_desc()
⋮----
tool = Tool(dummy_function, arg_desc={"x": "The x parameter"})
⋮----
def test_tool_with_default_args_without_type_hints()
⋮----
def foo(x=100)
⋮----
tool = Tool(foo)
⋮----
def test_tool_call_parses_args()
⋮----
args = {
⋮----
result = tool(**args)
⋮----
def test_tool_call_parses_nested_list_of_pydantic_model()
⋮----
def dummy_function(x: list[list[DummyModel]])
⋮----
def test_tool_call_kwarg()
⋮----
def fn(x: int, **kwargs)
⋮----
tool = Tool(fn)
⋮----
def test_tool_str()
⋮----
def add(x: int, y: int = 0) -> int
⋮----
"""Add two integers."""
⋮----
tool = Tool(add)
⋮----
@pytest.mark.asyncio
async def test_async_tool_from_function()
⋮----
tool = Tool(async_dummy_function)
⋮----
# Test async call
result = await tool.acall(x=42, y="hello")
⋮----
@pytest.mark.asyncio
async def test_async_tool_with_pydantic()
⋮----
tool = Tool(async_dummy_with_pydantic)
⋮----
# Test async call with pydantic model
⋮----
result = await tool.acall(model=model)
⋮----
# Test async call with dict
result = await tool.acall(model={"field1": "test", "field2": 123})
⋮----
@pytest.mark.asyncio
async def test_async_tool_with_complex_pydantic()
⋮----
tool = Tool(async_complex_dummy_function)
⋮----
profile = UserProfile(
⋮----
result = await tool.acall(profile=profile, priority=1, notes=[Note(content="Test note", author="Test author")])
⋮----
@pytest.mark.asyncio
async def test_async_tool_invalid_call()
⋮----
@pytest.mark.asyncio
async def test_async_tool_with_kwargs()
⋮----
async def fn(x: int, **kwargs)
⋮----
result = await tool.acall(x=1, y=2, z=3)
⋮----
@pytest.mark.asyncio
async def test_async_concurrent_calls()
⋮----
"""Test that multiple async tools can run concurrently."""
⋮----
# Create multiple concurrent calls
tasks = [tool.acall(x=i, y=f"hello{i}") for i in range(5)]
⋮----
# Run them concurrently and measure time
start_time = asyncio.get_event_loop().time()
results = await asyncio.gather(*tasks)
end_time = asyncio.get_event_loop().time()
⋮----
# Verify results, `asyncio.gather` returns results in the order of the tasks
⋮----
# Check that it ran concurrently (should take ~0.1s, not ~0.5s)
# We use 0.3s as threshold to account for some overhead
⋮----
@pytest.mark.filterwarnings("ignore::RuntimeWarning")
def test_async_tool_call_in_sync_mode()
⋮----
result = tool(x=1, y="hello")
⋮----
TOOL_CALL_TEST_CASES = [
⋮----
@pytest.mark.parametrize("tool_calls_data,expected", TOOL_CALL_TEST_CASES)
def test_tool_calls_format_basic(tool_calls_data, expected)
⋮----
"""Test ToolCalls.format with various basic scenarios."""
tool_calls_list = [ToolCalls.ToolCall(**data) for data in tool_calls_data]
tool_calls = ToolCalls(tool_calls=tool_calls_list)
result = tool_calls.format()
⋮----
def test_tool_calls_format_from_dict_list()
⋮----
"""Test format works with ToolCalls created from from_dict_list."""
tool_calls_dicts = [
⋮----
tool_calls = ToolCalls.from_dict_list(tool_calls_dicts)
⋮----
def test_toolcalls_vague_match()
⋮----
"""
    Test that ToolCalls can parse the data with slightly off format:

    - a single dict with "name" and "args"
    - a list of dicts with "name" and "args"
    - invalid input (should raise ValueError)
    """
# Single dict with "name" and "args" should parse as one ToolCall
data_single = {"name": "search", "args": {"query": "hello"}}
tc = ToolCalls.model_validate(data_single)
⋮----
# List of dicts with "name" and "args" should parse as multiple ToolCalls
data_list = [
tc = ToolCalls.model_validate(data_list)
⋮----
# Dict with "tool_calls" key containing a list of dicts
data_tool_calls = {
tc = ToolCalls.model_validate(data_tool_calls)
⋮----
# Invalid input should raise ValueError
⋮----
def test_tool_convert_input_schema_to_tool_args_no_input_params()
⋮----
def test_tool_convert_input_schema_to_tool_args_lang_chain()
⋮----
# Example from langchain docs:
# https://web.archive.org/web/20250723101359/https://api.python.langchain.com/en/latest/tools/langchain_core.tools.tool.html
⋮----
def test_tool_call_execute()
⋮----
def get_weather(city: str) -> str
⋮----
def add_numbers(a: int, b: int) -> int
⋮----
tools = [
⋮----
tool_call = dspy.ToolCalls.ToolCall(name="get_weather", args={"city": "Berlin"})
result = tool_call.execute(functions=tools)
⋮----
# Test individual tool call with function dict
tool_call2 = dspy.ToolCalls.ToolCall(name="add_numbers", args={"a": 7, "b": 13})
result2 = tool_call2.execute(functions={"add_numbers": add_numbers})
⋮----
# Test individual tool call with no arguments
def get_pi()
⋮----
tool_call3 = dspy.ToolCalls.ToolCall(name="get_pi", args={})
result3 = tool_call3.execute(functions={"get_pi": get_pi})
⋮----
# Test error case
tool_call4 = dspy.ToolCalls.ToolCall(name="nonexistent", args={})
⋮----
def test_tool_call_execute_with_local_functions()
⋮----
def main()
⋮----
def local_add(a: int, b: int) -> int
⋮----
def local_multiply(x: int, y: int) -> int
⋮----
# Test individual execution with local function
tool_call1 = dspy.ToolCalls.ToolCall(name="local_add", args={"a": 10, "b": 15})
result1 = tool_call1.execute()  # Should find local function automatically
⋮----
tool_call2 = dspy.ToolCalls.ToolCall(name="local_multiply", args={"x": 4, "y": 7})
result2 = tool_call2.execute()  # Should find local function automatically
⋮----
# Test locals take precedence over globals
⋮----
precedence_call = dspy.ToolCalls.ToolCall(name="local_add", args={"a": 1, "b": 2})
result = precedence_call.execute()
assert result == 3  # Should use local function (1+2=3), not global (1+2+1000=1003)
</file>

<file path="tests/adapters/test_two_step_adapter.py">
def test_two_step_adapter_call()
⋮----
class TestSignature(dspy.Signature)
⋮----
question: str = dspy.InputField(desc="The math question to solve")
solution: str = dspy.OutputField(desc="Step by step solution")
answer: float = dspy.OutputField(desc="The final numerical answer")
⋮----
program = dspy.Predict(TestSignature)
⋮----
mock_main_lm = mock.MagicMock(spec=dspy.LM)
⋮----
mock_extraction_lm = mock.MagicMock(spec=dspy.LM)
⋮----
result = program(question="What is 5 + 7?")
⋮----
# main LM call
⋮----
# assert first message
⋮----
content = call_kwargs["messages"][0]["content"]
⋮----
# assert second message
⋮----
content = call_kwargs["messages"][1]["content"]
⋮----
# extraction LM call
⋮----
@pytest.mark.asyncio
async def test_two_step_adapter_async_call()
⋮----
result = await program.acall(question="What is 5 + 7?")
⋮----
def test_two_step_adapter_parse()
⋮----
class ComplexSignature(dspy.Signature)
⋮----
input_text: str = dspy.InputField()
tags: list[str] = dspy.OutputField(desc="List of relevant tags")
confidence: float = dspy.OutputField(desc="Confidence score")
⋮----
first_response = "main LM response"
⋮----
adapter = dspy.TwoStepAdapter(mock_extraction_lm)
⋮----
result = adapter.parse(ComplexSignature, first_response)
⋮----
def test_two_step_adapter_parse_errors()
⋮----
question: str = dspy.InputField()
answer: str = dspy.OutputField()
</file>

<file path="tests/adapters/test_xml_adapter.py">
def test_xml_adapter_format_and_parse_basic()
⋮----
class TestSignature(dspy.Signature)
⋮----
question: str = dspy.InputField()
answer: str = dspy.OutputField()
⋮----
adapter = XMLAdapter()
# Format output fields as XML
fields_with_values = {FieldInfoWithName(name="answer", info=TestSignature.output_fields["answer"]): "Paris"}
xml = adapter.format_field_with_value(fields_with_values)
⋮----
# Parse XML output
completion = "<answer>Paris</answer>"
parsed = adapter.parse(TestSignature, completion)
⋮----
def test_xml_adapter_parse_multiple_fields()
⋮----
explanation: str = dspy.OutputField()
⋮----
completion = """
⋮----
def test_xml_adapter_parse_raises_on_missing_field()
⋮----
def test_xml_adapter_parse_casts_types()
⋮----
number: int = dspy.OutputField()
flag: bool = dspy.OutputField()
⋮----
def test_xml_adapter_parse_raises_on_type_error()
⋮----
completion = "<number>not_a_number</number>"
⋮----
def test_xml_adapter_format_and_parse_nested_model()
⋮----
class InnerModel(pydantic.BaseModel)
⋮----
value: int
label: str
⋮----
result: InnerModel = dspy.OutputField()
⋮----
fields_with_values = {
⋮----
# The output will be a JSON string inside the XML tag
⋮----
# Parse XML output (should parse as string, not as model)
completion = '<result>{"value": 5, "label": "foo"}</result>'
⋮----
# The parse_value helper will try to cast to InnerModel
⋮----
def test_xml_adapter_format_and_parse_list_of_models()
⋮----
class Item(pydantic.BaseModel)
⋮----
name: str
score: float
⋮----
items: list[Item] = dspy.OutputField()
⋮----
items = [Item(name="a", score=1.1), Item(name="b", score=2.2)]
fields_with_values = {FieldInfoWithName(name="items", info=TestSignature.output_fields["items"]): items}
⋮----
completion = f"<items>{json.dumps([i.model_dump() for i in items])}</items>"
⋮----
def test_xml_adapter_with_tool_like_output()
⋮----
# XMLAdapter does not natively support tool calls, but we can test structured output
class ToolCall(pydantic.BaseModel)
⋮----
args: dict
result: str
⋮----
tool_calls: list[ToolCall] = dspy.OutputField()
⋮----
tool_calls = [
⋮----
completion = (
⋮----
def test_xml_adapter_formats_nested_images()
⋮----
class ImageWrapper(pydantic.BaseModel)
⋮----
images: list[dspy.Image]
tag: list[str]
⋮----
class MySignature(dspy.Signature)
⋮----
image: ImageWrapper = dspy.InputField()
text: str = dspy.OutputField()
⋮----
image1 = dspy.Image(url="https://example.com/image1.jpg")
image2 = dspy.Image(url="https://example.com/image2.jpg")
image3 = dspy.Image(url="https://example.com/image3.jpg")
⋮----
image_wrapper = ImageWrapper(images=[image1, image2, image3], tag=["test", "example"])
demos = [
⋮----
image_wrapper_2 = ImageWrapper(images=[dspy.Image(url="https://example.com/image4.jpg")], tag=["test", "example"])
adapter = dspy.XMLAdapter()
messages = adapter.format(MySignature, demos, {"image": image_wrapper_2})
⋮----
# Image information in the few-shot example's user message
expected_image1_content = {"type": "image_url", "image_url": {"url": "https://example.com/image1.jpg"}}
expected_image2_content = {"type": "image_url", "image_url": {"url": "https://example.com/image2.jpg"}}
expected_image3_content = {"type": "image_url", "image_url": {"url": "https://example.com/image3.jpg"}}
⋮----
# The query image is formatted in the last user message
⋮----
def test_xml_adapter_with_code()
⋮----
# Test with code as input field
class CodeAnalysis(dspy.Signature)
⋮----
"""Analyze the time complexity of the code"""
⋮----
code: dspy.Code = dspy.InputField()
result: str = dspy.OutputField()
⋮----
messages = adapter.format(CodeAnalysis, [], {"code": "print('Hello, world!')"})
⋮----
# The output field type description should be included in the system message even if the output field is nested
⋮----
# The user message should include the question and the tools
⋮----
# Test with code as output field
class CodeGeneration(dspy.Signature)
⋮----
"""Generate code to answer the question"""
⋮----
code: dspy.Code = dspy.OutputField()
⋮----
result = adapter(
⋮----
def test_xml_adapter_full_prompt()
⋮----
class QA(dspy.Signature)
⋮----
query: str = dspy.InputField()
context: str | None = dspy.InputField()
⋮----
messages = adapter.format(QA, [], {"query": "when was Marie Curie born"})
⋮----
union_type_repr = "Union[str, NoneType]" if sys.version_info >= (3, 14) else "UnionType[str, NoneType]"
⋮----
expected_system = (
⋮----
expected_user = (
⋮----
def test_format_system_message()
⋮----
"""Answer the question with multiple answers and scores"""
⋮----
answers: list[str] = dspy.OutputField()
scores: list[float] = dspy.OutputField()
⋮----
system_message = adapter.format_system_message(MySignature)
⋮----
expected_system_message = """Your input fields are:
</file>

<file path="tests/callback/test_callback.py">
@pytest.fixture(autouse=True)
def reset_settings()
⋮----
# Make sure the settings are reset after each test
original_settings = dspy.settings.copy()
⋮----
class MyCallback(BaseCallback)
⋮----
"""A simple callback that records the calls."""
⋮----
def __init__(self)
⋮----
def on_module_start(self, call_id, instance, inputs)
⋮----
def on_module_end(self, call_id, outputs, exception)
⋮----
def on_lm_start(self, call_id, instance, inputs)
⋮----
def on_lm_end(self, call_id, outputs, exception)
⋮----
def on_adapter_format_start(self, call_id, instance, inputs)
⋮----
def on_adapter_format_end(self, call_id, outputs, exception)
⋮----
def on_adapter_parse_start(self, call_id, instance, inputs)
⋮----
def on_adapter_parse_end(self, call_id, outputs, exception)
⋮----
def on_tool_start(self, call_id, instance, inputs)
⋮----
def on_tool_end(self, call_id, outputs, exception)
⋮----
def test_callback_injection(args, kwargs)
⋮----
class Target(dspy.Module)
⋮----
@with_callbacks
        def forward(self, x: int, y: str, z: float) -> int
⋮----
callback = MyCallback()
⋮----
target = Target()
result = target.forward(*args, **kwargs)
⋮----
def test_callback_injection_local()
⋮----
target_1 = Target(callbacks=[callback])
result = target_1.forward(1, "2", 3.0)
⋮----
target_2 = Target()
result = target_2.forward(1, "2", 3.0)
⋮----
# Other instance should not trigger the callback
⋮----
def test_callback_error_handling()
⋮----
def test_multiple_callbacks()
⋮----
callback_1 = MyCallback()
callback_2 = MyCallback()
⋮----
result = target.forward(1, "2", 3.0)
⋮----
def test_callback_complex_module()
⋮----
cot = dspy.ChainOfThought("question -> answer", n=3)
result = cot(question="How are you?")
⋮----
# Parsing will run per output (n=3)
⋮----
@pytest.mark.asyncio
async def test_callback_async_module()
⋮----
result = await cot.acall(question="How are you?")
⋮----
def test_tool_calls()
⋮----
def tool_1(query: str) -> str
⋮----
"""A dummy tool function."""
⋮----
def tool_2(query: str) -> str
⋮----
"""Another dummy tool function."""
⋮----
class MyModule(dspy.Module)
⋮----
def forward(self, query: str) -> str
⋮----
query = self.tools[0](query=query)
⋮----
module = MyModule()
result = module("query")
⋮----
def test_active_id()
⋮----
# Test the call ID is generated and handled properly
class CustomCallback(BaseCallback)
⋮----
parent_call_id = ACTIVE_CALL_ID.get()
⋮----
class Parent(dspy.Module)
⋮----
def forward(self)
⋮----
class Child(dspy.Module)
⋮----
callback = CustomCallback()
⋮----
parent = Parent()
⋮----
# All three calls should have different call ids
⋮----
parent_call_id = callback.call_ids[0]
</file>

<file path="tests/clients/test_cache.py">
@dataclass
class DummyResponse
⋮----
message: str
usage: dict
⋮----
@pytest.fixture
def cache_config(tmp_path)
⋮----
"""Default cache configuration."""
⋮----
"disk_size_limit_bytes": 1024 * 1024,  # 1MB
⋮----
@pytest.fixture
def cache(cache_config)
⋮----
"""Create a cache instance with the default configuration."""
⋮----
def test_initialization(tmp_path)
⋮----
"""Test different cache initialization configurations."""
# Test memory-only cache
memory_cache = Cache(
⋮----
# Test disk-only cache
disk_cache = Cache(
⋮----
# Test disabled cache
disabled_cache = Cache(
⋮----
def test_invalid_cache_initialization()
⋮----
def test_cache_key_generation(cache)
⋮----
"""Test cache key generation with different types of inputs."""
# Test with simple dictionary
request = {"prompt": "Hello", "model": "openai/gpt-4o-mini", "temperature": 0.7}
key = cache.cache_key(request)
⋮----
assert len(key) == 64  # SHA-256 hash is 64 characters
⋮----
# Test with pydantic model
class TestModel(pydantic.BaseModel)
⋮----
name: str
value: int
⋮----
model = TestModel(name="test", value=42)
request_with_model = {"data": model}
key_with_model = cache.cache_key(request_with_model)
⋮----
# Test with pydantic model class
request_with_model_class = {"model_class": TestModel}
key_with_model_class = cache.cache_key(request_with_model_class)
⋮----
def test_put_and_get(cache)
⋮----
"""Test putting and getting from cache."""
# Test putting and getting from memory cache
⋮----
value = DummyResponse(message="This is a test response", usage={"prompt_tokens": 10, "completion_tokens": 20})
⋮----
result = cache.get(request)
⋮----
# Test with disk cache
# First, clear memory cache to ensure we're using disk cache
⋮----
# Get from disk cache
result_from_disk = cache.get(request)
⋮----
# Verify it was also added back to memory cache
⋮----
def test_cache_miss(cache)
⋮----
"""Test getting a non-existent key."""
request = {"prompt": "Non-existent", "model": "gpt-4"}
⋮----
def test_cache_key_error_handling(cache)
⋮----
"""Test error handling for unserializable objects."""
⋮----
# Test with a request that can't be serialized to JSON
class UnserializableObject
⋮----
request = {"data": UnserializableObject()}
⋮----
# Should not raise an exception
⋮----
def test_reset_memory_cache(cache)
⋮----
"""Test resetting memory cache."""
# Add some items to the memory cache
requests = [{"prompt": f"Hello {i}", "model": "openai/gpt-4o-mini"} for i in range(5)]
⋮----
# Verify items are in memory cache
⋮----
key = cache.cache_key(req)
⋮----
# Reset memory cache
⋮----
# Verify memory cache is empty
⋮----
# But disk cache still has the items
⋮----
result = cache.get(req)
⋮----
def test_save_and_load_memory_cache(cache, tmp_path)
⋮----
"""Test saving and loading memory cache."""
⋮----
# Save memory cache to a temporary file
temp_cache_file = tmp_path / "memory_cache.pkl"
⋮----
# Create a new cache instance with disk cache disabled
new_cache = Cache(
⋮----
# Load the memory cache without allowing pickle (default)
⋮----
# Load the memory cache with allow_pickle=True
⋮----
# Verify items are in the new memory cache
⋮----
result = new_cache.get(req)
⋮----
def test_request_cache_decorator(cache)
⋮----
"""Test the lm_cache decorator."""
⋮----
# Mock the dspy.cache attribute
⋮----
# Define a test function
⋮----
@request_cache()
        def test_function(prompt, model)
⋮----
# First call should compute the result
result1 = test_function(prompt="Hello", model="openai/gpt-4o-mini")
⋮----
# Second call with same arguments should use cache
⋮----
result2 = test_function(prompt="Hello", model="openai/gpt-4o-mini")
⋮----
# Call with different arguments should compute again
result3 = test_function(prompt="Different", model="openai/gpt-4o-mini")
⋮----
def test_request_cache_decorator_with_ignored_args_for_cache_key(cache)
⋮----
"""Test the request_cache decorator with ignored_args_for_cache_key."""
⋮----
@request_cache(ignored_args_for_cache_key=["model"])
        def test_function1(prompt, model)
⋮----
@request_cache()
        def test_function2(prompt, model)
⋮----
result1 = test_function1(prompt="Hello", model="openai/gpt-4o-mini")
result2 = test_function1(prompt="Hello", model="openai/gpt-4o")
⋮----
# Because model arg is ignored, the second call should return the same result as the first
⋮----
result3 = test_function2(prompt="Hello", model="openai/gpt-4o-mini")
result4 = test_function2(prompt="Hello", model="openai/gpt-4o")
⋮----
# Because model arg is not ignored, the second call should return a different result
⋮----
@pytest.mark.asyncio
async def test_request_cache_decorator_async(cache)
⋮----
"""Test the request_cache decorator with async functions."""
⋮----
@request_cache()
        async def test_function(prompt, model)
⋮----
result1 = await test_function(prompt="Hello", model="openai/gpt-4o-mini")
⋮----
result2 = await test_function(prompt="Hello", model="openai/gpt-4o-mini")
⋮----
result3 = await test_function(prompt="Different", model="openai/gpt-4o-mini")
⋮----
def test_cache_consistency_with_lm_call_modifies_the_request(cache)
⋮----
"""Test that the cache is consistent with the LM call that modifies the request."""
⋮----
@request_cache()
        def test_function(**kwargs)
⋮----
# The cache key should use the original request, not the modified one
⋮----
def test_cache_fallback_on_restricted_environment()
⋮----
"""Test that DSPy gracefully falls back to memory-only cache when disk cache fails."""
old_env = os.environ.get("DSPY_CACHEDIR")
⋮----
# Set an invalid cache directory that can't be created
⋮----
# Cache should work with memory-only fallback despite invalid disk path
test_request = {"model": "test", "prompt": "hello"}
⋮----
result = dspy.cache.get(test_request)
</file>

<file path="tests/clients/test_databricks.py">
"""Test the Databricks finetuning and deployment.

This test requires valid Databricks credentials, so it is skipped on github actions. Right now it is only used for
manual testing.
"""
⋮----
# Skip the test if the Databricks SDK is not configured or credentials are not available.
pytestmark = pytest.mark.skip(reason="Databricks SDK not configured or credentials not available")
⋮----
def test_create_directory_in_databricks_unity_catalog()
⋮----
w = WorkspaceClient()
⋮----
# Check that the directory was created successfully, otherwise `get_directory_metadata` will raise an exception.
⋮----
def test_create_finetuning_job()
⋮----
fake_training_data = [
⋮----
job = TrainingJobDatabricks()
⋮----
def test_deploy_finetuned_model()
⋮----
model_to_deploy = "main.chenmoney.finetuned_model"
⋮----
lm = dspy.LM(model="databricks/main_chenmoney_finetuned_model")
</file>

<file path="tests/clients/test_embedding.py">
# Mock response format similar to litellm's embedding response.
class MockEmbeddingResponse
⋮----
def __init__(self, embeddings)
⋮----
@pytest.fixture
def cache(tmp_path)
⋮----
original_cache = dspy.cache
⋮----
def test_litellm_embedding(cache)
⋮----
model = "text-embedding-ada-002"
inputs = ["hello", "world"]
mock_embeddings = [
⋮----
[0.1, 0.2, 0.3],  # embedding for "hello"
[0.4, 0.5, 0.6],  # embedding for "world"
⋮----
# Configure mock to return proper response format.
⋮----
# Create embedding instance and call it.
embedding = Embedder(model, caching=True)
result = embedding(inputs)
⋮----
# Verify litellm was called with correct parameters.
# Because we disable the litellm cache, it should be called with caching=False.
⋮----
# Second call should be cached.
⋮----
# Disable cache should issue new calls.
embedding = Embedder(model, caching=False)
⋮----
def test_callable_embedding(cache)
⋮----
inputs = ["hello", "world", "test"]
⋮----
expected_embeddings = [
⋮----
[0.7, 0.8, 0.9],  # embedding for "test"
⋮----
class EmbeddingFn
⋮----
def __init__(self)
⋮----
def __call__(self, texts)
⋮----
# Simple callable that returns random embeddings.
⋮----
embedding_fn = EmbeddingFn()
⋮----
# Create embedding instance with callable
embedding = Embedder(embedding_fn)
⋮----
# The second call should be cached.
⋮----
def test_invalid_model_type()
⋮----
# Test that invalid model type raises ValueError
⋮----
embedding = Embedder(123)  # Invalid model type
⋮----
@pytest.mark.asyncio
async def test_async_embedding()
⋮----
result = await embedding.acall(inputs)
</file>

<file path="tests/clients/test_inspect_global_history.py">
@pytest.fixture(autouse=True)
def clear_history()
⋮----
def test_inspect_history_basic(capsys)
⋮----
# Configure a DummyLM with some predefined responses
lm = DummyLM([{"response": "Hello"}, {"response": "How are you?"}])
⋮----
# Make some calls to generate history
predictor = dspy.Predict("query: str -> response: str")
⋮----
# Test inspecting all history
history = GLOBAL_HISTORY
⋮----
def test_inspect_history_with_n(capsys)
⋮----
"""Test that inspect_history works with n
    Random failures in this test most likely mean you are printing messages somewhere
    """
lm = DummyLM([{"response": "One"}, {"response": "Two"}, {"response": "Three"}])
⋮----
# Generate some history
⋮----
# Test getting last 2 entries
⋮----
def test_inspect_empty_history(capsys)
⋮----
# Configure fresh DummyLM
lm = DummyLM([])
⋮----
# Test inspecting empty history
⋮----
def test_inspect_history_n_larger_than_history(capsys)
⋮----
lm = DummyLM([{"response": "First"}, {"response": "Second"}])
⋮----
# Request more entries than exist
⋮----
assert len(history) == 2  # Should return all available entries
</file>

<file path="tests/clients/test_lm_local.py">
@patch("dspy.clients.lm_local.threading.Thread")
@patch("dspy.clients.lm_local.subprocess.Popen")
@patch("dspy.clients.lm_local.get_free_port")
@patch("dspy.clients.lm_local.wait_for_server")
def test_command_with_spaces_in_path(mock_wait, mock_port, mock_popen, mock_thread)
⋮----
mock_process = mock.Mock()
⋮----
lm = mock.Mock(spec=[])
⋮----
call_args = mock_popen.call_args
command = call_args[0][0]
⋮----
model_index = command.index("--model-path")
⋮----
@patch("dspy.clients.lm_local.threading.Thread")
@patch("dspy.clients.lm_local.subprocess.Popen")
@patch("dspy.clients.lm_local.get_free_port")
@patch("dspy.clients.lm_local.wait_for_server")
def test_command_construction_prevents_injection(mock_wait, mock_port, mock_popen, mock_thread)
⋮----
@patch("dspy.clients.lm_local.threading.Thread")
@patch("dspy.clients.lm_local.subprocess.Popen")
@patch("dspy.clients.lm_local.get_free_port")
@patch("dspy.clients.lm_local.wait_for_server")
def test_command_is_list_not_string(mock_wait, mock_port, mock_popen, mock_thread)
</file>

<file path="tests/clients/test_lm.py">
def make_response(output_blocks)
⋮----
def test_chat_lms_can_be_queried(litellm_test_server)
⋮----
expected_response = ["Hi!"]
⋮----
openai_lm = dspy.LM(
⋮----
azure_openai_lm = dspy.LM(
⋮----
def test_dspy_cache(litellm_test_server, tmp_path)
⋮----
original_cache = dspy.cache
⋮----
cache = dspy.cache
⋮----
lm = dspy.LM(
⋮----
cache_key = next(iter(cache.memory_cache.keys()))
⋮----
def test_disabled_cache_skips_cache_key(monkeypatch)
⋮----
def fake_completion(*, cache, num_retries, retry_strategy, **request)
⋮----
dummy_lm = DummyLM([{"answer": "ignored"}])
# TODO(isaacbmiller): Change from dummy_lm.forward to just dummy_lm.__call__ #8864
⋮----
def test_rollout_id_bypasses_cache(monkeypatch, tmp_path)
⋮----
calls: list[dict] = []
⋮----
lm = dspy.LM(model="openai/dspy-test-model", model_type="chat")
⋮----
def test_zero_temperature_rollout_warns_once(monkeypatch)
⋮----
def test_text_lms_can_be_queried(litellm_test_server)
⋮----
def test_lm_calls_support_callables(litellm_test_server)
⋮----
def azure_ad_token_provider(*args, **kwargs)
⋮----
lm_with_callable = dspy.LM(
⋮----
call_args = spy_completion.call_args.kwargs
⋮----
def test_lm_calls_support_pydantic_models(litellm_test_server)
⋮----
class ResponseFormat(pydantic.BaseModel)
⋮----
response: str
⋮----
def test_retry_number_set_correctly()
⋮----
lm = dspy.LM("openai/gpt-4o-mini", num_retries=3)
⋮----
def test_retry_made_on_system_errors()
⋮----
retry_tracking = [0]  # Using a list to track retries
⋮----
def mock_create(*args, **kwargs)
⋮----
# These fields are called during the error handling
mock_response = mock.Mock()
⋮----
lm = dspy.LM(model="openai/gpt-4o-mini", max_tokens=250, num_retries=3)
⋮----
def test_reasoning_model_token_parameter()
⋮----
test_cases = [
⋮----
("azure/gpt-5-chat", False),  # gpt-5-chat is NOT a reasoning model
⋮----
@pytest.mark.parametrize("model_name", ["openai/o1", "openai/gpt-5-nano", "openai/gpt-5-mini"])
def test_reasoning_model_requirements(model_name)
⋮----
# Should raise assertion error if temperature or max_tokens requirements not met
⋮----
temperature=0.7,  # Should be 1.0
max_tokens=1000,  # Should be >= 16_000
⋮----
# Should pass with correct parameters
⋮----
# Should pass with no parameters
⋮----
def test_gpt_5_chat_not_reasoning_model()
⋮----
"""Test that gpt-5-chat is NOT treated as a reasoning model."""
# Should NOT raise validation error - gpt-5-chat is not a reasoning model
⋮----
temperature=0.7,  # Can be any value
max_tokens=1000,  # Can be any value
⋮----
# Should use max_tokens, not max_completion_tokens
⋮----
def test_dump_state()
⋮----
def test_exponential_backoff_retry()
⋮----
time_counter = []
⋮----
lm = dspy.LM(model="openai/gpt-3.5-turbo", max_tokens=250, num_retries=3)
⋮----
# The first retry happens immediately regardless of the configuration
⋮----
def test_logprobs_included_when_requested()
⋮----
lm = dspy.LM(model="dspy-test-model", logprobs=True, cache=False)
⋮----
result = lm("question")
⋮----
@pytest.mark.asyncio
async def test_async_lm_call()
⋮----
mock_response = ModelResponse(choices=[Choices(message=Message(content="answer"))], model="openai/gpt-4o-mini")
⋮----
lm = dspy.LM(model="openai/gpt-4o-mini", cache=False)
result = await lm.acall("question")
⋮----
@pytest.mark.asyncio
async def test_async_lm_call_with_cache(tmp_path)
⋮----
"""Test the async LM call with caching."""
⋮----
lm = dspy.LM(model="openai/gpt-4o-mini")
⋮----
# Second call should hit the cache, so no new call to LiteLLM is made.
⋮----
# A new query should result in a new LiteLLM call and a new cache entry.
⋮----
def test_lm_history_size_limit()
⋮----
def test_disable_history()
⋮----
def test_responses_api()
⋮----
api_response = make_response(
⋮----
lm_result = lm("openai query")
⋮----
def test_lm_replaces_system_with_developer_role()
⋮----
def test_responses_api_tool_calls(litellm_test_server)
⋮----
expected_tool_call = {
expected_response = [{"tool_calls": [expected_tool_call]}]
⋮----
def test_reasoning_effort_responses_api()
⋮----
"""Test that reasoning_effort gets normalized to reasoning format for Responses API."""
⋮----
# OpenAI model with Responses API - should normalize
⋮----
call_kwargs = mock_responses.call_args.kwargs
⋮----
def test_call_reasoning_model_with_chat_api()
⋮----
"""Test that Chat API properly handles reasoning models and returns data in correct format."""
# Create message with reasoning_content attribute
message = Message(content="The answer is 4", role="assistant")
# Add reasoning_content attribute
⋮----
# Create choice with the message
mock_choice = Choices(message=message)
⋮----
# Mock response with reasoning content for chat completion
mock_response = ModelResponse(
⋮----
# Create reasoning model with chat API
⋮----
# Test the call
result = lm("What is 2 + 2?")
⋮----
# Verify the response format
⋮----
# Verify mock was called with correct parameters
⋮----
call_kwargs = mock_completion.call_args.kwargs
⋮----
def test_api_key_not_saved_in_json()
⋮----
predict = dspy.Predict("question -> answer")
⋮----
json_path = Path(tmpdir) / "program.json"
⋮----
saved_state = json.load(f)
⋮----
# Verify API key is not in the saved state
⋮----
# Verify other attributes are saved
⋮----
def test_responses_api_converts_images_correctly()
⋮----
# Test with base64 image
request_with_base64_image = {
⋮----
result = _convert_chat_request_to_responses_request(request_with_base64_image)
⋮----
content = result["input"][0]["content"]
⋮----
# First item should be text converted to input_text format
⋮----
# Second item should be converted to input_image format
⋮----
# Test with URL image
request_with_url_image = {
⋮----
result = _convert_chat_request_to_responses_request(request_with_url_image)
⋮----
def test_responses_api_converts_files_correctly()
⋮----
# Test with file data (base64 encoded)
request_with_file = {
⋮----
result = _convert_chat_request_to_responses_request(request_with_file)
⋮----
# Second item should be converted to input_file format
⋮----
# Test with file_id
request_with_file_id = {
⋮----
result = _convert_chat_request_to_responses_request(request_with_file_id)
⋮----
# Test with all file fields
request_with_all_fields = {
⋮----
result = _convert_chat_request_to_responses_request(request_with_all_fields)
⋮----
def test_responses_api_with_image_input()
⋮----
# Test with messages containing an image
messages = [
⋮----
lm_result = lm(messages=messages)
⋮----
call_args = dspy_responses.call_args.kwargs
⋮----
# Verify the request was converted correctly
⋮----
content = call_args["input"][0]["content"]
⋮----
# Check that image was converted to input_image format
image_content = [c for c in content if c.get("type") == "input_image"]
⋮----
def test_responses_api_with_pydantic_model_input()
⋮----
class TestModel(pydantic.BaseModel)
⋮----
answer: str
number: int
⋮----
# Test with messages containing a Pydantic model as response format
lm_result = lm("What is a good test answer?", response_format=TestModel)
⋮----
# Try to validate to Pydantic model
⋮----
response_format = call_args["text"]["format"]
⋮----
@pytest.mark.asyncio
async def test_streaming_passes_headers_correctly()
⋮----
custom_headers = {"Authorization": "Bearer my-custom-token"}
request = {
⋮----
mock_stream = mock.AsyncMock()
⋮----
async def empty_async_generator()
⋮----
yield  # Make it a generator
⋮----
stream_fn = _get_stream_completion_fn(request, {}, sync=False, headers=custom_headers)
⋮----
# Verify headers were passed to litellm.acompletion
⋮----
call_kwargs = mock_acompletion.call_args.kwargs
</file>

<file path="tests/datasets/test_dataset.py">
dummy_data = """content,question,answer
⋮----
class CSVDataset(Dataset)
⋮----
def __init__(self, file_path, input_keys=None, **kwargs) -> None
⋮----
df = pd.read_csv(file_path)
data = df.to_dict(orient="records")
⋮----
@pytest.fixture
def csv_file()
⋮----
@pytest.mark.extra
def test_input_keys(csv_file)
⋮----
dataset = CSVDataset(csv_file, input_keys=["content", "question"])
⋮----
inputs = example.inputs()
</file>

<file path="tests/evaluate/test_evaluate.py">
def new_example(question, answer)
⋮----
"""Helper function to create a new example."""
⋮----
def test_evaluate_initialization()
⋮----
devset = [new_example("What is 1+1?", "2")]
ev = Evaluate(
⋮----
def test_evaluate_call()
⋮----
devset = [new_example("What is 1+1?", "2"), new_example("What is 2+2?", "4")]
program = Predict("question -> answer")
⋮----
score = ev(program)
⋮----
@pytest.mark.extra
def test_construct_result_df()
⋮----
devset = [
⋮----
results = [
result_df = ev._construct_result_table(results, answer_exact_match.__name__)
⋮----
def test_multithread_evaluate_call()
⋮----
result = ev(program)
⋮----
def test_multi_thread_evaluate_call_cancelled(monkeypatch)
⋮----
# slow LM that sleeps for 1 second before returning the answer
class SlowLM(DummyLM)
⋮----
def __call__(self, *args, **kwargs)
⋮----
# spawn a thread that will sleep for .1 seconds then send a KeyboardInterrupt
def sleep_then_interrupt()
⋮----
input_thread = threading.Thread(target=sleep_then_interrupt)
⋮----
def test_evaluate_call_wrong_answer()
⋮----
# Create programs that do not return dictionary-like objects because Evaluate()
# has failed for such cases in the past
⋮----
@pytest.mark.parametrize("display_table", [True, False, 1])
@pytest.mark.parametrize("is_in_ipython_notebook_environment", [True, False])
def test_evaluate_display_table(program_with_example, display_table, is_in_ipython_notebook_environment, capfd)
⋮----
example_input = next(iter(example.inputs().values()))
example_output = {key: value for key, value in example.toDict().items() if key not in example.inputs()}
⋮----
# In console environments where IPython is not available, the table should be printed
# to the console
⋮----
def test_evaluate_callback()
⋮----
class TestCallback(BaseCallback)
⋮----
def __init__(self)
⋮----
callback = TestCallback()
⋮----
def test_evaluation_result_repr()
⋮----
result = EvaluationResult(score=100.0, results=[(new_example("What is 1+1?", "2"), {"answer": "2"}, 100.0)])
⋮----
def test_evaluate_save_as_json_with_history()
⋮----
"""Test that save_as_json works with Examples containing dspy.History objects."""
# Setup
⋮----
# Create history objects
history1 = dspy.History(
history2 = dspy.History(
⋮----
# Create examples with history
⋮----
# Create evaluator with save_as_json
⋮----
temp_json = f.name
⋮----
evaluator = Evaluate(
⋮----
result = evaluator(program)
⋮----
# Verify JSON file was created and is valid
⋮----
data = json.load(f)
⋮----
# Verify history was properly serialized in first record
⋮----
# Verify history was properly serialized in second record
⋮----
def test_evaluate_save_as_csv_with_history()
⋮----
"""Test that save_as_csv works with Examples containing dspy.History objects."""
⋮----
# Create history object
history = dspy.History(
⋮----
# Create example with history
⋮----
# Create evaluator with save_as_csv
⋮----
temp_csv = f.name
⋮----
# Verify CSV file was created
⋮----
reader = csv.DictReader(f)
rows = list(reader)
⋮----
# CSV will have string representation of the dict
</file>

<file path="tests/evaluate/test_metrics.py">
# FILEPATH: /Users/ahle/repos/dspy/tests/evaluate/test_metrics.py
⋮----
def test_answer_exact_match_string()
⋮----
example = dspy.Example(
pred = Predict("question -> answer")
⋮----
def test_answer_exact_match_list()
⋮----
def test_answer_exact_match_no_match()
</file>

<file path="tests/examples/test_baleen.py">
class GenerateAnswer(dspy.Signature)
⋮----
"""Answer questions with short factoid answers."""
⋮----
context = dspy.InputField(desc="may contain relevant facts")
question = dspy.InputField()
answer = dspy.OutputField(desc="often between 1 and 5 words")
⋮----
class GenerateSearchQuery(dspy.Signature)
⋮----
"""Write a simple search query that will help answer a complex question."""
⋮----
query = dspy.OutputField()
⋮----
class SimplifiedBaleen(dspy.Module)
⋮----
def __init__(self, passages_per_hop=3, max_hops=2)
⋮----
def forward(self, question)
⋮----
context = []
⋮----
query = self.generate_query[hop](context=context, question=question).query
passages = self.retrieve(query).passages
context = deduplicate(context + passages)
⋮----
pred = self.generate_answer(context=context, question=question)
⋮----
def load_hotpotqa()
⋮----
# Load the dataset.
dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)
# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.
trainset = [x.with_inputs("question") for x in dataset.train]
devset = [x.with_inputs("question") for x in dataset.dev]
⋮----
# @pytest.mark.slow_test
# TODO: Find a way to make this test run without openai
def _test_baleen()
⋮----
lm = dspy.OpenAI(model="gpt-3.5-turbo")
rm = dspy.ColBERTv2(url="http://20.102.90.50:2017/wiki17_abstracts")
⋮----
# Ask any question you like to this simple RAG program.
my_question = "How many storeys are in the castle that David Gregory inherited?"
⋮----
# Get the prediction. This contains `pred.context` and `pred.answer`.
uncompiled_baleen = SimplifiedBaleen()  # uncompiled (i.e., zero-shot) program
pred = uncompiled_baleen(my_question)
⋮----
def validate_context_and_answer_and_hops(example, pred, trace=None)
⋮----
hops = [example.question] + [outputs.query for *_, outputs in trace if "query" in outputs]
⋮----
def gold_passages_retrieved(example, pred, trace=None)
⋮----
gold_titles = set(map(dspy.evaluate.normalize_text, example["gold_titles"]))
found_titles = set(map(dspy.evaluate.normalize_text, [c.split(" | ")[0] for c in pred.context]))
⋮----
# TODO: Find a way to make this test run without the slow hotpotqa dataset
def _test_compiled_baleen()
⋮----
teleprompter = BootstrapFewShot(metric=validate_context_and_answer_and_hops)
compiled_baleen = teleprompter.compile(
⋮----
evaluate_on_hotpotqa = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)
uncompiled_baleen_retrieval_score = evaluate_on_hotpotqa(
# assert uncompiled_baleen_retrieval_score / 100 == 18 / 50
⋮----
compiled_baleen_retrieval_score = evaluate_on_hotpotqa(compiled_baleen, metric=gold_passages_retrieved)
# assert compiled_baleen_retrieval_score / 100 == 27 / 50
</file>

<file path="tests/metadata/test_metadata.py">
def test_metadata()
</file>

<file path="tests/predict/test_aggregation.py">
def test_majority_with_prediction()
⋮----
prediction = Prediction.from_completions([{"answer": "2"}, {"answer": "2"}, {"answer": "3"}])
result = majority(prediction)
⋮----
def test_majority_with_completions()
⋮----
completions = Completions([{"answer": "2"}, {"answer": "2"}, {"answer": "3"}])
result = majority(completions)
⋮----
def test_majority_with_list()
⋮----
completions = [{"answer": "2"}, {"answer": "2"}, {"answer": "3"}]
⋮----
def test_majority_with_normalize()
⋮----
completions = [{"answer": "2"}, {"answer": " 2"}, {"answer": "3"}]
result = majority(completions, normalize=normalize_text)
⋮----
def test_majority_with_field()
⋮----
completions = [
result = majority(completions, field="other")
⋮----
def test_majority_with_no_majority()
⋮----
completions = [{"answer": "2"}, {"answer": "3"}, {"answer": "4"}]
⋮----
assert result.completions[0]["answer"] == "2"  # The first completion is returned in case of a tie
</file>

<file path="tests/predict/test_best_of_n.py">
class DummyModule(dspy.Module)
⋮----
def __init__(self, signature, forward_fn)
⋮----
def forward(self, **kwargs) -> Prediction
⋮----
def test_refine_forward_success_first_attempt()
⋮----
lm = DummyLM([{"answer": "Brussels"}, {"answer": "City of Brussels"}, {"answer": "Brussels"}])
⋮----
module_call_count = [0]
⋮----
def count_calls(self, **kwargs)
⋮----
reward_call_count = [0]
⋮----
def reward_fn(kwargs, pred: Prediction) -> float
⋮----
# The answer should always be one word.
⋮----
predict = DummyModule("question -> answer", count_calls)
⋮----
best_of_n = BestOfN(module=predict, N=3, reward_fn=reward_fn, threshold=1.0)
result = best_of_n(question="What is the capital of Belgium?")
⋮----
def test_refine_module_default_fail_count()
⋮----
def always_raise(self, **kwargs)
⋮----
predict = DummyModule("question -> answer", always_raise)
⋮----
best_of_n = BestOfN(module=predict, N=3, reward_fn=lambda _, __: 1.0, threshold=0.0)
⋮----
def test_refine_module_custom_fail_count()
⋮----
def raise_on_second_call(self, **kwargs)
⋮----
predict = DummyModule("question -> answer", raise_on_second_call)
⋮----
best_of_n = BestOfN(module=predict, N=3, reward_fn=lambda _, __: 1.0, threshold=0.0, fail_count=1)
</file>

<file path="tests/predict/test_chain_of_thought.py">
def test_initialization_with_string_signature()
⋮----
lm = DummyLM([{"reasoning": "find the number after 1", "answer": "2"}])
⋮----
predict = ChainOfThought("question -> answer")
⋮----
@pytest.mark.asyncio
async def test_async_chain_of_thought()
⋮----
program = ChainOfThought("question -> answer")
result = await program.acall(question="What is 1+1?")
⋮----
def test_chain_of_thought_with_native_reasoning()
⋮----
"""Test ChainOfThought with a model that supports native reasoning, but using manual fields."""
⋮----
lm = dspy.LM(model="anthropic/claude-3-7-sonnet-20250219", cache=False)
⋮----
cot = ChainOfThought("question -> answer")
result = cot(question="What is the capital of France?")
⋮----
def test_chain_of_thought_with_manual_reasoning()
⋮----
"""Test ChainOfThought with manual reasoning where LM doesn't support native reasoning."""
lm = dspy.LM(model="openai/gpt-4o-mini")
</file>

<file path="tests/predict/test_code_act.py">
# This test suite requires deno to be installed. Please install deno following https://docs.deno.com/runtime/getting_started/installation/
is_deno_available = shutil.which("deno") is not None
skip_if_deno_not_available = pytest.mark.skipif(
⋮----
class BasicQA(Signature)
⋮----
question = dspy.InputField()
answer = dspy.OutputField(desc="often between 1 and 5 words")
⋮----
def add(a: float, b: float) -> float
⋮----
"add two numbers"
⋮----
@skip_if_deno_not_available
def test_codeact_code_generation()
⋮----
lm = DummyLM(
⋮----
program = CodeAct(BasicQA, tools=[add])
res = program(question="What is 1+1?")
⋮----
class ExtremumFinder(Signature)
⋮----
input_list = dspy.InputField()
maximum = dspy.OutputField(desc="The maximum of the given numbers")
minimum = dspy.OutputField(desc="The minimum of the given numbers")
⋮----
def extract_maximum_minimum(input_list: str) -> dict[str, float]
⋮----
numbers = list(map(float, input_list.split(",")))
⋮----
@skip_if_deno_not_available
def test_codeact_support_multiple_fields()
⋮----
program = CodeAct(ExtremumFinder, tools=[extract_maximum_minimum])
res = program(input_list="2, 3, 5, 6")
⋮----
@skip_if_deno_not_available
def test_codeact_code_parse_failure()
⋮----
@skip_if_deno_not_available
def test_codeact_code_execution_failure()
⋮----
class CustomTool
⋮----
def __call__(self, a: float, b: float) -> float
⋮----
@skip_if_deno_not_available
def test_codeact_tool_validation()
</file>

<file path="tests/predict/test_knn.py">
def mock_example(question: str, answer: str) -> dspy.Example
⋮----
"""Creates a mock DSP example with specified question and answer."""
⋮----
@pytest.fixture
def setup_knn() -> KNN
⋮----
"""Sets up a KNN instance with a mocked vectorizer for testing."""
trainset = [
⋮----
def test_knn_initialization(setup_knn)
⋮----
"""Tests the KNN initialization and checks if the trainset vectors are correctly created."""
knn = setup_knn
⋮----
def test_knn_query(setup_knn)
⋮----
"""Tests the KNN query functionality for retrieving the nearest neighbors."""
⋮----
query = {"question": "What is 3+3?"}  # A query close to "What is 2+2?"
nearest_samples = knn(**query)
⋮----
def test_knn_query_specificity(setup_knn)
⋮----
"""Tests the KNN query functionality for specificity of returned examples."""
⋮----
query = {"question": "What is the capital of Germany?"}  # A query close to "What is the capital of France?"
</file>

<file path="tests/predict/test_multi_chain_comparison.py">
class BasicQA(dspy.Signature)
⋮----
"""Answer questions with short factoid answers."""
⋮----
question = dspy.InputField()
answer = dspy.OutputField(desc="often between 1 and 5 words")
⋮----
# Example completions generated by a model for reference
completions = [
⋮----
def test_basic_example()
⋮----
# Pass signature to MultiChainComparison module
compare_answers = dspy.MultiChainComparison(BasicQA)
⋮----
# Call the MultiChainComparison on the completions
question = "What is the color of the sky?"
lm = DummyLM([{"rationale": "my rationale", "answer": "blue"}])
⋮----
final_pred = compare_answers(completions, question=question)
</file>

<file path="tests/predict/test_parallel.py">
def test_parallel_module()
⋮----
lm = DummyLM(
⋮----
class MyModule(dspy.Module)
⋮----
def __init__(self)
⋮----
def forward(self, input)
⋮----
output = MyModule()(dspy.Example(input="test input").with_inputs("input"))
⋮----
expected_outputs = {f"test output {i}" for i in range(1, 6)}
⋮----
def test_batch_module()
⋮----
res_lm = DummyLM(
⋮----
res1 = self.predictor.batch([input] * 5)
⋮----
res2 = self.predictor2.batch([input] * 5)
⋮----
# Check that we got all expected outputs without caring about order
⋮----
# Check that reasoning matches outputs for reason_result
⋮----
num = r.output.split()[-1]  # get the number from "test output X"
⋮----
def test_nested_parallel_module()
⋮----
# For nested structure, check first two outputs and nested outputs separately
⋮----
all_outputs = {output[0].output, output[1].output, output[2][0].output, output[2][1].output}
⋮----
def test_nested_batch_method()
⋮----
res = self.predictor.batch([dspy.Example(input=input).with_inputs("input")] * 2)
⋮----
result = MyModule().batch([dspy.Example(input="test input").with_inputs("input")] * 2)
⋮----
def test_batch_with_failed_examples()
⋮----
class FailingModule(dspy.Module)
⋮----
def forward(self, value: int) -> str
⋮----
module = FailingModule()
⋮----
examples = [
⋮----
dspy.Example(value=42).with_inputs("value"),  # This will fail
⋮----
def test_parallel_timeout_and_straggler_limit_params()
⋮----
parallel_default = dspy.Parallel()
⋮----
parallel_custom = dspy.Parallel(timeout=0, straggler_limit=5)
⋮----
def test_batch_timeout_and_straggler_limit_params()
⋮----
class SimpleModule(dspy.Module)
⋮----
def forward(self, value: int) -> int
⋮----
module = SimpleModule()
⋮----
results = module.batch(examples, timeout=0, straggler_limit=5)
</file>

<file path="tests/predict/test_predict.py">
def test_initialization_with_string_signature()
⋮----
signature_string = "input1, input2 -> output"
predict = Predict(signature_string)
expected_instruction = "Given the fields `input1`, `input2`, produce the fields `output`."
⋮----
def test_reset_method()
⋮----
predict_instance = Predict("input -> output")
⋮----
def test_lm_after_dump_and_load_state()
⋮----
lm = dspy.LM(
⋮----
expected_lm_state = {
⋮----
dumped_state = predict_instance.dump_state()
new_instance = Predict("input -> output")
⋮----
def test_call_method()
⋮----
lm = DummyLM([{"output": "test output"}])
⋮----
result = predict_instance(input="test input")
⋮----
def test_instructions_after_dump_and_load_state()
⋮----
predict_instance = Predict(Signature("input -> output", "original instructions"))
⋮----
new_instance = Predict(Signature("input -> output", "new instructions"))
⋮----
def test_demos_after_dump_and_load_state()
⋮----
class TranslateToEnglish(dspy.Signature)
⋮----
"""Translate content from a language to English."""
⋮----
content: str = dspy.InputField()
language: str = dspy.InputField()
translation: str = dspy.OutputField()
⋮----
original_instance = Predict(TranslateToEnglish)
⋮----
dumped_state = original_instance.dump_state()
⋮----
saved_state = orjson.dumps(dumped_state).decode()
loaded_state = orjson.loads(saved_state)
⋮----
new_instance = Predict(TranslateToEnglish)
⋮----
# Demos don't need to keep the same types after saving and loading the state.
⋮----
def test_typed_demos_after_dump_and_load_state()
⋮----
class Item(pydantic.BaseModel)
⋮----
name: str
quantity: int
⋮----
class InventorySignature(dspy.Signature)
⋮----
"""Handle inventory items and their translations."""
⋮----
items: list[Item] = dspy.InputField()
⋮----
translated_items: list[Item] = dspy.OutputField()
total_quantity: int = dspy.OutputField()
⋮----
original_instance = Predict(InventorySignature)
⋮----
# Test dump_state
⋮----
# Verify the input items were properly serialized
⋮----
# Test serialization/deserialization
⋮----
# Test load_state
new_instance = Predict(InventorySignature)
⋮----
# Verify the structure is maintained after loading
loaded_demo = new_instance.demos[0]
⋮----
# Verify output items were also properly maintained
⋮----
# def test_typed_demos_after_dump_and_load_state():
#     class TypedTranslateToEnglish(dspy.Signature):
#         """Translate content from a language to English."""
⋮----
#         class Input(pydantic.BaseModel):
#             content: str
#             language: str
⋮----
#         class Output(pydantic.BaseModel):
#             translation: str
⋮----
#         input: Input = dspy.InputField()
#         output: Output = dspy.OutputField()
⋮----
#     original_instance = TypedPredictor(TypedTranslateToEnglish).predictor
#     original_instance.demos = [
#         dspy.Example(
#             input=TypedTranslateToEnglish.Input(
#                 content="¿Qué tal?",
#                 language="SPANISH",
#             ),
#             output=TypedTranslateToEnglish.Output(
#                 translation="Hello there",
⋮----
#         ).with_inputs("input"),
#     ]
⋮----
#     dumped_state = original_instance.dump_state()
#     assert len(dumped_state["demos"]) == len(original_instance.demos)
#     assert dumped_state["demos"][0]["input"] == original_instance.demos[0].input.model_dump_json()
⋮----
#     saved_state = orjson.dumps(dumped_state).decode()
#     loaded_state = orjson.loads(saved_state)
⋮----
#     new_instance = TypedPredictor(TypedTranslateToEnglish).predictor
#     new_instance.load_state(loaded_state)
#     assert len(new_instance.demos) == len(original_instance.demos)
#     # Demos don't need to keep the same types after saving and loading the state.
#     assert new_instance.demos[0]["input"] == original_instance.demos[0].input.model_dump_json()
⋮----
def test_signature_fields_after_dump_and_load_state(tmp_path)
⋮----
class CustomSignature(dspy.Signature)
⋮----
"""I am just an instruction."""
⋮----
sentence = dspy.InputField(desc="I am an innocent input!")
sentiment = dspy.OutputField()
⋮----
file_path = tmp_path / "tmp.json"
original_instance = Predict(CustomSignature)
⋮----
class CustomSignature2(dspy.Signature)
⋮----
"""I am not a pure instruction."""
⋮----
sentence = dspy.InputField(desc="I am a malicious input!")
sentiment = dspy.OutputField(desc="I am a malicious output!", prefix="I am a prefix!")
⋮----
new_instance = Predict(CustomSignature2)
⋮----
# After loading, the fields should be the same.
⋮----
@pytest.mark.parametrize("filename", ["model.json", "model.pkl"])
def test_lm_field_after_dump_and_load_state(tmp_path, filename)
⋮----
file_path = tmp_path / filename
⋮----
original_predict = dspy.Predict("q->a")
⋮----
loaded_predict = dspy.Predict("q->a")
⋮----
def test_forward_method()
⋮----
program = Predict("question -> answer")
⋮----
result = program(question="What is 1+1?").answer
⋮----
def test_forward_method2()
⋮----
program = Predict("question -> answer1, answer2")
⋮----
result = program(question="What is 1+1?")
⋮----
def test_config_management()
⋮----
config = predict_instance.get_config()
⋮----
def test_multi_output()
⋮----
program = Predict("question -> answer", n=2)
⋮----
results = program(question="What is 1+1?")
⋮----
def test_multi_output2()
⋮----
program = Predict("question -> answer1, answer2", n=2)
⋮----
def test_datetime_inputs_and_outputs()
⋮----
# Define a model for datetime inputs and outputs
class TimedEvent(pydantic.BaseModel)
⋮----
event_name: str
event_time: datetime
⋮----
class TimedSignature(dspy.Signature)
⋮----
events: list[TimedEvent] = dspy.InputField()
summary: str = dspy.OutputField()
next_event_time: datetime = dspy.OutputField()
⋮----
program = Predict(TimedSignature)
⋮----
lm = DummyLM(
⋮----
output = program(
⋮----
def test_explicitly_valued_enum_inputs_and_outputs()
⋮----
class Status(enum.Enum)
⋮----
PENDING = "pending"
IN_PROGRESS = "in_progress"
COMPLETED = "completed"
⋮----
class StatusSignature(dspy.Signature)
⋮----
current_status: Status = dspy.InputField()
next_status: Status = dspy.OutputField()
⋮----
program = Predict(StatusSignature)
⋮----
output = program(current_status=Status.PENDING)
⋮----
def test_enum_inputs_and_outputs_with_shared_names_and_values()
⋮----
class TicketStatus(enum.Enum)
⋮----
OPEN = "CLOSED"
CLOSED = "RESOLVED"
RESOLVED = "OPEN"
⋮----
class TicketStatusSignature(dspy.Signature)
⋮----
current_status: TicketStatus = dspy.InputField()
next_status: TicketStatus = dspy.OutputField()
⋮----
program = Predict(TicketStatusSignature)
⋮----
# Mock reasoning and output
⋮----
"next_status": "RESOLVED",  # Refers to TicketStatus.CLOSED by value
⋮----
output = program(current_status=TicketStatus.OPEN)
assert output.next_status == TicketStatus.CLOSED  # By value
⋮----
def test_auto_valued_enum_inputs_and_outputs()
⋮----
Status = enum.Enum("Status", ["PENDING", "IN_PROGRESS", "COMPLETED"])  # noqa: N806
⋮----
"next_status": "IN_PROGRESS",  # Use the auto-assigned value for IN_PROGRESS
⋮----
def test_named_predictors()
⋮----
class MyModule(dspy.Module)
⋮----
def __init__(self)
⋮----
program = MyModule()
⋮----
# Check that it also works the second time.
program2 = copy.deepcopy(program)
⋮----
def test_output_only()
⋮----
class OutputOnlySignature(dspy.Signature)
⋮----
output = dspy.OutputField()
⋮----
predictor = Predict(OutputOnlySignature)
⋮----
lm = DummyLM([{"output": "short answer"}])
⋮----
def test_load_state_chaining()
⋮----
"""Test that load_state returns self for chaining."""
original = Predict("question -> answer")
⋮----
state = original.dump_state()
⋮----
new_instance = Predict("question -> answer").load_state(state)
⋮----
@pytest.mark.parametrize("adapter_type", ["chat", "json"])
def test_call_predict_with_chat_history(adapter_type)
⋮----
class SpyLM(dspy.LM)
⋮----
def __init__(self, *args, return_json=False, **kwargs)
⋮----
def __call__(self, prompt=None, messages=None, **kwargs)
⋮----
class MySignature(dspy.Signature)
⋮----
question: str = dspy.InputField()
history: dspy.History = dspy.InputField()
answer: str = dspy.OutputField()
⋮----
program = Predict(MySignature)
⋮----
lm = SpyLM("dummy_model")
⋮----
lm = SpyLM("dummy_model", return_json=True)
⋮----
# Verify the LM was called with correct messages
⋮----
messages = lm.calls[0]["messages"]
⋮----
def test_lm_usage()
⋮----
result = program(question="What is the capital of France?")
⋮----
def test_lm_usage_with_parallel()
⋮----
def program_wrapper(question)
⋮----
# Sleep to make it possible to cause a race condition
⋮----
parallelizer = dspy.Parallel()
input_pairs = [
results = parallelizer(input_pairs)
⋮----
@pytest.mark.asyncio
async def test_lm_usage_with_async()
⋮----
original_aforward = program.aforward
⋮----
async def patched_aforward(self, **kwargs)
⋮----
coroutines = [
results = await asyncio.gather(*coroutines)
⋮----
def test_positional_arguments()
⋮----
def test_error_message_on_invalid_lm_setup()
⋮----
# No LM is loaded.
⋮----
# LM is a string.
⋮----
def dummy_lm()
⋮----
# LM is not an instance of dspy.BaseLM.
⋮----
@pytest.mark.parametrize("adapter_type", ["chat", "json"])
def test_field_constraints(adapter_type)
⋮----
class ConstrainedSignature(dspy.Signature)
⋮----
"""Test signature with constrained fields."""
⋮----
# Input with length and value constraints
text: str = dspy.InputField(min_length=5, max_length=100, desc="Input text")
number: int = dspy.InputField(gt=0, lt=10, desc="A number between 0 and 10")
⋮----
# Output with multiple constraints
score: float = dspy.OutputField(ge=0.0, le=1.0, desc="Score between 0 and 1")
count: int = dspy.OutputField(multiple_of=2, desc="Even number count")
⋮----
program = Predict(ConstrainedSignature)
⋮----
# Call the predictor to trigger instruction generation
⋮----
# Get the system message containing the instructions
system_message = lm.calls[0]["messages"][0]["content"]
⋮----
# Verify constraints are included in the field descriptions
⋮----
@pytest.mark.asyncio
async def test_async_predict()
⋮----
result = await program.acall(question="What is the capital of France?")
⋮----
def test_predicted_outputs_piped_from_predict_to_lm_call()
⋮----
# If the signature has prediction as an input field, and the prediction is not set as the standard predicted output
# format, it should not be passed to the LM.
program = Predict("question, prediction -> judgement")
⋮----
def test_dump_state_pydantic_non_primitive_types()
⋮----
class WebsiteInfo(BaseModel)
⋮----
url: HttpUrl
description: str | None = None
created_at: datetime
⋮----
class TestSignature(dspy.Signature)
⋮----
website_info: WebsiteInfo = dspy.InputField()
⋮----
website_info = WebsiteInfo(
⋮----
serialized = serialize_object(website_info)
⋮----
json_str = orjson.dumps(serialized).decode()
reloaded = orjson.loads(json_str)
⋮----
predictor = Predict(TestSignature)
demo = {"website_info": website_info, "summary": "This is a test website."}
⋮----
state = predictor.dump_state()
json_str = orjson.dumps(state).decode()
reloaded_state = orjson.loads(json_str)
⋮----
demo_data = reloaded_state["demos"][0]
⋮----
def test_trace_size_limit()
⋮----
def test_disable_trace()
⋮----
def test_per_module_history_size_limit()
⋮----
def test_per_module_history_disabled()
⋮----
def test_input_field_default_value()
⋮----
class SignatureWithDefault(dspy.Signature)
⋮----
context: str = dspy.InputField(default="DEFAULT_CONTEXT")
⋮----
lm = SpyLM()
⋮----
predictor = Predict(SignatureWithDefault)
⋮----
user_message = lm.calls[0]["messages"][-1]["content"]
</file>

<file path="tests/predict/test_program_of_thought.py">
# This test suite requires deno to be installed. Please install deno following https://docs.deno.com/runtime/getting_started/installation/
is_deno_available = shutil.which("deno") is not None
⋮----
class BasicQA(Signature)
⋮----
question = dspy.InputField()
answer = dspy.OutputField(desc="often between 1 and 5 words")
⋮----
@pytest.mark.skipif(not is_deno_available, reason="Deno is not installed or not in PATH")
def test_pot_code_generation()
⋮----
lm = DummyLM(
⋮----
pot = ProgramOfThought(BasicQA)
res = pot(question="What is 1+1?")
⋮----
# This test ensures the old finetuned saved models still work
⋮----
@pytest.mark.skipif(not is_deno_available, reason="Deno is not installed or not in PATH")
def test_old_style_pot()
⋮----
class ExtremumFinder(Signature)
⋮----
input_list = dspy.InputField()
maximum = dspy.OutputField(desc="The maximum of the given numbers")
minimum = dspy.OutputField(desc="The minimum of the given numbers")
⋮----
@pytest.mark.skipif(not is_deno_available, reason="Deno is not installed or not in PATH")
def test_pot_support_multiple_fields()
⋮----
pot = ProgramOfThought(ExtremumFinder)
res = pot(input_list="2, 3, 5, 6")
⋮----
@pytest.mark.skipif(not is_deno_available, reason="Deno is not installed or not in PATH")
def test_pot_code_generation_with_one_error()
⋮----
@pytest.mark.skipif(not is_deno_available, reason="Deno is not installed or not in PATH")
def test_pot_code_generation_persistent_errors()
⋮----
max_iters = 3
⋮----
pot = ProgramOfThought(BasicQA, max_iters=max_iters)
⋮----
def test_pot_code_parse_error()
</file>

<file path="tests/predict/test_react.py">
@pytest.mark.extra
def test_tool_observation_preserves_custom_type()
⋮----
captured_calls = []
⋮----
class SpyChatAdapter(dspy.ChatAdapter)
⋮----
def format_user_message_content(self, signature, inputs, *args, **kwargs)
⋮----
def make_images()
⋮----
adapter = SpyChatAdapter()
lm = DummyLM(
⋮----
react = dspy.ReAct("question -> answer", tools=[make_images])
⋮----
sigs_with_obs = [sig for sig, inputs in captured_calls if "observation_0" in inputs]
⋮----
observation_content = lm.history[1]["messages"][1]["content"]
⋮----
def test_tool_calling_with_pydantic_args()
⋮----
class CalendarEvent(BaseModel)
⋮----
name: str
date: str
participants: dict[str, str]
⋮----
def write_invitation_letter(participant_name: str, event_info: CalendarEvent)
⋮----
class InvitationSignature(dspy.Signature)
⋮----
participant_name: str = dspy.InputField(desc="The name of the participant to invite")
event_info: CalendarEvent = dspy.InputField(desc="The information about the event")
invitation_letter: str = dspy.OutputField(desc="The invitation letter to be sent to the participant")
⋮----
react = dspy.ReAct(InvitationSignature, tools=[write_invitation_letter])
⋮----
outputs = react(
⋮----
expected_trajectory = {
⋮----
def test_tool_calling_without_typehint()
⋮----
def foo(a, b)
⋮----
"""Add two numbers."""
⋮----
react = dspy.ReAct("a, b -> c:int", tools=[foo])
⋮----
outputs = react(a=1, b=2)
⋮----
def test_trajectory_truncation()
⋮----
# Create a simple tool for testing
def echo(text: str) -> str
⋮----
# Create ReAct instance with our echo tool
react = dspy.ReAct("input_text -> output_text", tools=[echo])
⋮----
# Mock react.react to simulate multiple tool calls
call_count = 0
⋮----
def mock_react(**kwargs)
⋮----
# First 2 calls use the echo tool
⋮----
# The 3rd call raises context window exceeded error
⋮----
# The 4th call finishes
⋮----
# Call forward and get the result
result = react(input_text="test input")
⋮----
# Verify that older entries in the trajectory were truncated
⋮----
@pytest.mark.asyncio
async def test_context_window_exceeded_after_retries()
⋮----
# Test sync version
extract_calls = []
⋮----
def mock_extract(**kwargs)
⋮----
# Test async version
async_extract_calls = []
⋮----
async def mock_react_async(**kwargs)
⋮----
async def mock_extract_async(**kwargs)
⋮----
result = await react.acall(input_text="test input")
⋮----
def test_error_retry()
⋮----
# --- a tiny tool that always fails -------------------------------------
⋮----
# --- program under test -------------------------------------------------
⋮----
# (The model *would* succeed on the 3rd turn, but max_iters=2 stops earlier.)
⋮----
outputs = react(a=1, b=2, max_iters=2)
traj = outputs.trajectory
⋮----
# --- exact-match checks (thoughts + tool calls) -------------------------
control_expected = {
⋮----
# --- flexible checks for observations ----------------------------------
# We only care that each observation mentions our error string; we ignore
# any extra traceback detail or differing prefixes.
⋮----
obs = traj[f"observation_{i}"]
⋮----
@pytest.mark.asyncio
async def test_async_tool_calling_with_pydantic_args()
⋮----
async def write_invitation_letter(participant_name: str, event_info: CalendarEvent)
⋮----
outputs = await react.acall(
⋮----
@pytest.mark.asyncio
async def test_async_error_retry()
⋮----
# A tiny tool that always fails
async def foo(a, b)
⋮----
outputs = await react.acall(a=1, b=2, max_iters=2)
⋮----
# Exact-match checks (thoughts + tool calls)
⋮----
# Flexible checks for observations
</file>

<file path="tests/predict/test_refine.py">
class DummyModule(dspy.Module)
⋮----
def __init__(self, signature, forward_fn)
⋮----
def forward(self, **kwargs) -> Prediction
⋮----
def test_refine_forward_success_first_attempt()
⋮----
lm = DummyLM([{"answer": "Brussels"}, {"answer": "City of Brussels"}, {"answer": "Brussels"}])
⋮----
module_call_count = [0]
⋮----
def count_calls(self, **kwargs)
⋮----
reward_call_count = [0]
⋮----
def reward_fn(kwargs, pred: Prediction) -> float
⋮----
# The answer should always be one word.
⋮----
predict = DummyModule("question -> answer", count_calls)
⋮----
refine = Refine(module=predict, N=3, reward_fn=reward_fn, threshold=1.0)
result = refine(question="What is the capital of Belgium?")
⋮----
def test_refine_module_default_fail_count()
⋮----
def always_raise(self, **kwargs)
⋮----
predict = DummyModule("question -> answer", always_raise)
⋮----
refine = Refine(module=predict, N=3, reward_fn=lambda _, __: 1.0, threshold=0.0)
⋮----
def test_refine_module_custom_fail_count()
⋮----
def raise_on_second_call(self, **kwargs)
⋮----
predict = DummyModule("question -> answer", raise_on_second_call)
⋮----
refine = Refine(module=predict, N=3, reward_fn=lambda _, __: 1.0, threshold=0.0, fail_count=1)
</file>

<file path="tests/predict/test_retry.py">
# import functools
⋮----
# import pydantic
⋮----
# import dspy
# from dspy.primitives.assertions import assert_transform_module, backtrack_handler
# from dspy.utils import DummyLM
⋮----
# def test_retry_simple():
#     predict = dspy.Predict("question -> answer")
#     retry_module = dspy.Retry(predict)
⋮----
#     # Test Retry has created the correct new signature
#     for field in predict.signature.output_fields:
#         assert f"past_{field}" in retry_module.new_signature.input_fields
#     assert "feedback" in retry_module.new_signature.input_fields
⋮----
#     lm = DummyLM([{"answer": "blue"}])
#     dspy.configure(lm=lm)
#     result = retry_module.forward(
#         question="What color is the sky?",
#         past_outputs={"answer": "red"},
#         feedback="Try harder",
#     )
#     assert result.answer == "blue"
⋮----
# def test_retry_forward_with_feedback():
#     # First we make a mistake, then we fix it
#     lm = DummyLM([{"answer": "red"}, {"answer": "blue"}])
#     dspy.configure(lm=lm, trace=[])
⋮----
#     class SimpleModule(dspy.Module):
#         def __init__(self):
#             super().__init__()
#             self.predictor = dspy.Predict("question -> answer")
⋮----
#         def forward(self, **kwargs):
#             result = self.predictor(**kwargs)
#             print(f"SimpleModule got {result.answer=}")
#             dspy.Suggest(result.answer == "blue", "Please think harder")
#             return result
⋮----
#     program = SimpleModule()
#     program = assert_transform_module(
#         program.map_named_predictors(dspy.Retry),
#         functools.partial(backtrack_handler, max_backtracks=1),
⋮----
#     result = program(question="What color is the sky?")
⋮----
# # def test_retry_forward_with_typed_predictor():
# #     # First we make a mistake, then we fix it
# #     lm = DummyLM([{"output": '{"answer":"red"}'}, {"output": '{"answer":"blue"}'}])
# #     dspy.configure(lm=lm, trace=[])
⋮----
# #     class AnswerQuestion(dspy.Signature):
# #         """Answer questions with succinct responses."""
⋮----
# #         class Input(pydantic.BaseModel):
# #             question: str
⋮----
# #         class Output(pydantic.BaseModel):
# #             answer: str
⋮----
# #         input: Input = dspy.InputField()
# #         output: Output = dspy.OutputField()
⋮----
# #     class QuestionAnswerer(dspy.Module):
# #         def __init__(self):
# #             super().__init__()
# #             self.answer_question = dspy.TypedPredictor(AnswerQuestion)
⋮----
# #         def forward(self, **kwargs):
# #             result = self.answer_question(input=AnswerQuestion.Input(**kwargs)).output
# #             dspy.Suggest(result.answer == "blue", "Please think harder")
# #             return result
⋮----
# #     program = QuestionAnswerer()
# #     program = assert_transform_module(
# #         program.map_named_predictors(dspy.Retry),
# #         functools.partial(backtrack_handler, max_backtracks=1),
# #     )
⋮----
# #     result = program(question="What color is the sky?")
⋮----
# #     assert result.answer == "blue"
</file>

<file path="tests/predict/test_rlm.py">
"""
Tests for the RLM (Recursive Language Model) module.

Test organization:
- Unit tests (no Deno required): MockInterpreter, RLM formatting, signatures
- Integration tests (@pytest.mark.integration): PythonInterpreter with Deno
"""
⋮----
# ============================================================================
# Test Helpers and Factories
⋮----
def make_mock_predictor(responses: list[dict], async_mode: bool = False)
⋮----
"""Factory for mock predictors with scripted responses.

    Args:
        responses: List of dicts with keys like 'reasoning', 'code'.
        async_mode: If True, returns a predictor with acall() instead of __call__().
    """
⋮----
class MockPredictor
⋮----
def __init__(self)
⋮----
def _next_response(self)
⋮----
result = responses[self.idx % len(responses)]
⋮----
def __call__(self, **kwargs)
⋮----
async def acall(self, **kwargs)
⋮----
@contextmanager
def dummy_lm_context(responses: list[dict])
⋮----
"""Context manager for DummyLM setup."""
⋮----
lm = DummyLM(responses)
⋮----
# Common test tools
def echo_tool(text: str = "") -> str
⋮----
"""Echo the input text."""
⋮----
def add_tool(a: int = 0, b: int = 0) -> str
⋮----
"""Add two numbers."""
⋮----
def multiply_tool(a: int = 0, b: int = 0) -> str
⋮----
"""Multiply two numbers."""
⋮----
# Unit Tests: MockInterpreter
⋮----
class TestMockInterpreter
⋮----
"""Unit tests for MockInterpreter."""
⋮----
def test_scripted_responses(self)
⋮----
"""Test that MockInterpreter returns scripted responses in order."""
mock = MockInterpreter(responses=["first", "second", "third"])
⋮----
def test_returns_final_output_result(self)
⋮----
"""Test that MockInterpreter can return FinalOutput."""
mock = MockInterpreter(responses=["exploring", FinalOutput("42")])
⋮----
result = mock.execute("SUBMIT('42')")
⋮----
def test_raises_exception_from_responses(self)
⋮----
"""Test that MockInterpreter raises exceptions from responses."""
mock = MockInterpreter(responses=["ok", CodeInterpreterError("undefined variable")])
⋮----
def test_records_call_history(self)
⋮----
"""Test that MockInterpreter records call history for test assertions."""
mock = MockInterpreter(responses=["resp"])
⋮----
# Unit Tests: RLM Module (no interpreter needed)
⋮----
class TestRLMInitialization
⋮----
"""Tests for RLM module initialization."""
⋮----
def test_basic_initialization(self)
⋮----
"""Test RLM module initializes correctly with signature."""
rlm = RLM("context, query -> answer", max_iterations=5)
⋮----
assert rlm.tools == {}  # No user tools provided
⋮----
def test_custom_signature(self)
⋮----
"""Test RLM with custom signature."""
rlm = RLM("document, question -> summary, key_facts", max_iterations=5)
⋮----
def test_custom_tools(self)
⋮----
"""Test RLM with custom tools."""
def custom_tool(x: str = "") -> str
⋮----
rlm = RLM("context -> answer", max_iterations=5, tools=[custom_tool])
⋮----
assert len(rlm.tools) == 1  # Only user tools, not internal llm_query/llm_query_batched
⋮----
@pytest.mark.parametrize("tool_name", ["invalid-name", "123start"])
    def test_tool_validation_invalid_identifier(self, tool_name)
⋮----
"""Test RLM rejects tool names that aren't valid Python identifiers."""
def my_tool() -> str
⋮----
tool = Tool(my_tool, name=tool_name)
⋮----
@pytest.mark.parametrize("tool_name", ["llm_query", "SUBMIT", "print"])
    def test_tool_validation_reserved_names(self, tool_name)
⋮----
"""Test RLM rejects tool names that conflict with built-in functions."""
⋮----
@pytest.mark.parametrize("invalid_value", ["not a function", 123])
    def test_tool_validation_not_callable(self, invalid_value)
⋮----
"""Test RLM rejects tools that aren't callable."""
⋮----
def test_tools_dict_rejected(self)
⋮----
"""Test RLM rejects dict format for tools with helpful error."""
⋮----
def test_optional_parameters(self)
⋮----
"""Test RLM optional parameters and their defaults."""
⋮----
# Test defaults
rlm = RLM("context -> answer")
⋮----
# Test custom values
mock = MockInterpreter()
mock_lm = dspy.LM("openai/gpt-4o-mini")
rlm = RLM("context -> answer", max_llm_calls=100, sub_lm=mock_lm, interpreter=mock)
⋮----
def test_forward_validates_required_inputs(self)
⋮----
"""Test that forward() raises ValueError for missing required inputs."""
mock = MockInterpreter(responses=["result"])
⋮----
# Single missing input
rlm = RLM("context, query -> answer", max_iterations=3, interpreter=mock)
⋮----
rlm.forward(context="some context")  # Missing 'query'
⋮----
# Multiple missing inputs - all should be reported
rlm = RLM("a, b, c -> answer", max_iterations=3, interpreter=mock)
⋮----
rlm.forward(a="only a")  # Missing 'b' and 'c'
⋮----
def test_batched_query_errors_have_clear_markers(self)
⋮----
"""Test that errors in llm_query_batched are prefixed with [ERROR]."""
⋮----
mock_lm = MagicMock()
⋮----
rlm = RLM("context -> answer", max_llm_calls=10, sub_lm=mock_lm)
tools = rlm._make_llm_tools()
⋮----
results = tools["llm_query_batched"](prompts=["test prompt"])
⋮----
def test_tools_call_counter_is_thread_safe(self)
⋮----
"""Test that the LLM call counter is thread-safe for concurrent llm_query_batched calls.

        The call counter must be protected by a lock since llm_query_batched uses
        ThreadPoolExecutor for concurrent execution.
        """
⋮----
call_count = [0]
errors = []
⋮----
def make_call()
⋮----
futures = [executor.submit(make_call) for _ in range(10)]
⋮----
class TestRLMFormatting
⋮----
"""Tests for RLM formatting helpers."""
⋮----
def test_format_history(self)
⋮----
"""Test history formatting using REPLHistory."""
history = REPLHistory()
history = history.append(reasoning="Need to check the data", code="print(1)", output="1")
history = history.append(reasoning="Now calculate", code="x = 2", output="")
formatted = history.format()
⋮----
def test_format_history_empty(self)
⋮----
"""Test history formatting with empty history."""
⋮----
def test_action_signature_has_iteration_field(self)
⋮----
"""Test action signature includes iteration input field."""
⋮----
action_sig = rlm.generate_action.signature
⋮----
def test_format_output(self)
⋮----
"""Test output formatting."""
⋮----
formatted = rlm._format_output("output text")
⋮----
def test_format_output_empty(self)
⋮----
"""Test output formatting with empty output."""
⋮----
formatted = rlm._format_output("")
⋮----
def test_format_output_truncation(self)
⋮----
"""Test that long output is truncated."""
rlm = RLM("context -> answer", max_output_chars=100)
formatted = rlm._format_output("x" * 200)
⋮----
def test_format_variable_info_string(self)
⋮----
"""Test variable info formatting for string value using REPLVariable."""
var = REPLVariable.from_value("context", "Hello world", preview_chars=5)
formatted = var.format()
⋮----
assert "11" in formatted  # length
⋮----
assert "..." in formatted  # truncation indicator
⋮----
def test_format_variable_info_dict(self)
⋮----
"""Test variable info formatting for dict value using REPLVariable."""
var = REPLVariable.from_value("data", {"key": "value"})
⋮----
def test_build_variables_multiple(self)
⋮----
"""Test building multiple variables."""
rlm = RLM("context, query -> answer")
variables = rlm._build_variables(
⋮----
formatted = "\n\n".join(v.format() for v in variables)
⋮----
class TestREPLTypes
⋮----
"""Tests for the REPL type classes."""
⋮----
def test_repl_history_immutability(self)
⋮----
"""Test that REPLHistory.append() returns new instance."""
h1 = REPLHistory()
h2 = h1.append(code="print(1)", output="1")
assert len(h1) == 0  # Original unchanged
assert len(h2) == 1  # New has entry
⋮----
def test_repl_history_len_iter_bool(self)
⋮----
"""Test REPLHistory list-like interface."""
h = REPLHistory()
⋮----
h = h.append(code="x = 1", output="")
h = h.append(code="x = 2", output="")
⋮----
codes = [e.code for e in h]
⋮----
def test_repl_entry_format(self)
⋮----
"""Test REPLEntry formatting."""
entry = REPLEntry(reasoning="test reason", code="print(1)", output="1")
formatted = entry.format(index=0)
⋮----
def test_repl_entry_format_truncation(self)
⋮----
"""Test REPLEntry output truncation."""
entry = REPLEntry(code="print('x' * 1000)", output="x" * 1000)
formatted = entry.format(index=0, max_output_chars=50)
⋮----
def test_repl_variable_from_value(self)
⋮----
"""Test REPLVariable.from_value() factory."""
var = REPLVariable.from_value("test", "hello world")
⋮----
def test_repl_variable_truncation(self)
⋮----
"""Test REPLVariable preview truncation."""
var = REPLVariable.from_value("big", "x" * 1000, preview_chars=50)
assert len(var.preview) == 53  # 50 + "..."
⋮----
def test_repl_variable_with_field_info(self)
⋮----
"""Test REPLVariable includes desc and constraints from field_info."""
⋮----
# Create a field with description and constraints
field = dspy.InputField(desc="The user's question", ge=0, le=100)
⋮----
var = REPLVariable.from_value("query", "What is 2+2?", field_info=field)
⋮----
# Verify format includes the metadata
⋮----
def test_repl_variable_without_field_info(self)
⋮----
"""Test REPLVariable works without field_info."""
var = REPLVariable.from_value("data", [1, 2, 3])
⋮----
# Format should not include empty desc/constraints lines
⋮----
def test_build_variables_includes_field_metadata(self)
⋮----
"""Test _build_variables passes field_info to REPLVariable."""
⋮----
class QASig(dspy.Signature)
⋮----
"""Answer questions."""
context: str = dspy.InputField(desc="Background information")
question: str = dspy.InputField(desc="The question to answer")
answer: str = dspy.OutputField()
⋮----
rlm = RLM(QASig, max_iterations=3)
variables = rlm._build_variables(context="Some text", question="What?")
⋮----
# Find the context variable
context_var = next(v for v in variables if v.name == "context")
⋮----
question_var = next(v for v in variables if v.name == "question")
⋮----
class TestRLMCallMethod
⋮----
"""Tests for RLM __call__ method."""
⋮----
def test_call_is_alias_for_forward(self)
⋮----
"""Test that __call__ is an alias for forward()."""
mock = MockInterpreter(responses=[FinalOutput({"answer": "42"})])
rlm = RLM("query -> answer", max_iterations=3, interpreter=mock)
⋮----
result = rlm(query="What is the answer?")
⋮----
class TestRLMMaxIterationsFallback
⋮----
"""Tests for max_iterations reached and extract fallback."""
⋮----
def test_max_iterations_triggers_extract(self)
⋮----
"""Test that reaching max_iterations uses extract fallback."""
mock = MockInterpreter(responses=[
⋮----
# Mock the extract predictor to return a value
⋮----
result = rlm.forward(query="test")
⋮----
class TestRLMToolExceptions
⋮----
"""Tests for tool exception handling."""
⋮----
def test_tool_exception_returns_error_in_output(self)
⋮----
"""Test that tool exceptions are caught and returned as errors."""
def failing_tool() -> str
⋮----
rlm = RLM("query -> answer", max_iterations=5, interpreter=mock, tools=[failing_tool])
⋮----
class TestRLMDynamicSignature
⋮----
"""Tests for the dynamically built RLM signatures."""
⋮----
def test_action_signature_structure(self)
⋮----
"""Test action signature has required fields and instructions."""
rlm = RLM("document, question -> summary, answer")
⋮----
# Required input/output fields
⋮----
# Instructions mention key tools and variables
instructions = action_sig.instructions
⋮----
def test_extract_signature_structure(self)
⋮----
"""Test extract signature has required fields for all outputs."""
rlm = RLM("document, question -> summary, key_facts, confidence")
extract_sig = rlm.extract.signature
⋮----
# Integration Tests: PythonInterpreter (require Deno)
⋮----
@pytest.mark.integration
class TestPythonInterpreter
⋮----
"""Integration tests for the secure sandbox with tool support."""
⋮----
def test_start_prewarms_sandbox(self)
⋮----
"""Test that start() pre-warms the sandbox."""
interp = PythonInterpreter()
⋮----
# Before start, deno_process should be None
⋮----
# After start, it should be running
⋮----
assert interp.deno_process.poll() is None  # Still running
# Execute should work
result = interp.execute("print(42)")
⋮----
def test_start_is_idempotent(self)
⋮----
"""Test that start() can be called multiple times safely."""
⋮----
first_process = interp.deno_process
interp.start()  # Second call - should be idempotent
assert interp.deno_process is first_process  # Same process
⋮----
def test_basic_execution(self)
⋮----
"""Test basic code execution."""
⋮----
result = interp.execute("print(1 + 1)")
⋮----
def test_variable_injection(self)
⋮----
"""Test variable injection."""
⋮----
result = interp.execute(
⋮----
def test_variable_injection_with_none_values(self)
⋮----
"""Test variable injection with None values in dicts/lists (JSON null -> Python None)."""
⋮----
# Test None in dict
⋮----
# Test None in list
⋮----
# Test nested None
⋮----
def test_tool_call_kwargs(self)
⋮----
"""Test tool call with keyword arguments."""
def echo(message: str = "") -> str
⋮----
result = interp.execute('print(echo(message="hello"))')
⋮----
def test_tool_call_positional(self)
⋮----
"""Test tool call with positional arguments."""
def greet(name: str) -> str
⋮----
result = interp.execute('print(greet("world"))')
⋮----
def test_multiple_tools(self)
⋮----
"""Test multiple tools."""
def add(a: int = 0, b: int = 0) -> str
⋮----
def multiply(a: int = 0, b: int = 0) -> str
⋮----
result = interp.execute("""
⋮----
def test_tool_returns_list(self)
⋮----
"""Test tool that returns a list (like llm_query_batched)."""
def batch_process(items: list | None = None) -> list
⋮----
items = items or []
⋮----
def test_tool_returns_dict(self)
⋮----
"""Test tool that returns a dict."""
def get_info() -> dict
⋮----
def test_state_persists(self)
⋮----
"""Test that state persists across executions."""
⋮----
result = interp.execute("print(x + 5)")
⋮----
def test_syntax_error(self)
⋮----
"""Test syntax error handling."""
⋮----
def test_runtime_error(self)
⋮----
"""Test runtime error handling."""
⋮----
@pytest.mark.integration
class TestSandboxSecurity
⋮----
"""Integration tests for sandbox security restrictions."""
⋮----
def test_no_network_access(self)
⋮----
"""Test that network access is blocked."""
⋮----
def test_imports_work(self)
⋮----
"""Test that standard library imports work."""
⋮----
# Unit Tests: RLM with MockInterpreter (no Deno required)
⋮----
class TestRLMAsyncMock
⋮----
"""Unit tests for RLM aforward() using MockInterpreter (no Deno required)."""
⋮----
@pytest.mark.asyncio
    async def test_aforward_basic(self)
⋮----
"""Test aforward() returns Prediction with expected output (MockInterpreter)."""
⋮----
result = await rlm.aforward(query="What is the answer?")
⋮----
@pytest.mark.asyncio
    async def test_aforward_int_output_mock(self)
⋮----
"""Test aforward() returns int when signature expects int (MockInterpreter)."""
mock = MockInterpreter(responses=[FinalOutput({"count": 42})])
rlm = RLM("query -> count: int", max_iterations=3, interpreter=mock)
⋮----
result = await rlm.aforward(query="count items")
⋮----
@pytest.mark.asyncio
    async def test_aforward_multi_iteration_mock(self)
⋮----
"""Test aforward() handles multiple iterations before SUBMIT (MockInterpreter)."""
⋮----
rlm = RLM("query -> answer", max_iterations=5, interpreter=mock)
⋮----
result = await rlm.aforward(query="test")
⋮----
class TestRLMTypeCoercionMock
⋮----
"""Unit tests for RLM type coercion using MockInterpreter (no Deno required)."""
⋮----
def test_type_coercion(self, output_field, output_type, final_value, code, expected)
⋮----
"""Test RLM type coercion for various types (MockInterpreter)."""
mock = MockInterpreter(responses=[FinalOutput({output_field: final_value})])
rlm = RLM(f"query -> {output_field}: {output_type}", max_iterations=3, interpreter=mock)
⋮----
def test_type_error_retries(self)
⋮----
"""Test RLM retries when type validation fails (MockInterpreter)."""
⋮----
FinalOutput({"answer": "maybe"}),  # Invalid for Literal
FinalOutput({"answer": "yes"}),    # Valid
⋮----
rlm = RLM("query -> answer: Literal['yes', 'no']", max_iterations=5, interpreter=mock)
⋮----
result = rlm.forward(query="is it yes?")
⋮----
# Integration Tests: RLM Type Coercion with PythonInterpreter
⋮----
@pytest.mark.integration
class TestRLMTypeCoercion
⋮----
"""Tests for RLM type coercion through full forward pass with PythonInterpreter.

    Note: These tests let RLM create its own PythonInterpreter so it can register
    typed output_fields for SUBMIT based on the signature.
    """
⋮----
def test_type_coercion(self, output_field, output_type, code, expected, expected_type)
⋮----
"""Test RLM type coercion for various types with PythonInterpreter."""
rlm = RLM(f"query -> {output_field}: {output_type}", max_iterations=3)
⋮----
def test_submit_extracts_typed_value(self)
⋮----
"""Test RLM SUBMIT correctly extracts typed value."""
rlm = RLM("query -> count: int", max_iterations=3)
⋮----
result = rlm.forward(query="count items")
⋮----
# Integration Tests: RLM Multiple Output Fields
⋮----
@pytest.mark.integration
class TestRLMMultipleOutputs
⋮----
"""Tests for signatures with multiple typed output fields.

    Tests SUBMIT() calling patterns with multi-output signatures.
    """
⋮----
def test_multi_output_final_kwargs(self)
⋮----
"""SUBMIT(field1=val1, field2=val2) with keyword args."""
rlm = RLM("query -> name: str, count: int", max_iterations=3)
⋮----
def test_multi_output_final_positional(self)
⋮----
"""SUBMIT(val1, val2) with positional args mapped to field order."""
⋮----
def test_multi_output_three_fields(self)
⋮----
"""Signature with 3+ output fields of different types."""
rlm = RLM("query -> name: str, age: int, active: bool", max_iterations=3)
⋮----
def test_multi_output_final_missing_field_errors(self)
⋮----
"""SUBMIT() with missing field should return error in output."""
⋮----
# RLM should retry after getting error for missing field
⋮----
def test_multi_output_submit_vars(self)
⋮----
"""SUBMIT can pass variables directly for multiple outputs."""
⋮----
def test_multi_output_type_coercion(self)
⋮----
"""Each output field is coerced to its declared type."""
rlm = RLM("query -> count: int, ratio: float, flag: bool", max_iterations=3)
⋮----
# Integration Tests: RLM with DummyLM and PythonInterpreter
⋮----
@pytest.mark.integration
class TestRLMWithDummyLM
⋮----
"""End-to-end tests using DummyLM with RLM and PythonInterpreter.

    Note: These tests let RLM create its own PythonInterpreter so it can register
    typed output_fields for SUBMIT based on the signature.
    """
⋮----
def test_simple_computation_e2e(self)
⋮----
"""Test full RLM pipeline: DummyLM -> RLM -> PythonInterpreter -> result."""
⋮----
rlm = RLM("query -> answer: int", max_iterations=3)
result = rlm.forward(query="What is 2 + 3?")
⋮----
def test_multi_turn_computation_e2e(self)
⋮----
"""Test RLM with multiple turns before SUBMIT."""
⋮----
rlm = RLM("query -> answer: int", max_iterations=5)
result = rlm.forward(query="Double ten")
⋮----
def test_with_input_variables_e2e(self)
⋮----
"""Test RLM with input variables passed to sandbox."""
⋮----
rlm = RLM("numbers: list[int] -> total: int", max_iterations=3)
result = rlm.forward(numbers=[1, 2, 3, 4, 5])
⋮----
def test_with_tool_e2e(self)
⋮----
"""Test RLM calling a host-side tool through the sandbox."""
def lookup(key: str) -> str
⋮----
rlm = RLM("fruit -> color: str", max_iterations=3, tools=[lookup])
result = rlm.forward(fruit="apple")
⋮----
@pytest.mark.asyncio
    async def test_aforward_simple_computation_e2e(self)
⋮----
"""Test aforward() full pipeline: DummyLM -> RLM -> PythonInterpreter -> result."""
⋮----
result = await rlm.aforward(query="What is 2 + 3?")
⋮----
@pytest.mark.asyncio
    async def test_aforward_multi_turn_e2e(self)
⋮----
"""Test aforward() with multiple turns before SUBMIT."""
⋮----
result = await rlm.aforward(query="Double ten")
⋮----
@pytest.mark.asyncio
    async def test_aforward_with_input_variables_e2e(self)
⋮----
"""Test aforward() with input variables passed to sandbox."""
⋮----
result = await rlm.aforward(numbers=[1, 2, 3, 4, 5])
⋮----
# Integration Tests: RLM with real LM (require API key and Deno)
⋮----
@pytest.mark.skip(reason="Requires actual LM and Deno - run manually")
class TestRLMIntegration
⋮----
"""Integration tests that require a configured LM."""
⋮----
def test_simple_computation(self)
⋮----
"""Test RLM on simple computation."""
⋮----
result = rlm(
⋮----
def test_with_llm_query(self)
⋮----
"""Test RLM using the llm_query tool."""
</file>

<file path="tests/primitives/resources/saved_program.json">
{
    "react": {
      "traces": [],
      "train": [],
      "demos": [
        {
          "augmented": true,
          "question": "That Darn Cat! and Never a Dull Moment were both produced by what studio?",
          "trajectory": "[[ ## thought_0 ## ]]\nI need to find out which studio produced both \"That Darn Cat!\" and \"Never a Dull Moment.\" This information is likely available on Wikipedia, so I will search for it there.\n\n[[ ## tool_name_0 ## ]]\nsearch_wikipedia\n\n[[ ## tool_args_0 ## ]]\n{\"query\": \"That Darn Cat! and Never a Dull Moment studio production\"}\n\n[[ ## observation_0 ## ]]\n[1] «That Darn Cat! | That Darn Cat! is a 1965 American Walt Disney Productions thriller comedy film starring Hayley Mills (in her last of the six films she made for the Walt Disney Studios) and Dean Jones (starring in his first film for Disney) in a story about bank robbers, a kidnapping and a mischievous cat. The film was based on the 1963 novel \"Undercover Cat\" by Gordon and Mildred Gordon and was directed by Robert Stevenson. The title song was written by the Sherman Brothers and sung by Bobby Darin. The 1997 remake includes a cameo appearance by Dean Jones.»\n[2] «Never a Dull Moment (1968 film) | Never a Dull Moment is a 1968 American comedy crime film from Walt Disney Productions starring Dick Van Dyke and Edward G. Robinson and directed by Jerry Paris. The script by AJ Carothers was based on a novel by John Godey. The supporting cast features Dorothy Provine, Henry Silva, Slim Pickens and Jack Elam.»\n[3] «That Darn Cat (1997 film) | That Darn Cat is a 1997 American mystery comedy film starring Christina Ricci and Doug E. Doug. It is a remake of the 1965 film \"That Darn Cat! \", which in turn was based on the book \"Undercover Cat\" by Gordon and Mildred Gordon. It is directed by British TV veteran Bob Spiers (most famous for \"Fawlty Towers\", as well as \"Spice World\") and written by Scott Alexander and Larry Karaszewski, best known for \"Ed Wood\" and the first two \"Problem Child\" films.»",
          "next_thought": "Both \"That Darn Cat!\" and \"Never a Dull Moment\" were produced by Walt Disney Productions. I have the information I need to answer the question.",
          "next_tool_name": "finish",
          "next_tool_args": {}
        },
        {
          "question": "Was Yakov Protazanov or Marcel Duchamp born in 1881",
          "answer": "Yakov Alexandrovich Protazanov (Russian: Я́ков Алекса́ндрович Протаза́нов ; January 23 (O.S. February 4), 1881",
          "dspy_uuid": "15118a69-274d-4eea-b3eb-204d05e24012",
          "dspy_split": "train"
        }
      ],
      "signature": {
        "instructions": "Imagine you are a detective racing against time to solve a high-profile case involving two famous films: \"That Darn Cat!\" and \"Never a Dull Moment.\" Your mission is to uncover which studio produced these films before the press conference starts in one hour. You have access to a powerful tool: a Wikipedia search. \n\nGiven the fields `question`, produce the fields `answer`.\n\nYou are an Agent. In each episode, you will be given the fields `question` as input. And you can see your past trajectory so far. Your goal is to use one or more of the supplied tools to collect any necessary information for producing `answer`.\n\nTo do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task. After each tool call, you receive a resulting observation, which gets appended to your trajectory.\n\nWhen writing next_thought, you may reason about the current situation and plan for future steps. When selecting the next_tool_name and its next_tool_args, the tool must be one of:\n\n(1) search_wikipedia. It takes arguments {'query': {'type': 'string'}}.\n(2) finish, whose description is <desc>Marks the task as complete. That is, signals that all information for producing the outputs, i.e. `answer`, are now available to be extracted.<\/desc>. It takes arguments {}.\nWhen providing `next_tool_args`, the value inside the field must be in JSON format.",
        "fields": [
          {
            "prefix": "Question:",
            "description": "${question}"
          },
          {
            "prefix": "Trajectory:",
            "description": "${trajectory}"
          },
          {
            "prefix": "Next Thought:",
            "description": "${next_thought}"
          },
          {
            "prefix": "Next Tool Name:",
            "description": "${next_tool_name}"
          },
          {
            "prefix": "Next Tool Args:",
            "description": "${next_tool_args}"
          }
        ]
      },
      "lm": null
    },
    "extract.predict": {
      "traces": [],
      "train": [],
      "demos": [
        {
          "augmented": true,
          "question": "That Darn Cat! and Never a Dull Moment were both produced by what studio?",
          "trajectory": "[[ ## thought_0 ## ]]\nI need to find out which studio produced both \"That Darn Cat!\" and \"Never a Dull Moment.\" This information is likely available on Wikipedia, so I will search for it there.\n\n[[ ## tool_name_0 ## ]]\nsearch_wikipedia\n\n[[ ## tool_args_0 ## ]]\n{\"query\": \"That Darn Cat! and Never a Dull Moment studio production\"}\n\n[[ ## observation_0 ## ]]\n[1] «That Darn Cat! | That Darn Cat! is a 1965 American Walt Disney Productions thriller comedy film starring Hayley Mills (in her last of the six films she made for the Walt Disney Studios) and Dean Jones (starring in his first film for Disney) in a story about bank robbers, a kidnapping and a mischievous cat. The film was based on the 1963 novel \"Undercover Cat\" by Gordon and Mildred Gordon and was directed by Robert Stevenson. The title song was written by the Sherman Brothers and sung by Bobby Darin. The 1997 remake includes a cameo appearance by Dean Jones.»\n[2] «Never a Dull Moment (1968 film) | Never a Dull Moment is a 1968 American comedy crime film from Walt Disney Productions starring Dick Van Dyke and Edward G. Robinson and directed by Jerry Paris. The script by AJ Carothers was based on a novel by John Godey. The supporting cast features Dorothy Provine, Henry Silva, Slim Pickens and Jack Elam.»\n[3] «That Darn Cat (1997 film) | That Darn Cat is a 1997 American mystery comedy film starring Christina Ricci and Doug E. Doug. It is a remake of the 1965 film \"That Darn Cat! \", which in turn was based on the book \"Undercover Cat\" by Gordon and Mildred Gordon. It is directed by British TV veteran Bob Spiers (most famous for \"Fawlty Towers\", as well as \"Spice World\") and written by Scott Alexander and Larry Karaszewski, best known for \"Ed Wood\" and the first two \"Problem Child\" films.»\n\n[[ ## thought_1 ## ]]\nBoth \"That Darn Cat!\" and \"Never a Dull Moment\" were produced by Walt Disney Productions. I have the information I need to answer the question.\n\n[[ ## tool_name_1 ## ]]\nfinish\n\n[[ ## tool_args_1 ## ]]\n{}\n\n[[ ## observation_1 ## ]]\nCompleted.",
          "reasoning": "Both \"That Darn Cat!\" and \"Never a Dull Moment\" were produced by Walt Disney Productions, as confirmed by the information retrieved from Wikipedia.",
          "answer": "Walt Disney Productions"
        },
        {
          "question": "Are Smyrnium and Nymania both types of plant?",
          "answer": "yes",
          "dspy_uuid": "b57b5933-95c7-472a-801b-3cc9bc0a3b99",
          "dspy_split": "train"
        }
      ],
      "signature": {
        "instructions": "Given the very verbose fields `question`, produce the fields `answer`.",
        "fields": [
          {
            "prefix": "Question:",
            "description": "${question}"
          },
          {
            "prefix": "Trajectory:",
            "description": "${trajectory}"
          },
          {
            "prefix": "Reasoning: Let's think step by step in order to",
            "description": "${reasoning}"
          },
          {
            "prefix": "Answer:",
            "description": "${answer}"
          }
        ]
      },
      "lm": null
    },
    "metadata": {
      "dependency_versions": {
        "python": "3.13",
        "dspy": "3.0.0",
        "cloudpickle": "3.1"
      }
    }
  }
</file>

<file path="tests/primitives/test_base_module.py">
def test_deepcopy_basic()
⋮----
signature = dspy.Signature("q -> a")
cot = dspy.ChainOfThought(signature)
cot_copy = cot.deepcopy()
⋮----
# Parameters should be different objects with the same values.
⋮----
def test_deepcopy_with_uncopyable_modules()
⋮----
class CustomClass(dspy.Module)
⋮----
def __init__(self)
⋮----
self.lock = threading.Lock()  # Non-copyable object.
⋮----
model = CustomClass()
model_copy = model.deepcopy()
⋮----
# The lock should be refer to the same object (shallow copy).
⋮----
def test_deepcopy_with_nested_modules()
⋮----
class CustomClass1(dspy.Module)
⋮----
class CustomClass2(dspy.Module)
⋮----
model = CustomClass2()
⋮----
def test_save_and_load_with_json(tmp_path)
⋮----
model = dspy.ChainOfThought(dspy.Signature("q -> a"))
⋮----
# Nested example
⋮----
save_path = tmp_path / "model.json"
⋮----
new_model = dspy.ChainOfThought(dspy.Signature("q -> a"))
⋮----
@pytest.mark.extra
def test_save_and_load_with_pkl(tmp_path)
⋮----
# `datetime.date` is not json serializable, so we need to save with pickle.
class MySignature(dspy.Signature)
⋮----
"""Just a custom signature."""
⋮----
current_date: datetime.date = dspy.InputField()
target_date: datetime.date = dspy.InputField()
date_diff: int = dspy.OutputField(desc="The difference in days between the current_date and the target_date")
⋮----
trainset = [
trainset = [dspy.Example(**example).with_inputs("current_date", "target_date") for example in trainset]
⋮----
cot = dspy.ChainOfThought(MySignature)
⋮----
def dummy_metric(example, pred, trace=None)
⋮----
optimizer = dspy.BootstrapFewShot(max_bootstrapped_demos=4, max_labeled_demos=4, max_rounds=5, metric=dummy_metric)
compiled_cot = optimizer.compile(cot, trainset=trainset)
⋮----
save_path = tmp_path / "program.pkl"
⋮----
new_cot = dspy.ChainOfThought(MySignature)
⋮----
def test_save_with_extra_modules(tmp_path)
⋮----
# Create a temporary Python file with our custom module
custom_module_path = tmp_path / "custom_module.py"
⋮----
# Add the tmp_path to Python path so we can import the module
⋮----
cot = custom_module.MyModule()
⋮----
# Remove the custom module from sys.modules to simulate it not being available
⋮----
# Also remove it from sys.path
⋮----
# Test the loading fails without using `modules_to_serialize`
⋮----
loaded_module = dspy.load(tmp_path, allow_pickle=True)
⋮----
# Only need to clean up sys.path
⋮----
def test_load_with_version_mismatch(tmp_path)
⋮----
# Mock versions during save
save_versions = {"python": "3.9", "dspy": "2.4.0", "cloudpickle": "2.0"}
⋮----
# Mock versions during load
load_versions = {"python": "3.10", "dspy": "2.5.0", "cloudpickle": "2.1"}
⋮----
predict = dspy.Predict("question->answer")
⋮----
# Create a custom handler to capture log messages
class ListHandler(logging.Handler)
⋮----
def emit(self, record)
⋮----
# Add handler and set level
handler = ListHandler()
original_level = logger.level
⋮----
# Mock version during save
⋮----
# Mock version during load
⋮----
loaded_predict = dspy.Predict("question->answer")
⋮----
# Assert warnings were logged: 1 for pickle loading + 3 for version mismatches
⋮----
# First message is about pickle loading
⋮----
# Rest are version mismatch warnings
⋮----
# Verify the model still loads correctly despite version mismatches
⋮----
# Clean up: restore original level and remove handler
⋮----
@pytest.mark.llm_call
def test_single_module_call_with_usage_tracker(lm_for_test)
⋮----
predict = dspy.ChainOfThought("question -> answer")
output = predict(question="What is the capital of France?")
⋮----
lm_usage = output.get_lm_usage()
⋮----
# Test no usage being tracked when cache is enabled
⋮----
@pytest.mark.llm_call
def test_multi_module_call_with_usage_tracker(lm_for_test)
⋮----
class MyProgram(dspy.Module)
⋮----
def __call__(self, question: str) -> Prediction
⋮----
answer = self.predict1(question=question)
score = self.predict2(question=question, answer=answer)
⋮----
program = MyProgram()
output = program(question="What is the capital of France?")
⋮----
# TODO: prepare second model for testing this unit test in ci
⋮----
@pytest.mark.skipif(not os.getenv("OPENAI_API_KEY"), reason="Skip the test if OPENAI_API_KEY is not set.")
def test_usage_tracker_in_parallel()
⋮----
def __init__(self, lm)
⋮----
program1 = MyProgram(lm=dspy.LM("openai/gpt-4o-mini", cache=False))
program2 = MyProgram(lm=dspy.LM("openai/gpt-3.5-turbo", cache=False))
⋮----
parallelizer = dspy.Parallel()
⋮----
results = parallelizer(
⋮----
@pytest.mark.asyncio
async def test_usage_tracker_async_parallel()
⋮----
program = dspy.Predict("question -> answer")
⋮----
coroutines = [
⋮----
results = await asyncio.gather(*coroutines)
⋮----
lm_usage0 = results[0].get_lm_usage()["openai/gpt-4o-mini"]
lm_usage1 = results[1].get_lm_usage()["openai/gpt-4o-mini"]
⋮----
def test_usage_tracker_no_side_effect()
⋮----
def forward(self, question: str, **kwargs) -> str
⋮----
result = program(question="What is the capital of France?")
⋮----
def test_module_history()
⋮----
def __init__(self, **kwargs)
⋮----
def forward(self, question: str, **kwargs) -> Prediction
⋮----
# Second call only call the submodule.
⋮----
# The LM history entity exists in all the ancestor callers.
⋮----
# The same history entity is shared across all the ancestor callers to reduce memory usage.
⋮----
# No history is recorded when history is disabled.
⋮----
# History is recorded again when history is enabled.
⋮----
def test_module_history_with_concurrency()
⋮----
@pytest.mark.asyncio
async def test_module_history_async()
⋮----
async def aforward(self, question: str, **kwargs) -> Prediction
⋮----
def test_forward_direct_call_warning(capsys)
⋮----
class TestModule(dspy.Module)
⋮----
def forward(self, x)
⋮----
module = TestModule()
⋮----
captured = capsys.readouterr()
⋮----
def test_forward_through_call_no_warning(capsys)
</file>

<file path="tests/primitives/test_example.py">
def test_example_initialization()
⋮----
example = Example(a=1, b=2)
⋮----
def test_example_initialization_from_base()
⋮----
base = Example(a=1, b=2)
example = Example(base=base, c=3)
⋮----
def test_example_initialization_from_dict()
⋮----
base_dict = {"a": 1, "b": 2}
example = Example(base=base_dict, c=3)
⋮----
def test_example_set_get_item()
⋮----
example = Example()
⋮----
def test_example_attribute_access()
⋮----
example = Example(a=1)
⋮----
def test_example_deletion()
⋮----
_ = example.a
⋮----
def test_example_len()
⋮----
example = Example(a=1, b=2, dspy_hidden=3)
⋮----
def test_example_repr_str_img()
⋮----
example = Example(
⋮----
def test_example_repr_str()
⋮----
def test_example_eq()
⋮----
example1 = Example(a=1, b=2)
example2 = Example(a=1, b=2)
⋮----
def test_example_hash()
⋮----
def test_example_keys_values_items()
⋮----
def test_example_get()
⋮----
def test_example_with_inputs()
⋮----
example = Example(a=1, b=2).with_inputs("a")
⋮----
def test_example_inputs_labels()
⋮----
inputs = example.inputs()
⋮----
labels = example.labels()
⋮----
def test_example_copy_without()
⋮----
copied = example.copy(c=3)
⋮----
without_a = copied.without("a")
⋮----
_ = without_a.a
⋮----
def test_example_to_dict()
⋮----
def test_example_to_dict_with_history()
⋮----
"""Test that Example.toDict() properly serializes dspy.History objects."""
history = dspy.History(
example = Example(question="Test question", history=history, answer="Test answer")
⋮----
result = example.toDict()
⋮----
# Verify the result is a dictionary
⋮----
# Verify history is serialized to a dict (not a History object)
⋮----
# Verify JSON serialization works
⋮----
json_str = json.dumps(result)
restored = json.loads(json_str)
</file>

<file path="tests/primitives/test_module.py">
from dspy.primitives.module import Module, set_attribute_by_name  # Adjust the import based on your file structure
⋮----
class HopModule(dspy.Module)
⋮----
def __init__(self)
⋮----
def forward(self, question)
⋮----
query = self.predict1(question=question).query
⋮----
def test_module_initialization()
⋮----
module = Module()
⋮----
def test_named_predictors()
⋮----
module = HopModule()
named_preds = module.named_predictors()
⋮----
def test_predictors()
⋮----
preds = module.predictors()
⋮----
def test_forward()
⋮----
program = HopModule()
⋮----
result = program(question="What is 1+1?").answer
⋮----
def test_nested_named_predictors()
⋮----
class Hop2Module(dspy.Module)
⋮----
module = Hop2Module()
⋮----
def test_empty_module()
⋮----
def test_single_level()
⋮----
expected = [("self", module), ("self.sub", module.sub)]
⋮----
def test_multiple_levels()
⋮----
expected = [("self", module), ("self.sub", module.sub), ("self.sub.subsub", module.sub.subsub)]
⋮----
def test_multiple_sub_modules()
⋮----
expected = [("self", module), ("self.sub1", module.sub1), ("self.sub2", module.sub2)]
⋮----
def test_non_base_module_attributes()
⋮----
def test_complex_module_traversal()
⋮----
root = Module()
⋮----
expected_names = {
found_names = {name for name, _ in root.named_sub_modules()}
⋮----
def test_complex_module_traversal_with_same_module()
⋮----
same_module = Module()
⋮----
"self.sub_module.nested_list[1][key]",  # NOTE: named_sub_modules allows recursive structures
⋮----
"self.sub_module.nested_tuple[1][0]",  # NEW: named_sub_modules allows recursive structures, but named_parameters does not
⋮----
def test_complex_module_set_attribute_by_name()
⋮----
class DuplicateModule(Module)
⋮----
def test_named_parameters_duplicate_references()
⋮----
module = DuplicateModule()
# Only testing for whether exceptions are thrown or not
# As Module.named_parameters() is recursive, this is mainly for catching infinite recursion
⋮----
def test_load_dspy_program_cross_version()
⋮----
"""
    Test backward compatibility for loading a saved DSPy program.

    This test verifies that DSPy can load a program saved in version 3.0.1, ensuring compatibility with older versions.
    The saved state is located in 'test/primitives/resources/saved_program.json' and represents an optimized
    `dspy.ReAct` program.
    """
path = Path(__file__).parent / "resources" / "saved_program.json"
loaded_react = dspy.ReAct("question->answer", tools=[])
</file>

<file path="tests/primitives/test_python_interpreter.py">
# This test suite requires deno to be installed. Please install deno following https://docs.deno.com/runtime/getting_started/installation/
⋮----
def test_execute_simple_code()
⋮----
code = "print('Hello, World!')"
result = interpreter.execute(code)
⋮----
def test_import()
⋮----
code = "import math\nresult = math.sqrt(4)\nresult"
⋮----
def test_user_variable_definitions()
⋮----
code = "result = number + 1\nresult"
result = interpreter.execute(code, variables={"number": 4})
⋮----
def test_rejects_python_keywords_as_variable_names()
⋮----
"""Test that Python keywords are rejected as variable names."""
⋮----
# These are valid Python identifiers but reserved keywords
# Using them as variable names would cause syntax errors
keywords_to_test = ["for", "class", "import", "def", "return", "if", "while"]
⋮----
def test_failure_syntax_error()
⋮----
code = "+++"
⋮----
def test_failure_zero_division()
⋮----
code = "1+0/0"
⋮----
def test_exception_args()
⋮----
token = random.randint(1, 10**9)
code = f"raise ValueError({token})"
⋮----
def test_submit_with_list()
⋮----
"""Test SUBMIT() with a list argument returns FinalOutput with dict format."""
⋮----
code = f"SUBMIT(['The result is', {token}])"
result = interpreter(code)
⋮----
# SUBMIT now always returns a dict with "output" key for single-output default
⋮----
def test_enable_env_vars_flag()
⋮----
code = "import os\nresult = os.getenv('FOO_TEST_ENV')\nresult"
⋮----
def test_read_file_access_control(tmp_path)
⋮----
testfile_path = tmp_path / "test_temp_file.txt"
virtual_path = f"/sandbox/{testfile_path.name}"
⋮----
code = f"with open({virtual_path!r}, 'r') as f:\n    data = f.read()\ndata"
⋮----
code = (
⋮----
def test_enable_write_flag(tmp_path)
⋮----
testfile_path = tmp_path / "test_temp_output.txt"
⋮----
code = f"with open({virtual_path!r}, 'w') as f:\n    f.write('allowed')\n'ok'"
⋮----
code = f"with open({virtual_path!r}, 'w') as f:\n    f.write('should_not_sync')\n'done_no_sync'"
⋮----
def test_enable_net_flag()
⋮----
test_url = "https://example.com"
⋮----
code = f"import js\nresp = await js.fetch({test_url!r})\nresp.status"
⋮----
def test_interpreter_security_filesystem_access(tmp_path)
⋮----
"""
    Verify that the interpreter cannot read arbitrary files from the host system
    unless explicitly allowed.
    """
# 1. Create a "secret" file on the host
secret_file = tmp_path / "secret.txt"
secret_content = "This is a secret content"
⋮----
secret_path_str = str(secret_file.absolute())
⋮----
# 2. Attempt to read the file WITHOUT permission
malicious_code = f"""
⋮----
output = interpreter(malicious_code)
⋮----
# 3. Attempt to read the file WITH permission
⋮----
def test_tools_dict_is_copied()
⋮----
"""Test that tools dict is defensively copied, not stored by reference."""
tools = {"my_tool": lambda: "result"}
sandbox = PythonInterpreter(tools=tools)
⋮----
# Modify the original dict after construction
⋮----
# The sandbox should not see the new tool
⋮----
def test_serialize_tuple()
⋮----
"""Test that tuples can be serialized as variables."""
⋮----
result = interpreter.execute("x", variables={"x": (1, 2, 3)})
assert result == [1, 2, 3]  # Tuples become lists in JSON
⋮----
def test_serialize_set()
⋮----
"""Test that sets can be serialized as variables."""
⋮----
result = interpreter.execute("sorted(x)", variables={"x": {3, 1, 2}})
⋮----
def test_serialize_set_mixed_types()
⋮----
"""Test that sets with mixed types can be serialized (fallback to list)."""
⋮----
# Mixed types can't be sorted, so they serialize as a list in arbitrary order
# We verify the list contains the expected elements
result = interpreter.execute("x", variables={"x": {1, "a"}})
⋮----
def test_deno_command_dict_raises_type_error()
⋮----
"""Test that passing a dict as deno_command raises TypeError."""
⋮----
# =============================================================================
# Typed Tool Signature Tests
⋮----
def test_tool_with_typed_signature()
⋮----
"""Test that tools get proper typed signatures from inspect."""
⋮----
def my_tool(query: str, limit: int = 10) -> str
⋮----
# Tool should be callable with typed signature
result = sandbox.execute('my_tool(query="test", limit=5)')
⋮----
def test_tool_positional_args()
⋮----
"""Test that tools work with positional arguments."""
⋮----
def search(query: str, limit: int = 10) -> str
⋮----
result = sandbox.execute('search("hello")')
⋮----
def test_tool_keyword_args()
⋮----
"""Test that tools work with keyword arguments."""
⋮----
result = sandbox.execute('search(query="hello", limit=5)')
⋮----
def test_tool_default_args()
⋮----
"""Test that tool default arguments work correctly."""
⋮----
def greet(name: str, greeting: str = "Hello") -> str
⋮----
# Without default
result = sandbox.execute('greet("World")')
⋮----
# Overriding default
result = sandbox.execute('greet("World", "Hi")')
⋮----
# Multi-Output SUBMIT Tests
⋮----
def test_submit_with_typed_signature()
⋮----
"""Test SUBMIT with typed output signature."""
⋮----
output_fields = [
⋮----
result = sandbox.execute('SUBMIT(answer="the answer", confidence=0.95)')
⋮----
def test_submit_positional_args()
⋮----
"""Test SUBMIT with positional arguments."""
⋮----
result = sandbox.execute('SUBMIT("the answer", 0.95)')
⋮----
def test_submit_multi_output()
⋮----
"""Test SUBMIT with multiple output fields using positional args."""
⋮----
# Positional args: values mapped to output fields in order
code = """
result = sandbox.execute(code)
⋮----
def test_submit_wrong_arg_count()
⋮----
"""Test SUBMIT with wrong number of args gives clear error."""
⋮----
sandbox.execute("x = 1; SUBMIT(x)")  # Only 1 arg, expects 2
⋮----
def test_extract_parameters()
⋮----
"""Test that _extract_parameters correctly extracts function signatures."""
⋮----
def example_fn(required: str, optional: int = 5, untyped=None) -> str
⋮----
sandbox = PythonInterpreter()
params = sandbox._extract_parameters(example_fn)
⋮----
def test_extract_parameters_complex_types()
⋮----
"""Test that _extract_parameters handles complex types gracefully."""
⋮----
def complex_fn(items: list | None = None, data: dict[str, int] | None = None) -> list
⋮----
params = sandbox._extract_parameters(complex_fn)
⋮----
# Complex types like Union are not included in type annotation
⋮----
# Large Variable Injection Tests
⋮----
def test_large_variable_injection()
⋮----
"""Test that large strings are injected via filesystem to avoid Pyodide's FFI size limit."""
⋮----
# Create a string just over the threshold
large_data = "x" * (LARGE_VAR_THRESHOLD + 1024)
⋮----
result = interpreter.execute("len(data)", variables={"data": large_data})
⋮----
def test_large_variable_content_integrity()
⋮----
"""Test that large variable content is preserved exactly through filesystem injection."""
⋮----
# Create a string with recognizable pattern just over threshold
pattern = "ABCDEFGHIJ" * 100
large_data = pattern * ((LARGE_VAR_THRESHOLD // len(pattern)) + 1)
⋮----
# Check first and last parts to verify content integrity
⋮----
result = interpreter.execute(code, variables={"data": large_data})
⋮----
def test_mixed_small_and_large_variables()
⋮----
"""Test that small and large variables can be used together."""
⋮----
small_var = "hello"
large_var = "x" * (LARGE_VAR_THRESHOLD + 1024)
⋮----
code = "f'{small} has {len(small)} chars, large has {len(large)} chars'"
result = interpreter.execute(code, variables={"small": small_var, "large": large_var})
expected = f"{small_var} has {len(small_var)} chars, large has {len(large_var)} chars"
⋮----
def test_multiple_large_variables()
⋮----
"""Test that multiple large variables can be injected."""
⋮----
large_a = "a" * (LARGE_VAR_THRESHOLD + 100)
large_b = "b" * (LARGE_VAR_THRESHOLD + 200)
⋮----
code = "(len(var_a), len(var_b), var_a[0], var_b[0])"
result = interpreter.execute(code, variables={"var_a": large_a, "var_b": large_b})
⋮----
def test_large_list_variable()
⋮----
"""Test that large list variables are injected via filesystem and JSON parsed."""
⋮----
# Each element "x" serializes to ~3 chars, so divide threshold by 3
num_elements = LARGE_VAR_THRESHOLD // 3
large_list = ["x"] * num_elements
⋮----
code = "(len(data), data[0], data[-1], type(data).__name__)"
result = interpreter.execute(code, variables={"data": large_list})
⋮----
def test_nested_sets_and_tuples()
⋮----
"""Test that nested structures with sets and tuples are converted to JSON-compatible types."""
complex_data = {"tags": {1, 2, 3}, "coords": (10, 20), "nested": [{"inner_set": {"a", "b"}}]}
⋮----
result = interpreter.execute("data", variables={"data": complex_data})
# Sets become sorted lists, tuples become lists
⋮----
def test_small_variable_not_using_filesystem()
⋮----
"""Test that small variables are embedded in code, not using filesystem."""
small_var = "small string"
⋮----
interpreter = PythonInterpreter()
interpreter._pending_large_vars = {}  # Initialize
⋮----
def test_large_variable_threshold_boundary()
⋮----
"""Test behavior at exactly the threshold boundary.

    The threshold applies to the serialized size, not the original value.
    For strings, serialization adds 2 bytes (quotes).
    """
⋮----
# Serialized size at threshold - should use embedded (not filesystem)
# Account for 2 bytes of quotes added by repr()
at_threshold = "x" * (LARGE_VAR_THRESHOLD - 2)
⋮----
# Serialized size over threshold - should use filesystem
over_threshold = "x" * (LARGE_VAR_THRESHOLD - 1)
⋮----
def test_enable_read_paths_multiple_files(tmp_path)
⋮----
"""Test that enable_read_paths works with multiple files in the same directory.

    Regression test for bug where mounting multiple files to /sandbox/ failed
    because Pyodide's ErrnoError has errno but no message property, causing
    the 'directory exists' check to fail on the second file.
    """
file1 = tmp_path / "test1.txt"
file2 = tmp_path / "test2.txt"
file3 = tmp_path / "test3.txt"
</file>

<file path="tests/propose/test_grounded_proposer.py">
def test_propose_instructions_for_program(demo_candidates)
⋮----
# Set large number here so that lm always returns the same response
prompt_model = DummyLM([{"proposed_instruction": "instruction"}] * 10)
program = Predict("question -> answer")
trainset = []
⋮----
proposer = GroundedProposer(prompt_model=prompt_model, program=program, trainset=trainset, verbose=False)
result = proposer.propose_instructions_for_program(
⋮----
def test_propose_instruction_for_predictor(demo_candidates)
⋮----
class TrackingDummyLM(DummyLM)
⋮----
def copy(self, **kwargs)
⋮----
prompt_model = TrackingDummyLM([{"proposed_instruction": "instruction"}] * 10)
⋮----
proposer = GroundedProposer(
result = proposer.propose_instruction_for_predictor(
</file>

<file path="tests/reliability/complex_types/generated/test_many_types_1/inputs/input1.json">
{
  "assertions": [
    "The 'processedTupleField' should be a tuple containing a string and a number.",
    "The 'processedEnumField' should be one of the allowed enum values: 'option1', 'option2', or 'option3'.",
    "The 'processedDatetimeField' should be a date-time",
    "The 'processedLiteralField' should be exactly 'literalValue'.",
    "The 'processedObjectField' should contain 'subField1' (string), 'subField2' (number), and an additional boolean field 'additionalField'.",
    "The 'processedNestedObjectField' should contain 'tupleField' as a tuple with a string and float, 'enumField' (one of the allowed enum values), 'datetimeField' (string formatted as date-time), 'literalField' (exactly 'literalValue'), and an additional boolean field 'additionalField'."
  ],
  "input": {
    "datetimeField": "2023-10-12T07:20:50.52Z",
    "enumField": "option1",
    "literalField": "literalValue",
    "nestedObjectField": {
      "datetimeField": "2023-10-12T07:20:50.52Z",
      "enumField": "option2",
      "literalField": "literalValue",
      "tupleField": ["nestedString", 789]
    },
    "objectField": {
      "subField1": "example",
      "subField2": 456
    },
    "tupleField": ["string1", 123]
  }
}
</file>

<file path="tests/reliability/complex_types/generated/test_many_types_1/inputs/input2.json">
{
  "assertions": [
    "The 'processedTupleField' should be an tuple with exactly two elements: the first element being a string and the second element being a number.",
    "The 'processedEnumField' should be one of the predefined options: 'option1', 'option2', or 'option3'.",
    "The 'processedDatetimeField' should be a date-time",
    "The 'processedLiteralField' should be the enum 'literalValue'.",
    "The 'processedObjectField' should be an object containing 'subField1' as a string, 'subField2' as a number, and an 'additionalField' as a boolean.",
    "The 'processedNestedObjectField' should be an object containing 'tupleField' as a tuple with a string and float, 'enumField' as one of the predefined options (option1, option2, or option3), 'datetimeField' as a 'date-time' object, 'literalField' as the string 'literalValue', and an 'additionalField' as a boolean."
  ],
  "input": {
    "datetimeField": "2023-10-01T12:00:00Z",
    "enumField": "option1",
    "literalField": "literalValue",
    "nestedObjectField": {
      "datetimeField": "2023-11-01T12:00:00Z",
      "enumField": "option2",
      "literalField": "literalValue",
      "tupleField": ["nestedString", 789]
    },
    "objectField": {
      "subField1": "Patriotism is a feeling of love, devotion, and sense of attachment to one's country. This attachment can be a combination of many different feelings relating to one's homeland, including ethnic, cultural, political or historical aspects. It encompasses a set of concepts closely related to those of nationalism. In the context of patriotism, people may express their feelings in a variety of ways, including supporting their country's interests and policies, celebrating national holidays, and participating in civic activities. Patriotism often involves a sense of pride in one's country and a willingness to defend it against any threats. It can also include a commitment to improving the country and making it a better place for future generations. The concept of patriotism is often linked with the idea of national identity, which is the sense of a nation as a cohesive whole, as represented by distinctive traditions, culture, language, and politics. Patriots may feel a strong sense of loyalty and duty to their country, and they may take actions to support and protect it. However, it is important to note that patriotism can also be a complex and sometimes controversial concept. While it can inspire positive actions and a sense of community, it can also lead to exclusionary or aggressive behaviors if taken to an extreme. In some cases, excessive patriotism can result in nationalism, which can lead to conflicts with other nations or groups. Despite these potential issues, many people view patriotism as a positive force that can unite people and inspire them to work together for the common good. It can foster a sense of belonging and purpose, and it can motivate individuals to contribute to the well-being of their country. Overall, patriotism is a multifaceted and deeply personal sentiment that can manifest in many different ways, depending on an individual's experiences, beliefs, and values.",
      "subField2": 456
    },
    "tupleField": ["exampleString", 123]
  }
}
</file>

<file path="tests/reliability/complex_types/generated/test_many_types_1/program.py">
### Input models ###
⋮----
class EnumField(Enum)
⋮----
option1 = "option1"
option2 = "option2"
option3 = "option3"
⋮----
class LiteralField(Enum)
⋮----
literalValue = "literalValue"
⋮----
class ObjectField(BaseModel)
⋮----
subField1: str
subField2: float
⋮----
class NestedObjectField(BaseModel)
⋮----
tupleField: Tuple[str, float]
enumField: EnumField
datetimeField: datetime
literalField: LiteralField
⋮----
class ProgramInputs(BaseModel)
⋮----
objectField: ObjectField
nestedObjectField: NestedObjectField
⋮----
### Output models ###
⋮----
class ProcessedEnumField(Enum)
⋮----
class ProcessedLiteralField(Enum)
⋮----
class ProcessedObjectField(BaseModel)
⋮----
additionalField: bool
⋮----
class ProcessedNestedObjectField(BaseModel)
⋮----
class ProgramOutputs(BaseModel)
⋮----
processedTupleField: Tuple[str, float]
processedEnumField: ProcessedEnumField
processedDatetimeField: datetime
processedLiteralField: ProcessedLiteralField
processedObjectField: ProcessedObjectField
processedNestedObjectField: ProcessedNestedObjectField
⋮----
### Program definition ###
⋮----
class BaseSignature(dspy.Signature)
⋮----
"""
    The program is designed to process various data types including tuples, enums, datetime values, literals, objects, and nested objects containing these types. The program will accept inputs of these types, perform specified operations on them, and return the results. The operations could include validation, transformation, and extraction of information from these inputs.
    """
⋮----
program_signature = BaseSignature
⋮----
program_signature = program_signature.append(
⋮----
program = dspy.Predict(program_signature)
</file>

<file path="tests/reliability/complex_types/generated/test_many_types_1/schema.json">
{
  "description": "The program is designed to process various data types including tuples, enums, datetime values, literals, objects, and nested objects containing these types. The program will accept inputs of these types, perform specified operations on them, and return the results. The operations could include validation, transformation, and extraction of information from these inputs.",
  "properties": {
    "datetimeField": {
      "desc": null,
      "format": "date-time",
      "prefix": "Datetime Field:",
      "type": "string"
    },
    "enumField": {
      "enum": ["option1", "option2", "option3"],
      "type": "string"
    },
    "literalField": {
      "const": "literalValue",
      "enum": ["literalValue"],
      "type": "string"
    },
    "nestedObjectField": {
      "properties": {
        "datetimeField": {
          "format": "date-time",
          "type": "string"
        },
        "enumField": {
          "enum": ["option1", "option2", "option3"],
          "type": "string"
        },
        "literalField": {
          "const": "literalValue",
          "enum": ["literalValue"],
          "type": "string"
        },
        "tupleField": {
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "number"
              }
            ]
          },
          "maxItems": 2,
          "minItems": 2,
          "type": "array"
        }
      },
      "required": ["tupleField", "enumField", "datetimeField", "literalField"],
      "type": "object"
    },
    "objectField": {
      "properties": {
        "subField1": {
          "type": "string"
        },
        "subField2": {
          "type": "number"
        }
      },
      "required": ["subField1", "subField2"],
      "type": "object"
    },
    "processedDatetimeField": {
      "desc": null,
      "format": "date-time",
      "prefix": "Processed Datetime Field:",
      "type": "string"
    },
    "processedEnumField": {
      "enum": ["option1", "option2", "option3"],
      "type": "string"
    },
    "processedLiteralField": {
      "const": "literalValue",
      "enum": ["literalValue"],
      "type": "string"
    },
    "processedNestedObjectField": {
      "properties": {
        "additionalField": {
          "type": "boolean"
        },
        "datetimeField": {
          "format": "date-time",
          "type": "string"
        },
        "enumField": {
          "enum": ["option1", "option2", "option3"],
          "type": "string"
        },
        "literalField": {
          "const": "literalValue",
          "enum": ["literalValue"],
          "type": "string"
        },
        "tupleField": {
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "number"
              }
            ]
          },
          "maxItems": 2,
          "minItems": 2,
          "type": "array"
        }
      },
      "required": [
        "tupleField",
        "enumField",
        "datetimeField",
        "literalField",
        "additionalField"
      ],
      "type": "object"
    },
    "processedObjectField": {
      "properties": {
        "additionalField": {
          "type": "boolean"
        },
        "subField1": {
          "type": "string"
        },
        "subField2": {
          "type": "number"
        }
      },
      "required": ["subField1", "subField2", "additionalField"],
      "type": "object"
    },
    "processedTupleField": {
      "desc": null,
      "items": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "number"
          }
        ]
      },
      "prefix": "Processed Tuple Field:",
      "type": "array"
    },
    "tupleField": {
      "desc": null,
      "items": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "number"
          }
        ]
      },
      "prefix": "Tuple Field:",
      "type": "array"
    }
  },
  "required": [
    "tupleField",
    "enumField",
    "datetimeField",
    "literalField",
    "objectField",
    "nestedObjectField",
    "processedTupleField",
    "processedEnumField",
    "processedDatetimeField",
    "processedLiteralField",
    "processedObjectField",
    "processedNestedObjectField"
  ],
  "type": "object"
}
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_1/inputs/input1.json">
{
  "assertions": [
    "The top-level output should contain the key 'resultLevel1'.",
    "'resultLevel1' should contain the key 'resultLevel2'.",
    "'resultLevel2' should contain the key 'resultLevel3'.",
    "'resultLevel3' should contain the key 'resultLevel4'.",
    "'resultLevel4' should contain the key 'resultLevel5'.",
    "'resultLevel5' should contain the key 'outputField1' which should be of type boolean.",
    "'resultLevel5' should contain the key 'outputField2' which should be an array of strings.",
    "'outputField1' should indicate success or failure with a boolean value.",
    "'outputField2' should contain messages represented as strings."
  ],
  "input": {
    "level1": {
      "level2": {
        "level3": {
          "level4": {
            "level5": {
              "field1": "test_string",
              "field2": 42
            }
          }
        }
      }
    }
  }
}
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_1/inputs/input2.json">
{
  "assertions": [
    "The output should have a top-level field named 'resultLevel1'.",
    "Within 'resultLevel1', there should be a nested field named 'resultLevel2'.",
    "Within 'resultLevel2', there should be a nested field named 'resultLevel3'.",
    "Within 'resultLevel3', there should be a nested field named 'resultLevel4'.",
    "Within 'resultLevel4', there should be a nested field named 'resultLevel5'.",
    "Within 'resultLevel5', there should be a field named 'outputField1' which must be of type boolean.",
    "Within 'resultLevel5', there should be a field named 'outputField2' which must be an array of strings."
  ],
  "input": {
    "level1": {
      "level2": {
        "level3": {
          "level4": {
            "level5": {
              "field1": "test string",
              "field2": 123.45
            }
          }
        }
      }
    }
  }
}
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_1/program.py">
### Input models ###
⋮----
class Level5(BaseModel)
⋮----
field1: str = Field(..., description="A string field at the deepest level")
field2: float = Field(..., description="A numerical field at the deepest level")
⋮----
class Level4(BaseModel)
⋮----
level5: Level5
⋮----
class Level3(BaseModel)
⋮----
level4: Level4
⋮----
class Level2(BaseModel)
⋮----
level3: Level3
⋮----
class Level1(BaseModel)
⋮----
level2: Level2
⋮----
class ProgramInputs(BaseModel)
⋮----
level1: Level1
⋮----
### Output models ###
⋮----
class ResultLevel5(BaseModel)
⋮----
outputField1: bool = Field(..., description="A boolean field indicating success or failure")
outputField2: list[str] = Field(..., description="An array of strings representing messages")
⋮----
class ResultLevel4(BaseModel)
⋮----
resultLevel5: ResultLevel5
⋮----
class ResultLevel3(BaseModel)
⋮----
resultLevel4: ResultLevel4
⋮----
class ResultLevel2(BaseModel)
⋮----
resultLevel3: ResultLevel3
⋮----
class ResultLevel1(BaseModel)
⋮----
resultLevel2: ResultLevel2
⋮----
class ProgramOutputs(BaseModel)
⋮----
resultLevel1: ResultLevel1
⋮----
### Program definition ###
⋮----
class BaseSignature(dspy.Signature)
⋮----
"""
    The AI program is designed to process hierarchical data structures with multiple levels of nesting. The program will take a deeply nested input structure representing a complex dataset, perform specific transformations, validations, and computations, and then produce an equally complex nested output structure. The program is suitable for applications that require detailed data processing, such as multi-level data aggregation, hierarchical data validation, and nested data transformation.
    """
⋮----
program_signature = BaseSignature
⋮----
program_signature = program_signature.append(
⋮----
program = dspy.Predict(program_signature)
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_1/schema.json">
{
  "description": "The AI program is designed to process hierarchical data structures with multiple levels of nesting. The program will take a deeply nested input structure representing a complex dataset, perform specific transformations, validations, and computations, and then produce an equally complex nested output structure. The program is suitable for applications that require detailed data processing, such as multi-level data aggregation, hierarchical data validation, and nested data transformation.",
  "properties": {
    "level1": {
      "properties": {
        "level2": {
          "properties": {
            "level3": {
              "properties": {
                "level4": {
                  "properties": {
                    "level5": {
                      "properties": {
                        "field1": {
                          "description": "A string field at the deepest level",
                          "type": "string"
                        },
                        "field2": {
                          "description": "A numerical field at the deepest level",
                          "type": "number"
                        }
                      },
                      "required": ["field1", "field2"],
                      "type": "object"
                    }
                  },
                  "required": ["level5"],
                  "type": "object"
                }
              },
              "required": ["level4"],
              "type": "object"
            }
          },
          "required": ["level3"],
          "type": "object"
        }
      },
      "required": ["level2"],
      "type": "object"
    },
    "resultLevel1": {
      "properties": {
        "resultLevel2": {
          "properties": {
            "resultLevel3": {
              "properties": {
                "resultLevel4": {
                  "properties": {
                    "resultLevel5": {
                      "properties": {
                        "outputField1": {
                          "description": "A boolean field indicating success or failure",
                          "type": "boolean"
                        },
                        "outputField2": {
                          "description": "An array of strings representing messages",
                          "items": {
                            "type": "string"
                          },
                          "type": "array"
                        }
                      },
                      "required": ["outputField1", "outputField2"],
                      "type": "object"
                    }
                  },
                  "required": ["resultLevel5"],
                  "type": "object"
                }
              },
              "required": ["resultLevel4"],
              "type": "object"
            }
          },
          "required": ["resultLevel3"],
          "type": "object"
        }
      },
      "required": ["resultLevel2"],
      "type": "object"
    }
  },
  "required": ["level1", "resultLevel1"],
  "type": "object"
}
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_2/inputs/input1.json">
{
  "assertions": [
    "The output should contain a 'customer_summary' object with the required properties: 'customer_id', 'customer_type', and 'value'.",
    "'customer_summary.customer_id' should be a string and match the 'customer_id' from the input.",
    "'customer_summary.customer_type' should be an object containing 'is_premium' (a boolean) and 'category' (a string).",
    "'customer_summary.value' should be a string and reflect the 'value' from the input's customer details.",
    "The output should contain a 'transaction_summary' object with the required properties: 'transaction_id', 'total_amount', and 'details'.",
    "'transaction_summary.transaction_id' should be a string and match the 'transaction_id' from the input.",
    "'transaction_summary.total_amount' should be a number and match the 'amount' from the input.",
    "'transaction_summary.details' should be an object containing 'value' (a number) and 'timestamp' (a date-time value)."
  ],
  "input": {
    "customer": {
      "customer_id": "C12345",
      "customer_type": true,
      "details": {
        "age": 30,
        "value": "Gold"
      }
    },
    "transaction": {
      "amount": 150.75,
      "details": {
        "timestamp": "2023-10-01T10:00:00Z",
        "value": 150.75
      },
      "transaction_id": "T98765"
    }
  }
}
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_2/program.py">
### Input models ###
⋮----
class Details(BaseModel)
⋮----
value: str = Field(..., description="Customer's value category")
age: int = Field(..., description="Customer's age")
⋮----
class Customer(BaseModel)
⋮----
customer_id: str = Field(..., description="Unique identifier for the customer")
customer_type: bool = Field(..., description="Indicates if the customer is a premium member")
details: Details
⋮----
class Details1(BaseModel)
⋮----
value: float = Field(..., description="Monetary value of the transaction")
timestamp: datetime = Field(..., description="Timestamp of the transaction")
⋮----
class Transaction(BaseModel)
⋮----
transaction_id: str = Field(..., description="Unique identifier for the transaction")
amount: float = Field(..., description="Transaction amount")
details: Details1
⋮----
class ProgramInputs(BaseModel)
⋮----
customer: Customer
transaction: Transaction
⋮----
### Output models ###
⋮----
class CustomerType(BaseModel)
⋮----
is_premium: bool = Field(..., description="Indicates if the customer is a premium member")
category: str = Field(..., description="Customer's membership category")
⋮----
class CustomerSummary(BaseModel)
⋮----
customer_type: CustomerType
⋮----
class TransactionSummary(BaseModel)
⋮----
total_amount: float = Field(..., description="Total transaction amount")
⋮----
class ProgramOutputs(BaseModel)
⋮----
customer_summary: CustomerSummary
transaction_summary: TransactionSummary
⋮----
### Program definition ###
⋮----
class BaseSignature(dspy.Signature)
⋮----
"""
    This AI program is designed to process complex datasets with multiple nested input fields and produce structured output fields. It can handle cases where nested fields have the same name but different types, ensuring that the data is accurately processed and transformed. The program is particularly useful for applications that require detailed data analysis, integration of multiple data sources, and handling of heterogeneous data types.
    """
⋮----
program_signature = BaseSignature
⋮----
program_signature = program_signature.append(
⋮----
program = dspy.ChainOfThought(program_signature)
</file>

<file path="tests/reliability/complex_types/generated/test_nesting_2/schema.json">
{
  "description": "This AI program is designed to process complex datasets with multiple nested input fields and produce structured output fields. It can handle cases where nested fields have the same name but different types, ensuring that the data is accurately processed and transformed. The program is particularly useful for applications that require detailed data analysis, integration of multiple data sources, and handling of heterogeneous data types.",
  "properties": {
    "customer": {
      "properties": {
        "customer_id": {
          "description": "Unique identifier for the customer",
          "type": "string"
        },
        "customer_type": {
          "description": "Indicates if the customer is a premium member",
          "type": "boolean"
        },
        "details": {
          "properties": {
            "age": {
              "description": "Customer's age",
              "type": "integer"
            },
            "value": {
              "description": "Customer's value category",
              "type": "string"
            }
          },
          "required": ["value", "age"],
          "type": "object"
        }
      },
      "required": ["customer_id", "customer_type", "details"],
      "type": "object"
    },
    "customer_summary": {
      "properties": {
        "customer_id": {
          "description": "Unique identifier for the customer",
          "type": "string"
        },
        "customer_type": {
          "properties": {
            "category": {
              "description": "Customer's membership category",
              "type": "string"
            },
            "is_premium": {
              "description": "Indicates if the customer is a premium member",
              "type": "boolean"
            }
          },
          "required": ["is_premium", "category"],
          "type": "object"
        },
        "value": {
          "description": "Customer's value category",
          "type": "string"
        }
      },
      "required": ["customer_id", "customer_type", "value"],
      "type": "object"
    },
    "transaction": {
      "properties": {
        "amount": {
          "description": "Transaction amount",
          "type": "number"
        },
        "details": {
          "properties": {
            "timestamp": {
              "description": "Timestamp of the transaction",
              "format": "date-time",
              "type": "string"
            },
            "value": {
              "description": "Monetary value of the transaction",
              "type": "number"
            }
          },
          "required": ["value", "timestamp"],
          "type": "object"
        },
        "transaction_id": {
          "description": "Unique identifier for the transaction",
          "type": "string"
        }
      },
      "required": ["transaction_id", "amount", "details"],
      "type": "object"
    },
    "transaction_summary": {
      "properties": {
        "details": {
          "properties": {
            "timestamp": {
              "description": "Timestamp of the transaction",
              "format": "date-time",
              "type": "string"
            },
            "value": {
              "description": "Monetary value of the transaction",
              "type": "number"
            }
          },
          "required": ["value", "timestamp"],
          "type": "object"
        },
        "total_amount": {
          "description": "Total transaction amount",
          "type": "number"
        },
        "transaction_id": {
          "description": "Unique identifier for the transaction",
          "type": "string"
        }
      },
      "required": ["transaction_id", "total_amount", "details"],
      "type": "object"
    }
  },
  "required": [
    "customer",
    "transaction",
    "customer_summary",
    "transaction_summary"
  ],
  "type": "object"
}
</file>

<file path="tests/reliability/generate/__init__.py">
def _directory_contains_program(dir_path: str) -> bool
</file>

<file path="tests/reliability/generate/__main__.py">
parser = argparse.ArgumentParser(
⋮----
args = parser.parse_args()
</file>

<file path="tests/reliability/generate/utils.py">
def _retry(retries)
⋮----
"""
    A decorator to retry a function a specified number of times.

    Args:
        retries (int): The number of retries before failing.
    """
⋮----
def decorator(func)
⋮----
@wraps(func)
        def wrapper(*args, **kwargs)
⋮----
attempt = 0
⋮----
@_retry(retries=5)
def generate_test_program(dst_path: str, additional_instructions: Optional[str] = None) -> dspy.Module
⋮----
"""
    Generate a DSPy program for a reliability test case and save it to a destination path.

    Args:
        dst_path: The directory path to which to save the generated program.
        additional_instructions: Additional instructions for generating the program signature.
    Return:
        A dspy.Module object representing the generated program.
    """
⋮----
def generate_models(schema: dict[str, Any], class_name: str) -> str
⋮----
tmp_schema_path = os.path.join(tmp_dir, "schema.json")
tmp_model_path = os.path.join(tmp_dir, "model.py")
⋮----
# For enums with only one value, use the value as a literal instead of an enum
# in order to test literals
⋮----
# Don't use con* field types, which are deprecated in recent pydantic versions
⋮----
# Remove annotation imports from __future__, which break compatibility with Python's
# built-in type hints
⋮----
# Remove comments inserted by datamodel-code-generator from the generated model file
⋮----
input_fields = set(input_schema.get("properties", {}))
⋮----
# Update required fields, if they exist
⋮----
# Disable caching and use a nonzero temperature to ensure that new programs are generated
# upon retry if there's an error in the generation process (e.g. the program has an
# invalid signature)
⋮----
generated_signature = _get_test_program_generation_program()(
input_schema = json.loads(generated_signature.program_input_fields)
output_schema = json.loads(generated_signature.program_output_fields)
# If there are conflicting field names between input and output schemas, rename the output
# fields to avoid conflicts
output_schema = rename_conflicting_fields(input_schema, output_schema)
⋮----
# Generate input and output models
input_models = generate_models(schema=input_schema, class_name="ProgramInputs")
output_models = generate_models(schema=output_schema, class_name="ProgramOutputs")
⋮----
# Write program code
program_code = (
program_path = os.path.join(tmp_dir, "program.py")
⋮----
# Validate the generated program by loading it before copying it to the destination path
⋮----
# Write schema
⋮----
# Copy all generated files to the destination path
⋮----
"""
    Generate test inputs for a reliability test case and save them to a destination path.

    Args:
        dst_path: The directory path to which to save the generated test inputs.
        program_path: The path to the program for which to generate test inputs.
        num_inputs: The number of test inputs to generate.
        additional_instructions: Additional instructions for generating the test inputs.
    """
# Disable caching and use a nonzero temperature to ensure that new inputs are generated
# upon retry if there's an error in the generation process (e.g. the input doesn't match the
# program signature)
⋮----
program: dspy.Module
program_input_schema: pydantic.BaseModel
⋮----
signature_json_schema = _get_json_schema(program.signature)
⋮----
generated_test_inputs = _get_test_inputs_generation_program()(
⋮----
def find_max_input_number(directory)
⋮----
max_number = 0
pattern = re.compile(r"input(\d+)\.json")
⋮----
match = pattern.match(filename)
⋮----
number = int(match.group(1))
max_number = max(max_number, number)
⋮----
base_input_number = find_max_input_number(dst_path) + 1
⋮----
output_assertions = _get_assertions_generation_program()(
⋮----
# Verify that the generated input is valid JSON and matches the input signature of the
# program before saving it to the destination path
⋮----
test_input_file_path = os.path.join(tmp_dir, f"input{base_input_number + idx}.json")
json_program_input = json.loads(test_input.program_input)
⋮----
def load_generated_program(path) -> Tuple[dspy.Module, pydantic.BaseModel]
⋮----
"""
    Loads a generated program from the specified file.

    Args:
        path: The path to the file containing the generated program.
    Returns:
        A tuple containing: 1. a dspy.Module object representing the generated program
        and 2. a pydantic.BaseModel object representing the program's input schema.
    """
⋮----
path = os.path.join(path, "program.py")
⋮----
program_module = _import_program_module_from_path(module_name="program", file_path=path)
⋮----
@dataclass
class GeneratedTestCase
⋮----
"""
    Represents a DSPy reliability test case that has been generated with the help of a
    DSPy program generator and program input generator.
    """
⋮----
# The name of the test case for identification / debugging with pytest
name: str
# The local filesystem path to the program that the test case is testing.
program_path: str
# A JSON  representation of the input to the program that the test case is testing.
program_input: str
# The assertions that the output of the program must satisfy for the test case to pass.
output_assertions: list[str]
⋮----
def load_generated_cases(dir_path) -> list[GeneratedTestCase]
⋮----
"""
    Recursively loads generated test cases from the specified directory and its subdirectories.

    Args:
        dir_path: The path to the directory containing the generated test cases.
    Returns:
        A list of GeneratedTestCase objects.
    """
test_cases = []
⋮----
# Walk through all directories and subdirectories in dir_path
⋮----
# Check if the directory contains a program.py and an inputs directory
⋮----
program_path = os.path.join(root, "program.py")
inputs_path = os.path.join(root, "inputs")
⋮----
# Load each JSON test input file in the inputs directory
⋮----
# Best effort to extract a meaningful enclosing directory name
# from the test path that can be used as part of the test case name
readable_dir_name = os.path.basename(os.path.dirname(os.path.dirname(root)))
test_case_name = (
program_input_and_assertions = json.load(f)
program_input = program_input_and_assertions["input"]
assertions = program_input_and_assertions["assertions"]
⋮----
# Create a GeneratedTestCase object and add it to the list
⋮----
def run_generated_case(generated_case: GeneratedTestCase)
⋮----
"""
    Runs a generated reliability test case by 1. running the test case program on the test case
    input using the global DSPy configuration and 2. verifying that the output of the program
    satisfies the assertions specified in the test case.

    Args:
        generated_case: The generated test case to run.
    """
⋮----
program_input = _json_input_to_program_input(
program_output = program(**program_input)
⋮----
def _get_test_program_signature_and_module_definition(program_description: str) -> str
⋮----
"""
    Generate the signature and model definition for a test DSPy program.

    Args:
        program_description: A description of the generated program.
    """
use_cot = random.choice([True, False])
⋮----
program_var_definition = "program = dspy.ChainOfThought(program_signature)"
⋮----
program_var_definition = "program = dspy.Predict(program_signature)"
⋮----
def _get_test_program_generation_program() -> dspy.Module
⋮----
"""
    Create a DSPy program for generating other DSPy test programs.

    Returns:
        A dspy.Module object representing the program generation program.
    """
⋮----
class ProgramGeneration(dspy.Signature)
⋮----
"""
        Creates an AI program definition, including the AI program's description, input fields, and output fields.
        The AI program should be designed to solve a real problem for its users and produce correct outputs for a variety of inputs.

        The input fields and the output fields must be represented in JSON Schema format, including field names, types, and descriptions.
        The JSON schema definitions themselves MUST be valid JSON without any extra text (no backticks, no explanatory text, etc.).

        It's very important to be sure that the additional instructions, if specified, are obeyed
        precisely in absolutely all cases.
        """
⋮----
additional_instructions: str = dspy.InputField(
program_description: str = dspy.OutputField(
program_input_fields: str = dspy.OutputField(
program_output_fields: str = dspy.OutputField(
⋮----
def _get_test_inputs_generation_program() -> dspy.Module
⋮----
"""
    Create a DSPy program for generating test inputs for a given DSPy test program.

    Returns:
        A dspy.Module object representing the test input generation program.
    """
⋮----
class _TestInputsGeneration(dspy.Signature)
⋮----
"""
        Given the description and input / output signature (format) of an AI program that is designed to produce correct outputs for a variety
        of inputs while adhering to the input / output signature, generate test inputs used to verify that the program
        indeed produces correct outputs. The AI program uses LLM prompting with carefully crafted prompt templates to generate
        responses.

        When generating an input, do not think about how the program will respond. Instead, focus on creating
        valid and interesting inputs that are likely to test the program's capabilities.

        It's very important to be sure that the additional instructions, if specified, are obeyed
        precisely in absolutely all cases.
        """
⋮----
program_description: str = dspy.InputField(
program_input_signature: str = dspy.InputField(
program_output_signature: str = dspy.InputField(
additional_instructions: str = dspy.InputField(description="Additional instructions for generating test inputs")
test_inputs: list[_TestInput] = dspy.OutputField(
⋮----
class _TestInput(pydantic.BaseModel)
⋮----
"""
    Represents a generated test input for a DSPy program.
    """
⋮----
program_input: str = pydantic.Field(
⋮----
def _get_assertions_generation_program() -> dspy.Module
⋮----
"""
    Create a DSPy program for generating assertions that verify the correctness of outputs
    from other DSPy programs.
    """
⋮----
"""
        Given 1. the description and input / output signature (format) of an AI program that is designed to produce correct outputs for a variety
        of inputs while adhering to the input / output signature and 2. an example input to the AI program, generate assertions that can be used
        to verify the correctness of the program output.

        Assertions should be expressed in natural language where possible, rather than code. Only
        include code if necessary to clarify the assertion. Assertions should be objective and verifiable,
        with minimal subjectivity only where absolutely necessary.

        There should be a limited number of assertions, ideally about 5, that are sufficient to
        verify the correctness of the program output.

        If it's too difficult to generate accurate assertions, leave them blank.
        """
⋮----
program_input: str = dspy.InputField(
⋮----
output_assertions: list[str] = dspy.OutputField(
⋮----
def _clean_json_schema_property(prop: dict[str, Any]) -> dict[str, Any]
⋮----
"""
    Remove unnecessary keys from a JSON schema property dictionary, as well as
    all of its child properties.

    Args:
        prop: The JSON schema property dictionary to clean.
    Returns:
        The cleaned JSON schema property dictionary.
    """
cleaned_prop = {
⋮----
# Recursively clean nested properties
⋮----
def _get_json_schema(signature: dspy.Signature) -> dict[str, Any]
⋮----
"""
    Obtain the JSON schema representation of a DSPy signature.

    Args:
        signature: The DSPy signature for which to generate a JSON schema.
    Returns:
        A JSON schema representation of the signature.
    """
⋮----
def expand_refs(schema: dict[str, Any], definitions: dict[str, Any]) -> dict[str, Any]
⋮----
"""
        Expand $ref fields in a JSON schema, inlining the referenced schema definitions
        directly into the $ref field locations.
        """
⋮----
ref_path = schema["$ref"].replace("#/$defs/", "")
ref_schema = definitions.get(ref_path, {})
⋮----
# Recursively expand the reference schema as well
⋮----
# Recursively expand properties in the schema
⋮----
signature_schema_with_refs = signature.schema()
definitions = signature_schema_with_refs.pop("$defs", {})
⋮----
def _split_schema(schema: dict[str, Any]) -> Tuple[dict[str, Any], dict[str, Any]]
⋮----
"""
    Split a JSON schema into input and output components based on DSPy field types.

    Args:
        schema: The JSON schema to split.
    Returns:
        A tuple containing the input and output components of the schema.
    """
inputs = {}
outputs = {}
⋮----
# Traverse the properties to categorize inputs and outputs
⋮----
# Clean the property
cleaned_prop = _clean_schema(prop)
⋮----
# Determine if the property is input or output based on __dspy_field_type
field_type = prop.get("__dspy_field_type")
⋮----
# Handle nested properties for complex models
⋮----
def _clean_schema(prop: dict[str, Any]) -> dict[str, Any]
⋮----
"""
    Recursively clean a JSON schema property by removing unnecessary keys.

    Args:
        prop: The JSON schema property to clean.
    Returns:
        A cleaned version of the property.
    """
keys_to_remove = ["__dspy_field_type", "title"]  # Add any other keys to be removed here
⋮----
# Iterate through the dictionary, applying cleaning recursively if value is a nested dict
⋮----
k: (_clean_schema(v) if isinstance(v, dict) else v)  # Recurse if value is a dict
⋮----
def _json_input_to_program_input(input_schema: pydantic.BaseModel, json_input: str) -> dict[str, Any]
⋮----
"""
    Convert a JSON input string to a DSPy program input dictionary, validating it against the
    provided program signature.

    Args:
        input_schema: A pydantic model representing the program input schema.
        json_input: The JSON input string to convert to a DSPy program input.
    Returns:
        The converted DSPy program input dictionary.
    """
json_input = json.loads(json_input)
program_input: pydantic.BaseModel = input_schema.model_validate(json_input)
⋮----
@contextmanager
def _temporarily_prepend_to_system_path(path)
⋮----
"""
    Temporarily prepend a path to the system path for the duration of a context.

    Args:
        path: The path to prepend to the system path.
    """
original_sys_path = sys.path.copy()
⋮----
def _import_program_module_from_path(module_name: str, file_path: str)
⋮----
"""
    Import a Python module containing a DSPy program from a specified file path.

    Args:
        module_name: The name of the module containing the DSPy program to import.
        file_path: The path to the file containing the module definition.
    """
program_dir = os.path.dirname(file_path)
⋮----
spec = importlib.util.spec_from_file_location(module_name, file_path)
module = importlib.util.module_from_spec(spec)
⋮----
def _remove_line_from_file(file_path: str, line_to_remove: str)
⋮----
"""
    Remove all instances of a specific line from a file.

    Args:
        file_path: The path to the file from which to remove all instances of the line.
        line_to_remove: The line to remove from the file.
    """
# Read all lines from the file
⋮----
lines = file.readlines()
⋮----
# Write all lines back except the one to remove
⋮----
def _remove_comments_from_file(file_path: str) -> None
⋮----
"""
    Removes all lines with comments from the specified file.

    Args:
        file_path: Path to the file where comments should be removed.
    """
# Read the file contents
⋮----
# Filter out lines that start with '#'
cleaned_lines = [line for line in lines if not line.strip().startswith("#")]
⋮----
# Write the cleaned lines back to the file
⋮----
def _write_pretty_json(data: dict[str, Any], path: Optional[str] = None) -> Optional[str]
⋮----
"""
    Format JSON data with indentation, and write it to a file if specified.

    Args:
        data: The JSON data to format.
        path: The optional path to which to write the formatted JSON data.
    Returns:
        The formatted JSON data as a string, if no path is specified.
    """
formatted_json = json.dumps(data, indent=4)
</file>

<file path="tests/reliability/input_formats/generated/test_markdown_1/inputs/input1.json">
{
  "assertions": [
    "Each top-level heading (indicated by `#`) should appear as a top-level entry in the TOC.",
    "Each second-level heading (indicated by `##`) should be nested under the appropriate top-level heading in the TOC.",
    "Each third-level heading (indicated by `###`) should be nested under the appropriate second-level heading in the TOC.",
    "Each entry in the TOC should be linked to the corresponding section in the document, using markdown link syntax."
  ],
  "input": {
    "markdown_content": "# The American Space Program\n\nThe American space program has a rich history of exploration and discovery.\n\n## Early Beginnings\n\nThe journey began in the late 1950s with the launch of the first artificial satellite.\n\n### The Space Race\n\nThe competition between the United States and the Soviet Union led to rapid advancements in space technology.\n\n## Moon Landing\n\nIn 1969, NASA successfully landed the first humans on the moon.\n\n### Apollo Missions\n\nThe Apollo missions were a series of spaceflights that landed humans on the moon and brought them back safely.\n\n## Space Shuttle Era\n\nThe development of the Space Shuttle program marked a new era in space exploration.\n\n### Reusable Spacecraft\n\nThe Space Shuttle was the first reusable spacecraft, capable of multiple missions.\n\n## International Space Station\n\nThe International Space Station (ISS) is a collaborative effort between multiple countries.\n\n### Living in Space\n\nAstronauts live and work on the ISS for extended periods, conducting scientific research.\n\n## Future Missions\n\nNASA continues to plan for future missions to Mars and beyond.\n\n### Mars Exploration\n\nExploration of Mars is a key objective for NASA's future missions.\n\n### Beyond Mars\n\nThe ultimate goal is to explore beyond Mars and into the outer reaches of the solar system.\n\n## Conclusion\n\nThe American space program has achieved many milestones and continues to push the boundaries of space exploration."
  }
}
</file>

<file path="tests/reliability/input_formats/generated/test_markdown_1/inputs/input2.json">
{
  "assertions": [
    "Each entry in the TOC should be a markdown link pointing to the corresponding section in the document.",
    "The hierarchy of the TOC should match the levels of headings in the input markdown content (e.g., H1 headings as top-level, H2 headings nested under H1, etc.).",
    "The TOC should include all headings from the input markdown content, in the order they appear.",
    "The TOC should not include any non-heading content from the input markdown document."
  ],
  "input": {
    "markdown_content": "# Introduction\n\nThis is the introduction section.\n\n## Overview\n\nAn overview of the document.\n\n### Details\n\nMore detailed information.\n\n#### Subdetails\n\nEven more detailed information.\n\n## Another Section\n\nContent of another section.\n\n### Subsection\n\nDetails of the subsection.\n\n```python\ndef example_function():\n    print(\"Hello, World!\")\n```\n\n# Conclusion\n\nFinal thoughts."
  }
}
</file>

<file path="tests/reliability/input_formats/generated/test_markdown_1/program.py">
### Input models ###
⋮----
class ProgramInputs(BaseModel)
⋮----
markdown_content: str = Field(
⋮----
### Output models ###
⋮----
class ProgramOutputs(BaseModel)
⋮----
table_of_contents: str = Field(..., description="The generated table of contents in markdown format.")
⋮----
### Program definition ###
⋮----
class BaseSignature(dspy.Signature)
⋮----
"""
    The program is designed to generate a table of contents (TOC) from a given markdown document. It will parse the markdown content, identify headings, and create a hierarchical TOC based on the heading levels. The TOC will be presented in markdown format, with each entry linked to the corresponding section in the document.
    """
⋮----
program_signature = BaseSignature
⋮----
program_signature = program_signature.append(
⋮----
program = dspy.ChainOfThought(program_signature)
</file>

<file path="tests/reliability/input_formats/generated/test_markdown_1/schema.json">
{
  "description": "The program is designed to generate a table of contents (TOC) from a given markdown document. It will parse the markdown content, identify headings, and create a hierarchical TOC based on the heading levels. The TOC will be presented in markdown format, with each entry linked to the corresponding section in the document.",
  "properties": {
    "markdown_content": {
      "desc": "The content of the markdown document from which the table of contents will be generated.",
      "description": "The content of the markdown document from which the table of contents will be generated.",
      "prefix": "Markdown Content:",
      "type": "string"
    },
    "table_of_contents": {
      "desc": "The content of the markdown document from which the table of contents will be generated.",
      "description": "The content of the markdown document from which the table of contents will be generated.",
      "prefix": "Table Of Contents:",
      "type": "string"
    }
  },
  "required": ["markdown_content", "table_of_contents"],
  "type": "object"
}
</file>

<file path="tests/reliability/__init__.py">

</file>

<file path="tests/reliability/conftest.py">
# Standard list of models that should be used for periodic DSPy reliability testing
MODEL_LIST = [
⋮----
def pytest_generate_tests(metafunc)
⋮----
"""
    Hook to parameterize reliability test cases with each model defined in the
    reliability tests YAML configuration
    """
known_failing_models = getattr(metafunc.function, "_known_failing_models", [])
⋮----
params = [(model, model in known_failing_models) for model in MODEL_LIST]
ids = [f"{model}" for model, _ in params]  # Custom IDs for display
⋮----
@pytest.fixture(autouse=True)
def configure_model(request)
⋮----
"""
    Fixture to configure the DSPy library with a particular configured model and adapter
    before executing a test case.
    """
module_dir = os.path.dirname(os.path.abspath(__file__))
conf_path = os.path.join(module_dir, "reliability_conf.yaml")
reliability_conf = parse_reliability_conf_yaml(conf_path)
adapter = get_adapter(reliability_conf)
⋮----
model_params = reliability_conf.models.get(model_name)
⋮----
lm = dspy.LM(**model_params)
⋮----
# Store `should_ignore_failure` flag on the request node for use in post-test handling
⋮----
@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call)
⋮----
"""
    Hook to conditionally ignore failures in a given test case for known failing models.
    """
outcome = yield
rep = outcome.get_result()
⋮----
should_ignore_failure = getattr(item, "should_ignore_failure", False)
</file>

<file path="tests/reliability/README.md">
# DSPy Reliability Tests

This directory contains reliability tests for DSPy programs. The purpose of these tests is to verify that DSPy programs reliably produce expected outputs across multiple large language models (LLMs), regardless of model size or capability. These tests are designed to ensure that DSPy programs maintain robustness and accuracy across diverse LLM configurations.

### Overview

Each test in this directory executes a DSPy program using various LLMs. By running the same tests across different models, these tests help validate that DSPy programs handle a wide range of inputs effectively and produce reliable outputs, even in cases where the model might struggle with the input or task.

### Key Features

- **Diverse LLMs**: Each DSPy program is tested with multiple LLMs, ranging from smaller models to more advanced, high-performance models. This approach allows us to assess the consistency and generality of DSPy program outputs across different model capabilities.
- **Challenging and Adversarial Tests**: Some of the tests are intentionally challenging or adversarial, crafted to push the boundaries of DSPy. These challenging cases allow us to gauge the robustness of DSPy and identify areas for potential improvement.
- **Cross-Model Compatibility**: By testing with different LLMs, we aim to ensure that DSPy programs perform well across model types and configurations, reducing model-specific edge cases and enhancing program versatility.

### Running the Tests

- First, populate the configuration file `reliability_tests_conf.yaml` (located in this directory) with the necessary LiteLLM model/provider names and access credentials for 1. each LLM you want to test and 2. the LLM judge that you want to use for assessing the correctness of outputs in certain test cases. These should be placed in the `litellm_params` section for each model in the defined `model_list`. You can also use `litellm_params` to specify values for LLM hyperparameters like `temperature`. Any model that lacks configured `litellm_params` in the configuration file will be ignored during testing.

  The configuration must also specify a DSPy adapter to use when testing, e.g. `"chat"` (for `dspy.ChatAdapter`) or `"json"` (for `dspy.JSONAdapter`).

  An example of `reliability_tests_conf.yaml`:

      ```yaml
      adapter: chat
      model_list:
        # The model to use for judging the correctness of program
        # outputs throughout reliability test suites. We recommend using
        # a high quality model as the judge, such as OpenAI GPT-4o
        - model_name: "judge"
          litellm_params:
            model: "openai/gpt-4o"
            api_key: "<my_openai_api_key>"
        - model_name: "gpt-4o"
          litellm_params:
            model: "openai/gpt-4o"
            api_key: "<my_openai_api_key>"
        - model_name: "claude-3.5-sonnet"
          litellm_params:
            model: "anthropic/claude-3.5"
            api_key: "<my_anthropic_api_key>"

- Second, to run the tests, run the following command from this directory:

  ```bash
      pytest .
  ```

  This will execute all tests for the configured models and display detailed results for each model configuration. Tests are set up to mark expected failures for known challenging cases where a specific model might struggle, while actual (unexpected) DSPy reliability issues are flagged as failures (see below).

#### Running specific generated tests

You can run specific generated tests by using the `-k` flag with `pytest`. For example, to test the generated program located at `tests/reliability/complex_types/generated/test_nesting_1` against generated test input `input1.json`, you can run the following command from this directory:

```bash
pytest test_generated.py -k "test_nesting_1-input1"
```

### Test generation

You can generate test DSPy programs and test inputs from text descriptions using the `tests.reliability.generate` CLI, or the `tests.reliability.generate.generate_test_cases` API. For example, to generate a test classification program and 3 challenging test inputs in the `tests/reliability/classification/generated` directory, you can run the following command from the DSPy repository root directory:

```bash
python \
    -m tests.reliability.generate \
    -d tests/reliability/classification/generated/test_example \
    -p "Generate a program that performs a classification task involving objects with multiple properties. The task should be realistic" \
    -i "Based on the program description, generate a challenging example" \
    -n 3
```

The test program will be written to `tests/reliability/classification/generated/test_example/program.py`, and the test inputs will be written as JSON files to the `tests/reliability/classification/generated/test_exaple/inputs/` directory.

All generated tests should be located in directories with the structure `tests/reliability/<test_type>/generated/<test_name>`, where `<test_type>` is the type of test (e.g., `classification`, `complex_types`, `chat`, etc.), and `<test_name>` is a descriptive name for the test.

### Known Failing Models

Some tests may be expected to fail with certain models, especially in challenging cases. These known failures are logged but do not affect the overall test result. This setup allows us to keep track of model-specific limitations without obstructing general test outcomes. Models that are known to fail a particular test case are specified using the `@known_failing_models` decorator. For example:

```
@known_failing_models(["llama-3.2-3b-instruct"])
def test_program_with_complex_deeply_nested_output_structure():
    ...
```
</file>

<file path="tests/reliability/reliability_conf.yaml">
adapter: chat
model_list:
  # The model to use for judging the correctness of program
  # outputs throughout reliability test suites. We recommend using
  # a high quality model as the judge, such as OpenAI GPT-4o
  - model_name: "judge"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gpt-4o"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gpt-4o-mini"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gpt-4-turbo"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gpt-o1"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gpt-o1-mini"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "claude-3.5-sonnet"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "claude-3.5-haiku"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gemini-1.5-pro"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "gemini-1.5-flash"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "llama-3.1-405b-instruct"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "llama-3.1-70b-instruct"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "llama-3.1-8b-instruct"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "llama-3.2-3b-instruct"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # api_base: "<api_base>"
  - model_name: "deepseek-r1"
    litellm_params:
      # model: "<litellm_provider>/<litellm_model_name>"
      # api_key: "api key"
      # max_tokens: 10000
</file>

<file path="tests/reliability/test_generated.py">
_DIR_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)))
⋮----
def test_generated_cases(generated_case)
</file>

<file path="tests/reliability/test_pydantic_models.py">
@pytest.mark.reliability
def test_qa_with_pydantic_answer_model()
⋮----
class Answer(pydantic.BaseModel)
⋮----
value: str
certainty: float = pydantic.Field(
comments: list[str] = pydantic.Field(
⋮----
class QA(dspy.Signature)
⋮----
question: str = dspy.InputField()
answer: Answer = dspy.OutputField()
⋮----
program = dspy.Predict(QA)
question = "What is the capital of France?"
answer = program(question=question).answer
⋮----
@pytest.mark.parametrize("module", [dspy.Predict, dspy.ChainOfThought])
@pytest.mark.reliability
def test_color_classification_using_enum(module)
⋮----
Color = Enum("Color", ["RED", "GREEN", "BLUE"])
⋮----
class Colorful(dspy.Signature)
⋮----
text: str = dspy.InputField()
color: Color = dspy.OutputField()
⋮----
program = module(Colorful)
# Note: The precise text, including the trailing period, is important here for ensuring that
# the program is correctly extracting the color from the text; previous implementations have
# produced invalid enum responses for "The sky is blue.", but they have produced valid enum
# responses for "The sky is blue" (without the period).
color = program(text="The sky is blue.").color
⋮----
@pytest.mark.reliability
def test_entity_extraction_with_multiple_primitive_outputs()
⋮----
class ExtractEntityFromDescriptionOutput(pydantic.BaseModel)
⋮----
entity_hu: str = pydantic.Field(description="The extracted entity in Hungarian, cleaned and lowercased.")
entity_en: str = pydantic.Field(description="The English translation of the extracted Hungarian entity.")
is_inverted: bool = pydantic.Field(
categories: str = pydantic.Field(description="English categories separated by '|' to which the entity belongs.")
review: bool = pydantic.Field(
⋮----
class ExtractEntityFromDescription(dspy.Signature)
⋮----
"""Extract an entity from a Hungarian description, provide its English translation, categories, and an inverted flag."""
⋮----
description: str = dspy.InputField(description="The input description in Hungarian.")
entity: ExtractEntityFromDescriptionOutput = dspy.OutputField(
⋮----
program = dspy.ChainOfThought(ExtractEntityFromDescription)
description = "A kávé egy növényi eredetű ital, amelyet a kávébabból készítenek."
⋮----
extracted_entity = program(description=description).entity
⋮----
@pytest.mark.parametrize("module", [dspy.Predict, dspy.ChainOfThought])
@pytest.mark.reliability
def test_tool_calling_with_literals(module)
⋮----
next_tool_names = [
⋮----
class ToolCalling(dspy.Signature)
⋮----
"""
        Given the fields question, produce the fields response.
        You will be given question and your goal is to finish with response.
        To do this, you will interleave Thought, Tool Name, and Tool Args, and receive a resulting Observation.
        """
⋮----
trajectory: str = dspy.InputField()
next_thought: str = dspy.OutputField()
next_tool_name: Literal[
next_tool_args: dict[str, Any] = dspy.OutputField()
response_status: Literal["success", "error", "pending"] = dspy.OutputField()
user_intent: Literal["informational", "transactional", "exploratory"] = dspy.OutputField()
⋮----
program = dspy.Predict(ToolCalling)
prediction = program(
</file>

<file path="tests/reliability/utils.py">
JUDGE_MODEL_NAME = "judge"
⋮----
"""
    With the help of an LLM judge, assert that the specified output of a DSPy program is correct,
    according to the specified grading guidelines.

    Args:
        program_input: The input to a DSPy program.
        program_output: The output from the DSPy program.
        grading_guidelines: The grading guidelines for judging the correctness of the
                            program output.
    """
⋮----
grading_guidelines = [grading_guidelines]
⋮----
judge_response = _get_judge_program()(
⋮----
def known_failing_models(models: list[str])
⋮----
"""
    Decorator to allow specific test cases to fail for certain models. This is useful when a
    model is known to be unable to perform a specific task (e.g. output formatting with complex
    schemas) to the required standard.

    Args:
        models: List of model names for which the test case is allowed to fail.
    """
⋮----
def decorator(test_func)
⋮----
@wraps(test_func)
        def wrapper(*args, **kwargs)
⋮----
@contextmanager
def judge_dspy_configuration(**extra_judge_config)
⋮----
"""
    Context manager to temporarily configure the DSPy to use the the judge model
    from `reliability_conf.yaml`.

    Args:
        extra_judge_config: Extra configuration parameters to apply on top of the judge model
                            configuration from `reliability_conf.yaml`.
    """
module_dir = os.path.dirname(os.path.abspath(__file__))
conf_path = os.path.join(module_dir, "reliability_conf.yaml")
reliability_conf = parse_reliability_conf_yaml(conf_path)
adapter = get_adapter(reliability_conf)
judge_params = reliability_conf.models.get(JUDGE_MODEL_NAME)
⋮----
def _get_judge_program()
⋮----
class JudgeResponse(pydantic.BaseModel)
⋮----
correct: bool = pydantic.Field("Whether or not the judge output is correct")
justification: str = pydantic.Field("Justification for the correctness of the judge output")
⋮----
class JudgeSignature(dspy.Signature)
⋮----
"""
        Given the input and output of an AI program, determine whether the output is correct,
        according to the provided guidelines. Only consider the guidelines when determining correctness.

        Outputs often look like Python objects. Analyze these objects very carefully to make sure
        you don't miss certain fields or values.
        """
⋮----
program_input: str = dspy.InputField(description="The input to an AI program / model that is being judged")
program_output: str = dspy.InputField(
guidelines: str = dspy.InputField(
judge_response: JudgeResponse = dspy.OutputField()
⋮----
class ReliabilityTestConf(pydantic.BaseModel)
⋮----
adapter: str
models: dict[str, Any]
⋮----
@lru_cache(maxsize=None)
def parse_reliability_conf_yaml(conf_file_path: str) -> ReliabilityTestConf
⋮----
conf = yaml.safe_load(file)
⋮----
model_dict = {}
⋮----
model_name = conf_entry.get("model_name")
⋮----
litellm_params = conf_entry.get("litellm_params")
⋮----
adapter = conf.get("adapter")
⋮----
def get_adapter(reliability_conf: ReliabilityTestConf) -> dspy.Adapter
</file>

<file path="tests/retrievers/test_colbertv2.py">
def test_get_request_raises_on_server_error()
⋮----
mock_response = MagicMock()
⋮----
def test_post_request_raises_on_server_error()
⋮----
def test_get_request_success()
⋮----
result = colbertv2_get_request_v2("http://test3", "query", k=3)
⋮----
def test_post_request_success()
⋮----
result = colbertv2_post_request_v2("http://test4", "query", k=3)
</file>

<file path="tests/retrievers/test_embeddings.py">
def dummy_corpus()
⋮----
def dummy_embedder(texts)
⋮----
embeddings = []
⋮----
def test_embeddings_basic_search()
⋮----
corpus = dummy_corpus()
embedder = dummy_embedder
⋮----
retriever = Embeddings(corpus=corpus, embedder=embedder, k=1)
⋮----
query = "I saw a dog running."
result = retriever(query)
⋮----
def test_embeddings_multithreaded_search()
⋮----
queries = [
⋮----
def worker(query_text, expected_passage)
⋮----
result = retriever(query_text)
⋮----
futures = [executor.submit(worker, q, expected) for q, expected in queries]
# Results will be in original order
results = [f.result() for f in futures]
⋮----
def test_embeddings_save_load()
⋮----
original_retriever = Embeddings(corpus=corpus, embedder=embedder, k=2, normalize=False, brute_force_threshold=1000)
⋮----
save_path = os.path.join(temp_dir, "test_embeddings")
⋮----
# Save original
⋮----
# Verify files were created
⋮----
assert not os.path.exists(os.path.join(save_path, "faiss_index.bin"))  # No FAISS for small corpus
⋮----
# Load into new instance
new_retriever = Embeddings(corpus=["dummy"], embedder=embedder, k=1, normalize=True, brute_force_threshold=500)
⋮----
# Verify configuration was loaded correctly
⋮----
# Verify search results are preserved
query = "cat sitting"
original_result = original_retriever(query)
loaded_result = new_retriever(query)
⋮----
def test_embeddings_from_saved()
⋮----
original_retriever = Embeddings(corpus=corpus, embedder=embedder, k=3, normalize=True, brute_force_threshold=1000)
⋮----
loaded_retriever = Embeddings.from_saved(save_path, embedder)
⋮----
def test_embeddings_load_nonexistent_path()
</file>

<file path="tests/signatures/test_adapter_file.py">
@pytest.fixture
def sample_text_file()
⋮----
tmp_file_path = tmp_file.name
⋮----
def count_messages_with_file_pattern(messages)
⋮----
pattern = {"type": "file", "file": lambda x: isinstance(x, dict)}
⋮----
def check_pattern(obj, pattern)
⋮----
def count_patterns(obj, pattern)
⋮----
count = 0
⋮----
def setup_predictor(signature, expected_output)
⋮----
lm = DummyLM([expected_output])
⋮----
def test_file_from_local_path(sample_text_file)
⋮----
file_obj = dspy.File.from_path(sample_text_file)
⋮----
def test_file_from_path_method(sample_text_file)
⋮----
def test_file_from_path_with_custom_filename(sample_text_file)
⋮----
file_obj = dspy.File.from_path(sample_text_file, filename="custom.txt")
⋮----
def test_file_from_bytes()
⋮----
file_bytes = b"Test file content"
file_obj = dspy.File.from_bytes(file_bytes)
⋮----
def test_file_from_bytes_with_filename()
⋮----
file_obj = dspy.File.from_bytes(file_bytes, filename="test.txt")
⋮----
def test_file_from_file_id()
⋮----
file_obj = dspy.File.from_file_id("file-abc123")
⋮----
def test_file_from_file_id_with_filename()
⋮----
file_obj = dspy.File.from_file_id("file-abc123", filename="document.pdf")
⋮----
def test_file_from_dict_with_file_data()
⋮----
file_obj = dspy.File(file_data="data:text/plain;base64,dGVzdA==", filename="test.txt")
⋮----
def test_file_from_dict_with_file_id()
⋮----
file_obj = dspy.File(file_id="file-xyz789")
⋮----
def test_file_format_with_file_data()
⋮----
file_obj = dspy.File.from_bytes(b"test", filename="test.txt")
formatted = file_obj.format()
⋮----
def test_file_format_with_file_id()
⋮----
file_obj = dspy.File.from_file_id("file-123")
⋮----
def test_file_repr_with_file_data()
⋮----
file_obj = dspy.File.from_bytes(b"Test content", filename="test.txt")
repr_str = repr(file_obj)
⋮----
def test_file_repr_with_file_id()
⋮----
file_obj = dspy.File.from_file_id("file-abc", filename="doc.pdf")
⋮----
def test_file_str()
⋮----
file_obj = dspy.File.from_bytes(b"test")
str_repr = str(file_obj)
⋮----
def test_encode_file_to_dict_from_path(sample_text_file)
⋮----
result = encode_file_to_dict(sample_text_file)
⋮----
def test_encode_file_to_dict_from_bytes()
⋮----
result = encode_file_to_dict(b"test content")
⋮----
def test_invalid_file_string()
⋮----
def test_invalid_dict()
⋮----
def test_file_in_signature(sample_text_file)
⋮----
signature = "document: dspy.File -> summary: str"
expected = {"summary": "This is a summary"}
⋮----
result = predictor(document=file_obj)
⋮----
def test_file_list_in_signature(sample_text_file)
⋮----
class FileListSignature(dspy.Signature)
⋮----
documents: list[dspy.File] = dspy.InputField()
summary: str = dspy.OutputField()
⋮----
expected = {"summary": "Multiple files"}
⋮----
files = [
result = predictor(documents=files)
⋮----
def test_optional_file_field()
⋮----
class OptionalFileSignature(dspy.Signature)
⋮----
document: dspy.File | None = dspy.InputField()
output: str = dspy.OutputField()
⋮----
result = predictor(document=None)
⋮----
def test_save_load_file_signature(sample_text_file)
⋮----
examples = [dspy.Example(document=file_obj, summary="Test summary")]
⋮----
optimizer = dspy.teleprompt.LabeledFewShot(k=1)
compiled_predictor = optimizer.compile(student=predictor, trainset=examples, sample=False)
⋮----
loaded_predictor = dspy.Predict(signature)
⋮----
def test_file_frozen()
⋮----
def test_file_with_all_fields()
⋮----
file_data_uri = "data:text/plain;base64,dGVzdA=="
file_obj = dspy.File(file_data=file_data_uri, file_id="file-123", filename="test.txt")
⋮----
def test_file_path_not_found()
⋮----
def test_file_custom_mime_type(sample_text_file)
⋮----
file_obj = dspy.File.from_path(sample_text_file, mime_type="text/custom")
⋮----
def test_file_from_bytes_custom_mime()
⋮----
file_obj = dspy.File.from_bytes(b"audio data", mime_type="audio/mp3")
⋮----
def test_file_data_uri_in_format()
⋮----
file_obj = dspy.File.from_bytes(b"test", filename="test.txt", mime_type="text/plain")
</file>

<file path="tests/signatures/test_adapter_image.py">
@pytest.fixture
def sample_pil_image()
⋮----
"""Fixture to provide a sample image for testing"""
url = "https://images.dog.ceo/breeds/dane-great/n02109047_8912.jpg"
response = requests.get(url)
⋮----
@pytest.fixture
def sample_dspy_image_download()
⋮----
@pytest.fixture
def sample_url()
⋮----
@pytest.fixture
def sample_dspy_image_no_download()
⋮----
def count_messages_with_image_url_pattern(messages)
⋮----
pattern = {"type": "image_url", "image_url": {"url": lambda x: isinstance(x, str)}}
⋮----
def check_pattern(obj, pattern)
⋮----
def count_patterns(obj, pattern)
⋮----
count = 0
⋮----
def setup_predictor(signature, expected_output)
⋮----
"""Helper to set up a predictor with DummyLM"""
lm = DummyLM([expected_output])
⋮----
def test_basic_image_operations(test_case)
⋮----
"""Consolidated test for basic image operations"""
⋮----
# Convert string URLs to dspy.Image objects
inputs = {
⋮----
result = predictor(**inputs)
⋮----
# Check result based on output field name
output_field = next(f for f in ["probabilities", "generated_code", "bboxes", "captions"] if hasattr(result, f))
⋮----
"""Test different input formats for image fields"""
signature = "image: dspy.Image, class_labels: list[str] -> probabilities: dict[str, float]"
expected = {"probabilities": {"dog": 0.8, "cat": 0.1, "bird": 0.1}}
⋮----
input_map = {
⋮----
actual_input = input_map[image_input]
# TODO(isaacbmiller): Support the cases without direct dspy.Image coercion
⋮----
result = predictor(image=actual_input, class_labels=["dog", "cat", "bird"])
⋮----
def test_predictor_save_load(sample_url, sample_pil_image)
⋮----
"""Test saving and loading predictors with image fields"""
signature = "image: dspy.Image -> caption: str"
examples = [
⋮----
optimizer = dspy.teleprompt.LabeledFewShot(k=1)
compiled_predictor = optimizer.compile(student=predictor, trainset=examples, sample=False)
⋮----
loaded_predictor = dspy.Predict(signature)
⋮----
def test_save_load_complex_default_types()
⋮----
"""Test saving and loading predictors with complex default types (lists of images)"""
⋮----
class ComplexTypeSignature(dspy.Signature)
⋮----
image_list: list[dspy.Image] = dspy.InputField(desc="A list of images")
caption: str = dspy.OutputField(desc="A caption for the image list")
⋮----
loaded_predictor = dspy.Predict(ComplexTypeSignature)
⋮----
result = loaded_predictor(**examples[0].inputs())
⋮----
class BasicImageSignature(dspy.Signature)
⋮----
"""Basic signature with a single image input"""
⋮----
image: dspy.Image = dspy.InputField()
output: str = dspy.OutputField()
⋮----
class ImageListSignature(dspy.Signature)
⋮----
"""Signature with a list of images input"""
⋮----
image_list: list[dspy.Image] = dspy.InputField()
⋮----
def test_save_load_complex_types(test_case)
⋮----
"""Test saving and loading predictors with complex types"""
signature_cls = test_case["signature_class"]
⋮----
# Convert string URLs to dspy.Image objects in input
processed_input = {}
⋮----
# Create example and predictor
examples = [dspy.Example(**processed_input, **test_case["expected"]).with_inputs(*processed_input.keys())]
⋮----
# Test save and load
⋮----
loaded_predictor = dspy.Predict(signature_cls)
⋮----
# Run prediction
result = loaded_predictor(**processed_input)
⋮----
# Verify output matches expected
⋮----
# Verify correct number of image URLs in messages
⋮----
def test_save_load_pydantic_model()
⋮----
"""Test saving and loading predictors with pydantic models"""
⋮----
class ImageModel(pydantic.BaseModel)
⋮----
image: dspy.Image
image_list: list[dspy.Image] | None = None
output: str
⋮----
class PydanticSignature(dspy.Signature)
⋮----
model_input: ImageModel = dspy.InputField()
⋮----
# Create model instance
model_input = ImageModel(
⋮----
examples = [dspy.Example(model_input=model_input, output="Multiple photos").with_inputs("model_input")]
⋮----
loaded_predictor = dspy.Predict(PydanticSignature)
⋮----
result = loaded_predictor(model_input=model_input)
⋮----
def test_optional_image_field()
⋮----
"""Test that optional image fields are not required"""
⋮----
class OptionalImageSignature(dspy.Signature)
⋮----
image: dspy.Image | None = dspy.InputField()
⋮----
result = predictor(image=None)
⋮----
def test_pdf_url_support()
⋮----
"""Test support for PDF files from URLs"""
pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
⋮----
# Create a dspy.Image object from the PDF URL with download=True
pdf_image = dspy.Image(pdf_url, download=True)
⋮----
# The data URI should contain application/pdf in the MIME type
⋮----
# Test using it in a predictor
class PDFSignature(dspy.Signature)
⋮----
document: dspy.Image = dspy.InputField(desc="A PDF document")
summary: str = dspy.OutputField(desc="A summary of the PDF")
⋮----
result = predictor(document=pdf_image)
⋮----
# Ensure the URL was properly expanded in messages
messages_str = str(lm.history[-1]["messages"])
⋮----
def test_different_mime_types()
⋮----
"""Test support for different file types and MIME type detection"""
# Test with various file types
file_urls = {
⋮----
expected_mime_types = {
⋮----
# Download and encode
encoded = encode_image(url, download_images=True)
⋮----
# Check for correct MIME type in the encoded data - using 'in' instead of startswith
# to account for possible parameters in the MIME type
⋮----
def test_mime_type_from_response_headers()
⋮----
"""Test that MIME types from response headers are correctly used"""
# This URL returns proper Content-Type header
⋮----
# Make an actual request to get the content type from headers
response = requests.get(pdf_url)
expected_mime_type = response.headers.get("Content-Type", "")
⋮----
# Should be application/pdf or similar
⋮----
# Encode with download to test MIME type from headers
encoded = encode_image(pdf_url, download_images=True)
⋮----
# The encoded data should contain the correct MIME type
⋮----
def test_pdf_from_file()
⋮----
"""Test handling a PDF file from disk"""
# Download a PDF to a temporary file
⋮----
tmp_file_path = tmp_file.name
⋮----
# Create a dspy.Image from the file
pdf_image = dspy.Image(tmp_file_path)
⋮----
# The constructor encodes the file into a data URI we can inspect directly
⋮----
# Test the image in a predictor
class FilePDFSignature(dspy.Signature)
⋮----
document: dspy.Image = dspy.InputField(desc="A PDF document from file")
⋮----
# Clean up the temporary file
⋮----
def test_image_repr()
⋮----
"""Test string representation of Image objects"""
url_image = dspy.Image("https://example.com/dog.jpg")
⋮----
sample_pil = PILImage.new("RGB", (60, 30), color="red")
pil_image = dspy.Image(sample_pil)
⋮----
def test_from_methods_warn(tmp_path)
⋮----
"""Deprecated from_* methods emit warnings"""
tmp_file = tmp_path / "test.png"
⋮----
sample_pil = PILImage.new("RGB", (10, 10), color="blue")
⋮----
def test_invalid_string_format()
⋮----
"""Test that invalid string formats raise a ValueError"""
invalid_string = "this_is_not_a_url_or_file"
⋮----
# Should raise a ValueError and not pass the string through
⋮----
image = dspy.Image(invalid_string)
⋮----
def test_pil_image_with_download_parameter()
⋮----
"""Test behavior when PIL image is passed with download=True"""
⋮----
# PIL image should be encoded regardless of download parameter
image_no_download = dspy.Image(sample_pil)
image_with_download = dspy.Image(sample_pil, download=True)
⋮----
# Both should result in base64 encoded data URIs
⋮----
# They should be identical since PIL images are always encoded
</file>

<file path="tests/signatures/test_custom_types.py">
def test_basic_custom_type_resolution()
⋮----
"""Test basic custom type resolution with both explicit and automatic mapping."""
class CustomType(pydantic.BaseModel)
⋮----
value: str
⋮----
# Custom types can be explicitly mapped
explicit_sig = Signature(
⋮----
# Custom types can also be auto-resolved from caller's scope
auto_sig = Signature("input: CustomType -> output: str")
⋮----
def test_type_alias_for_nested_types()
⋮----
"""Test using type aliases for nested types."""
class Container
⋮----
class NestedType(pydantic.BaseModel)
⋮----
NestedType = Container.NestedType
alias_sig = Signature("input: str -> output: NestedType")
⋮----
class Container2
⋮----
class Query(pydantic.BaseModel)
⋮----
text: str
class Score(pydantic.BaseModel)
⋮----
score: float
⋮----
signature = dspy.Signature("query: Container2.Query -> score: Container2.Score")
⋮----
class GlobalCustomType(pydantic.BaseModel)
⋮----
"""A type defined at module level for testing module-level resolution."""
⋮----
notes: str = ""
⋮----
def test_module_level_type_resolution()
⋮----
"""Test resolution of types defined at module level."""
# Module-level types can be auto-resolved
sig = Signature("name: str -> result: GlobalCustomType")
⋮----
# Create module-level nested class for testing
class OuterContainer
⋮----
class InnerType(pydantic.BaseModel)
⋮----
name: str
value: int
⋮----
def test_recommended_patterns()
⋮----
"""Test recommended patterns for working with custom types in signatures."""
⋮----
# PATTERN 1: Local type with auto-resolution
class LocalType(pydantic.BaseModel)
⋮----
sig1 = Signature("input: str -> output: LocalType")
⋮----
# PATTERN 2: Module-level type with auto-resolution
sig2 = Signature("input: str -> output: GlobalCustomType")
⋮----
# PATTERN 3: Nested type with dot notation
sig3 = Signature("input: str -> output: OuterContainer.InnerType")
⋮----
# PATTERN 4: Nested type using alias
InnerTypeAlias = OuterContainer.InnerType
sig4 = Signature("input: str -> output: InnerTypeAlias")
⋮----
# PATTERN 5: Nested type with dot notation
sig5 = Signature("input: str -> output: OuterContainer.InnerType")
⋮----
def test_expected_failure()
⋮----
# InnerType DNE when not OuterContainer.InnerTypes, so this type shouldnt be resolved
⋮----
def test_module_type_resolution()
⋮----
class TestModule(dspy.Module)
⋮----
def __init__(self)
⋮----
def predict(self, input: str) -> str
⋮----
module = TestModule()
sig = module.predict.signature
⋮----
sig = Signature("input: CustomType -> output: str", custom_types={"CustomType": CustomType})
⋮----
sig = Signature("input: CustomType -> output: str")
</file>

<file path="tests/signatures/test_signature.py">
def test_field_types_and_custom_attributes()
⋮----
class TestSignature(Signature)
⋮----
"""Instructions"""
⋮----
input1: str = InputField()
input2: int = InputField()
output1: list[str] = OutputField()
output2 = OutputField()
⋮----
def test_no_input_output()
⋮----
input1: str
⋮----
def test_no_input_output2()
⋮----
input1: str = pydantic.Field()
⋮----
def test_all_fields_have_prefix()
⋮----
input = InputField(prefix="Modified:")
output = OutputField()
⋮----
def test_signature_parsing()
⋮----
signature = Signature("input1, input2 -> output")
⋮----
def test_with_signature()
⋮----
signature1 = Signature("input1, input2 -> output")
signature2 = signature1.with_instructions("This is a test")
⋮----
def test_with_updated_field()
⋮----
signature2 = signature1.with_updated_fields("input1", prefix="Modified:")
⋮----
def test_empty_signature()
⋮----
def test_instructions_signature()
⋮----
def test_signature_instructions()
⋮----
sig1 = Signature("input1 -> output1", instructions="This is a test")
⋮----
sig2 = Signature("input1 -> output1", "This is a test")
⋮----
def test_signature_instructions_none()
⋮----
sig1 = Signature("a, b -> c")
⋮----
def test_signature_from_dict()
⋮----
signature = Signature(
⋮----
def test_signature_equality()
⋮----
sig1 = Signature("input1 -> output1")
sig2 = Signature("input1 -> output1")
⋮----
def test_signature_inequality()
⋮----
sig2 = Signature("input2 -> output2")
⋮----
def test_equality_format()
⋮----
input = InputField(format=lambda x: x)
⋮----
def test_signature_reverse()
⋮----
sig = Signature("input1 -> output1")
⋮----
def test_insert_field_at_various_positions()
⋮----
class InitialSignature(Signature)
⋮----
output1: int = OutputField()
⋮----
s1 = InitialSignature.prepend("new_input_start", InputField(), str)
s2 = InitialSignature.append("new_input_end", InputField(), str)
assert "new_input_start" == list(s1.input_fields.keys())[0]  # noqa: RUF015
⋮----
s3 = InitialSignature.prepend("new_output_start", OutputField(), str)
s4 = InitialSignature.append("new_output_end", OutputField(), str)
assert "new_output_start" == list(s3.output_fields.keys())[0]  # noqa: RUF015
⋮----
def test_order_preserved_with_mixed_annotations()
⋮----
class ExampleSignature(dspy.Signature)
⋮----
text: str = dspy.InputField()
output = dspy.OutputField()
pass_evaluation: bool = dspy.OutputField()
⋮----
expected_order = ["text", "output", "pass_evaluation"]
actual_order = list(ExampleSignature.fields.keys())
⋮----
def test_infer_prefix()
⋮----
def test_insantiating()
⋮----
sig = Signature("input -> output")
⋮----
value = sig(input="test", output="test")
⋮----
def test_insantiating2()
⋮----
class SubSignature(Signature)
⋮----
input = InputField()
⋮----
value = SubSignature(input="test", output="test")
⋮----
def test_multiline_instructions()
⋮----
lm = DummyLM([{"output": "short answer"}])
⋮----
class MySignature(Signature)
⋮----
"""First line
        Second line
            Third line"""
⋮----
predictor = dspy.Predict(MySignature)
⋮----
def test_dump_and_load_state()
⋮----
class CustomSignature(dspy.Signature)
⋮----
"""I am just an instruction."""
⋮----
sentence = dspy.InputField(desc="I am an innocent input!")
sentiment = dspy.OutputField()
⋮----
state = CustomSignature.dump_state()
expected = {
⋮----
class CustomSignature2(dspy.Signature)
⋮----
"""I am a malicious instruction."""
⋮----
sentence = dspy.InputField(desc="I am an malicious input!")
⋮----
# Overwrite the state with the state of CustomSignature.
loaded_signature = CustomSignature2.load_state(state)
⋮----
# After `load_state`, the state should be the same as CustomSignature.
⋮----
# CustomSignature2 should not have been modified.
⋮----
def test_typed_signatures_basic_types()
⋮----
sig = Signature("input1: int, input2: str -> output: float")
⋮----
def test_typed_signatures_generics()
⋮----
sig = Signature(
⋮----
def test_typed_signatures_unions_and_optionals()
⋮----
# Optional[str] is actually Union[str, None]
# Depending on the environment, it might resolve to Union[str, None] or Optional[str], either is correct.
# We'll just check for a Union containing str and NoneType:
input_opt_annotation = sig.input_fields["input_opt"].annotation
⋮----
input_union_annotation = sig.input_fields["input_union"].annotation
⋮----
output_union_annotation = sig.output_fields["output_union"].annotation
⋮----
def test_typed_signatures_any()
⋮----
sig = Signature("input_any: Any -> output_any: Any")
⋮----
def test_typed_signatures_nested()
⋮----
input_nested_ann = sig.input_fields["input_nested"].annotation
⋮----
union_arg = input_nested_ann.__args__[0]
⋮----
output_nested_ann = sig.output_fields["output_nested"].annotation
⋮----
# The second arg is Optional[float], which is Union[float, None]
second_arg = output_nested_ann.__args__[1]
⋮----
# The third arg is list[str]
third_arg = output_nested_ann.__args__[2]
⋮----
def test_typed_signatures_from_dict()
⋮----
fields = {
sig = Signature(fields)
⋮----
def test_typed_signatures_complex_combinations()
⋮----
input_complex_ann = sig.input_fields["input_complex"].annotation
⋮----
# value_arg: list[Optional[Tuple[int, str]]]
⋮----
inner_union = value_arg.__args__[0]
# inner_union should be Optional[Tuple[int, str]]
# which is Union[Tuple[int, str], None]
⋮----
tuple_type = [t for t in inner_union.__args__ if t != type(None)][0]  # noqa: RUF015
⋮----
output_complex_ann = sig.output_fields["output_complex"].annotation
⋮----
possible_args = set(output_complex_ann.__args__)
# Expecting list[str] and dict[str, Any]
# Because sets don't preserve order, just check membership.
# Find the list[str] arg
list_arg = next(a for a in possible_args if getattr(
dict_arg = next(a for a in possible_args if getattr(
⋮----
def test_make_signature_from_string()
⋮----
def test_signature_field_with_constraints()
⋮----
inputs: str = InputField()
outputs1: str = OutputField(min_length=5, max_length=10)
outputs2: int = OutputField(ge=5, le=10)
⋮----
output1_constraints = MySignature.output_fields["outputs1"].json_schema_extra["constraints"]
⋮----
output2_constraints = MySignature.output_fields["outputs2"].json_schema_extra["constraints"]
⋮----
def test_basic_custom_type()
⋮----
class CustomType(pydantic.BaseModel)
⋮----
value: str
⋮----
test_signature = dspy.Signature(
⋮----
lm = DummyLM([{"output": "processed"}])
⋮----
custom_obj = CustomType(value="test")
pred = dspy.Predict(test_signature)(input=custom_obj)
⋮----
def test_custom_type_from_different_module()
⋮----
test_signature = dspy.Signature("input: Path -> output: str")
⋮----
lm = DummyLM([{"output": "/test/path"}])
⋮----
path_obj = Path("/test/path")
pred = dspy.Predict(test_signature)(input=path_obj)
⋮----
def test_pep604_union_type_inline()
⋮----
# input1 and input2 test that both 'T | None' and 'None | T' are interpreted as Optional types,
# regardless of the order of None in the union expression.
⋮----
input1_annotation = sig.input_fields["input1"].annotation
⋮----
input2_annotation = sig.input_fields["input2"].annotation
⋮----
def test_pep604_union_type_inline_equivalence()
⋮----
sig1 = Signature("input: str | None -> output: int | str")
sig2 = Signature("input: Optional[str] -> output: Union[int, str]")
⋮----
# PEP 604 union types in inline signatures should be equivalent to Optional and Union types
⋮----
# Check that the annotations are equivalent
⋮----
def test_pep604_union_type_inline_nested()
⋮----
input_annotation = sig.input_fields["input"].annotation
⋮----
# Check for the correct union: Union[str, int, float, NoneType]
⋮----
def test_pep604_union_type_class_nested()
⋮----
class Sig1(Signature)
⋮----
input: str | (int | float) | None = InputField()
output: str = OutputField()
⋮----
input_annotation = Sig1.input_fields["input"].annotation
⋮----
# Check for the correct union: UnionType[str, int, float, NoneType]
⋮----
def test_pep604_union_type_class_equivalence()
⋮----
input: str | None = InputField()
output: int | str = OutputField()
⋮----
class Sig2(Signature)
⋮----
output: Union[int, str] = OutputField()  # noqa: UP007
⋮----
# PEP 604 union types in class signatures should be equivalent to Optional and Union types
⋮----
# Check that the pep604 annotations are of type UnionType
⋮----
def test_pep604_union_type_insert()
⋮----
class PEP604Signature(Signature)
⋮----
# This test ensures that inserting a field into a signature with a PEP 604 UnionType works
⋮----
# Insert a new input field at the start
NewSig = PEP604Signature.prepend("new_input", InputField(), float | int)
⋮----
new_input_annotation = NewSig.input_fields["new_input"].annotation
⋮----
# The original union type field should still be present and correct
input_annotation = NewSig.input_fields["input"].annotation
output_annotation = NewSig.output_fields["output"].annotation
⋮----
def test_pep604_union_type_with_custom_types()
⋮----
pred = dspy.Predict(sig)(input=custom_obj)
</file>

<file path="tests/streaming/test_streaming.py">
@pytest.mark.anyio
async def test_streamify_yields_expected_response_chunks(litellm_test_server)
⋮----
lm = dspy.LM(
⋮----
class TestSignature(dspy.Signature)
⋮----
input_text: str = dspy.InputField()
output_text: str = dspy.OutputField()
⋮----
program = dspy.streamify(dspy.Predict(TestSignature))
output_stream1 = program(input_text="Test")
output_chunks1 = [chunk async for chunk in output_stream1]
last_chunk1 = output_chunks1[-1]
⋮----
output_stream2 = program(input_text="Test")
output_chunks2 = [chunk async for chunk in output_stream2]
# Since the input is cached, only one chunk should be
# yielded containing the prediction
⋮----
last_chunk2 = output_chunks2[-1]
⋮----
@pytest.mark.anyio
async def test_streaming_response_yields_expected_response_chunks(litellm_test_server)
⋮----
output_stream_from_program = streaming_response(program(input_text="Test"))
output_stream_for_server_response = streaming_response(output_stream_from_program)
output_chunks = [chunk async for chunk in output_stream_for_server_response]
⋮----
@pytest.mark.anyio
async def test_default_status_streaming()
⋮----
class MyProgram(dspy.Module)
⋮----
def __init__(self)
⋮----
def __call__(self, x: str)
⋮----
question = self.generate_question(x=x)
⋮----
lm = dspy.utils.DummyLM([{"answer": "red"}, {"answer": "blue"}])
⋮----
program = dspy.streamify(MyProgram())
output = program("sky")
⋮----
status_messages = []
⋮----
@pytest.mark.anyio
async def test_custom_status_streaming()
⋮----
class MyStatusMessageProvider(StatusMessageProvider)
⋮----
def tool_start_status_message(self, instance, inputs)
⋮----
def tool_end_status_message(self, outputs)
⋮----
def module_start_status_message(self, instance, inputs)
⋮----
program = dspy.streamify(MyProgram(), status_message_provider=MyStatusMessageProvider())
⋮----
@pytest.mark.anyio
async def test_concurrent_status_message_providers()
⋮----
class MyStatusMessageProvider1(StatusMessageProvider)
⋮----
class MyStatusMessageProvider2(StatusMessageProvider)
⋮----
# Store the original callbacks to verify they're not modified
original_callbacks = list(dspy.settings.callbacks)
⋮----
lm = dspy.utils.DummyLM([{"answer": "red"}, {"answer": "blue"}, {"answer": "green"}, {"answer": "yellow"}])
⋮----
# Results storage for each thread
results = {}
⋮----
async def run_with_provider1()
⋮----
program = dspy.streamify(MyProgram(), status_message_provider=MyStatusMessageProvider1())
⋮----
async def run_with_provider2()
⋮----
program = dspy.streamify(MyProgram(), status_message_provider=MyStatusMessageProvider2())
output = program("ocean")
⋮----
# Run both tasks concurrently
⋮----
# Verify provider1 got its expected messages
⋮----
# Verify provider2 got its expected messages
⋮----
# Verify that the global callbacks were not modified
⋮----
@pytest.mark.llm_call
@pytest.mark.anyio
async def test_stream_listener_chat_adapter(lm_for_test)
⋮----
def __call__(self, x: str, **kwargs)
⋮----
answer = self.predict1(question=x, **kwargs)
judgement = self.predict2(question=x, answer=answer, **kwargs)
⋮----
my_program = MyProgram()
program = dspy.streamify(
# Turn off the cache to ensure the stream is produced.
⋮----
output = program(x="why did a chicken cross the kitchen?")
all_chunks = []
⋮----
# The last chunk can be from either predictor because sometimes small LMs miss the `[[ ## completed ## ]]` marker,
# which results in an extra chunk that flushes out the buffer.
⋮----
@pytest.mark.anyio
async def test_default_status_streaming_in_async_program()
⋮----
async def acall(self, x: str)
⋮----
question = await self.generate_question.acall(x=x)
⋮----
program = dspy.streamify(MyProgram(), is_async_program=True)
⋮----
@pytest.mark.llm_call
@pytest.mark.anyio
async def test_stream_listener_json_adapter(lm_for_test)
⋮----
@pytest.mark.anyio
async def test_streaming_handles_space_correctly()
⋮----
my_program = dspy.Predict("question->answer")
⋮----
async def gpt_4o_mini_stream(*args, **kwargs)
⋮----
output = program(question="What is the capital of France?")
⋮----
@pytest.mark.llm_call
def test_sync_streaming(lm_for_test)
⋮----
def test_sync_status_streaming()
⋮----
sync_output = dspy.streaming.apply_sync_streaming(output)
⋮----
@pytest.mark.anyio
async def test_stream_listener_returns_correct_chunk_chat_adapter()
⋮----
def forward(self, question, **kwargs)
⋮----
answer = self.predict1(question=question, **kwargs).answer
judgement = self.predict2(question=question, answer=answer, **kwargs)
⋮----
async def gpt_4o_mini_stream_1(*args, **kwargs)
⋮----
# Recorded streaming from openai/gpt-4o-mini
⋮----
async def gpt_4o_mini_stream_2()
⋮----
stream_generators = [gpt_4o_mini_stream_1, gpt_4o_mini_stream_2]
⋮----
async def completion_side_effect(*args, **kwargs)
⋮----
return stream_generators.pop(0)()  # return new async generator instance
⋮----
output = program(question="why did a chicken cross the kitchen?")
⋮----
@pytest.mark.anyio
async def test_stream_listener_returns_correct_chunk_json_adapter()
⋮----
async def gpt_4o_mini_stream_2(*args, **kwargs)
⋮----
@pytest.mark.anyio
async def test_stream_listener_returns_correct_chunk_chat_adapter_untokenized_stream()
⋮----
async def gemini_stream_1(*args, **kwargs)
⋮----
async def gemini_stream_2(*args, **kwargs)
⋮----
@pytest.mark.anyio
async def test_stream_listener_missing_completion_marker_chat_adapter()
⋮----
"""Test that streaming works correctly when LLM response omits a final completion marker.

    This test verifies that:
    1. All tokens are yielded including those in the buffer
    2. The last chunk is properly marked with is_last_chunk=True
    3. No tokens are lost when the completion marker is missing
    """
⋮----
async def incomplete_stream(*args, **kwargs)
⋮----
"""Stream that includes start marker but MISSING completion marker"""
# Start marker
⋮----
# Content tokens - more than 10 to ensure buffering happens
⋮----
# NO COMPLETION MARKER
⋮----
output = program(question="Test question")
⋮----
final_prediction = None
⋮----
final_prediction = value
⋮----
full_content = "".join([chunk.chunk for chunk in all_chunks])
expected_content = "This is a test response with many tokens to ensure buffering works correctly."
⋮----
@pytest.mark.anyio
async def test_stream_listener_returns_correct_chunk_json_adapter_untokenized_stream()
⋮----
@pytest.mark.anyio
async def test_status_message_non_blocking()
⋮----
def dummy_tool()
⋮----
program = dspy.streamify(MyProgram(), status_message_provider=StatusMessageProvider())
⋮----
timestamps = []
⋮----
# timestamps[0]: tool start message
# timestamps[1]: tool end message
# There should be ~1 second delay between the tool start and end messages because we explicitly sleep for 1 second
# in the tool.
⋮----
@pytest.mark.anyio
async def test_status_message_non_blocking_async_program()
⋮----
async def dummy_tool()
⋮----
async def aforward(self, question, **kwargs)
⋮----
program = dspy.streamify(MyProgram(), status_message_provider=StatusMessageProvider(), is_async_program=True)
⋮----
@pytest.mark.anyio
async def test_stream_listener_allow_reuse()
⋮----
stream_generators = [gpt_4o_mini_stream, gpt_4o_mini_stream]
⋮----
concat_message = "".join([chunk.chunk for chunk in all_chunks])
# The listener functions twice.
⋮----
@pytest.mark.anyio
async def test_stream_listener_returns_correct_chunk_xml_adapter()
⋮----
async def xml_stream_1(*args, **kwargs)
⋮----
async def xml_stream_2(*args, **kwargs)
⋮----
stream_generators = [xml_stream_1, xml_stream_2]
⋮----
# Verify answer chunks
answer_chunks = [chunk for chunk in all_chunks if chunk.signature_field_name == "answer"]
⋮----
# Verify judgement chunks
judgement_chunks = [chunk for chunk in all_chunks if chunk.signature_field_name == "judgement"]
⋮----
@pytest.mark.anyio
async def test_streaming_allows_custom_chunk_types()
⋮----
@dataclass
    class CustomChunk
⋮----
text: str
⋮----
async def send_to_stream()
⋮----
chunk = CustomChunk(text="hello")
⋮----
syncified_send_to_stream = syncify(send_to_stream)
⋮----
@pytest.mark.anyio
async def test_streaming_allows_custom_streamable_type()
⋮----
class CustomType(Type)
⋮----
message: str
⋮----
@classmethod
        def is_streamable(cls) -> bool
⋮----
@classmethod
        def adapt_to_native_lm_feature(cls, signature, field_name, lm, lm_kwargs)
⋮----
@classmethod
        def parse_stream_chunk(cls, chunk)
⋮----
@classmethod
        def parse_lm_response(cls, response: dict) -> "CustomType"
⋮----
class CustomSignature(dspy.Signature)
⋮----
question: str = dspy.InputField()
answer: CustomType = dspy.OutputField()
⋮----
async def stream(*args, **kwargs)
⋮----
@pytest.mark.anyio
async def test_streaming_with_citations()
⋮----
class AnswerWithSources(dspy.Signature)
⋮----
"""Answer questions using provided documents with citations."""
⋮----
documents: list[Document] = dspy.InputField()
⋮----
answer: str = dspy.OutputField()
citations: Citations = dspy.OutputField()
⋮----
def forward(self, documents, question, **kwargs)
⋮----
async def citation_stream(*args, **kwargs)
⋮----
# Stream chunks with citation data in provider_specific_fields
# To verify the realistic scenario with more than 10 chunks in the stream, include more than 10 chunks before the citation.
⋮----
# Mock the final response choice to include provider_specific_fields with citations
⋮----
# Create test documents
docs = [Document(data="Water boils at 100°C at standard pressure.", title="Physics Facts")]
⋮----
output = program(documents=docs, question="What temperature does water boil?")
citation_chunks = []
answer_chunks = []
⋮----
# Test that we received citation chunks from streaming
⋮----
citation_chunk = citation_chunks[0]
⋮----
# Verify the answer chunks are correct
⋮----
# Test that prediction contains the expected fields
⋮----
# Test Pydantic Models
class SimpleResponse(pydantic.BaseModel)
⋮----
status: str
⋮----
class NestedResponse(pydantic.BaseModel)
⋮----
title: str
content: dict
metadata: SimpleResponse
⋮----
class ComplexResponse(pydantic.BaseModel)
⋮----
items: list[str]
settings: dict[str, str]
active: bool
⋮----
@pytest.mark.anyio
async def test_chat_adapter_simple_pydantic_streaming()
⋮----
"""Test ChatAdapter streaming with a simple pydantic model."""
⋮----
response: SimpleResponse = dspy.OutputField()
⋮----
async def chat_stream(*args, **kwargs)
⋮----
# Simulate streaming of a pydantic model via ChatAdapter format
⋮----
output = program(question="Say hello")
chunks = []
⋮----
# Verify we got chunks for the pydantic field
⋮----
# Combine all chunks to verify the content
full_content = "".join(chunk.chunk for chunk in chunks)
⋮----
@pytest.mark.anyio
async def test_chat_adapter_with_generic_type_annotation()
⋮----
response: list[str] | int = dspy.OutputField()
⋮----
@pytest.mark.anyio
async def test_chat_adapter_nested_pydantic_streaming()
⋮----
"""Test ChatAdapter streaming with nested pydantic model."""
⋮----
response: NestedResponse = dspy.OutputField()
⋮----
async def nested_stream(*args, **kwargs)
⋮----
output = program(question="Generate nested response")
⋮----
@pytest.mark.anyio
async def test_chat_adapter_mixed_fields_streaming()
⋮----
"""Test ChatAdapter streaming with both pydantic and string fields."""
⋮----
summary: str = dspy.OutputField()
details: SimpleResponse = dspy.OutputField()
⋮----
async def mixed_stream(*args, **kwargs)
⋮----
# First output field (summary - string)
⋮----
# Second output field (details - pydantic)
⋮----
output = program(question="Generate mixed response")
summary_chunks = []
details_chunks = []
⋮----
# Verify both field types were streamed
⋮----
summary_content = "".join(chunk.chunk for chunk in summary_chunks)
details_content = "".join(chunk.chunk for chunk in details_chunks)
⋮----
@pytest.mark.anyio
async def test_json_adapter_simple_pydantic_streaming()
⋮----
"""Test JSONAdapter streaming with a simple pydantic model."""
⋮----
async def json_stream(*args, **kwargs)
⋮----
# Simulate JSON streaming with proper bracket balance tracking
⋮----
)  # Close main object
⋮----
output = program(question="Say hello in JSON")
⋮----
@pytest.mark.anyio
async def test_json_adapter_bracket_balance_detection()
⋮----
"""Test JSONAdapter correctly detects field completion using bracket balance."""
⋮----
response: ComplexResponse = dspy.OutputField()
⋮----
async def complex_json_stream(*args, **kwargs)
⋮----
# Test nested objects and arrays for bracket counting
⋮----
)  # +1 bracket
⋮----
)  # -1 bracket
⋮----
)  # -1 bracket (should end field)
⋮----
output = program(question="Generate complex JSON")
⋮----
# Check that the last chunk is marked as the last
⋮----
@pytest.mark.anyio
async def test_json_adapter_multiple_fields_detection()
⋮----
"""Test JSONAdapter correctly detects when next field starts."""
⋮----
first: SimpleResponse = dspy.OutputField()
second: SimpleResponse = dspy.OutputField()
⋮----
async def multi_field_stream(*args, **kwargs)
⋮----
# First field
⋮----
# Second field starts
⋮----
output = program(question="Generate two responses")
first_chunks = []
second_chunks = []
⋮----
# Verify both fields were detected and streamed
⋮----
first_content = "".join(chunk.chunk for chunk in first_chunks)
second_content = "".join(chunk.chunk for chunk in second_chunks)
⋮----
def test_stream_listener_could_form_end_identifier_chat_adapter()
⋮----
listener = dspy.streaming.StreamListener(signature_field_name="answer")
⋮----
# Should return True for partial bracket sequences
⋮----
# Should return True for partial field names after "[[ ##"
⋮----
# Should return False for text that clearly cannot form the pattern
⋮----
def test_stream_listener_could_form_end_identifier_json_adapter()
⋮----
listener = dspy.streaming.StreamListener(signature_field_name="output")
⋮----
# Should return True for partial quote/brace sequences
⋮----
# Should return False for text that cannot form the pattern
⋮----
def test_stream_listener_could_form_end_identifier_xml_adapter()
⋮----
listener = dspy.streaming.StreamListener(signature_field_name="result")
⋮----
# Should return True for partial closing tag
⋮----
@pytest.mark.anyio
async def test_streaming_reasoning_model()
⋮----
"""Test streaming behavior for reasoning-capable models using dspy.Reasoning.

    This test verifies that:
    1. Reasoning content is extracted from delta.reasoning_content in stream chunks
    2. Reasoning chunks are streamed independently from regular content
    3. The final prediction contains a Reasoning object with the full reasoning content
    """
⋮----
class ReasoningSignature(dspy.Signature)
⋮----
reasoning: dspy.Reasoning = dspy.OutputField()
⋮----
async def reasoning_stream(*args, **kwargs)
⋮----
"""Simulate streaming from a reasoning model like Claude 3.7 Sonnet"""
# Reasoning content comes through delta.reasoning_content
⋮----
# Regular answer content comes through delta.content
⋮----
output = program(question="Why did a chicken cross the kitchen?")
reasoning_chunks = []
⋮----
# Verify reasoning chunks were streamed
⋮----
# Verify answer chunks were streamed
⋮----
full_answer = "".join([chunk.chunk for chunk in answer_chunks])
⋮----
# Verify final prediction has Reasoning object
⋮----
expected_reasoning = (
⋮----
@pytest.mark.anyio
async def test_stream_listener_empty_last_chunk_chat_adapter()
⋮----
"""Test that StreamListener emits an empty chunk marking field end.

    This test covers the scenario where:
    1. Tokens that cannot form the end identifier are immediately yielded
    2. The last chunk received contains only the marker for the next field (or completion marker)
    3. An empty chunk with is_last_chunk=True is emitted to properly mark field end
    """
⋮----
predict = dspy.Predict("question->reasoning, answer")
⋮----
async def mock_stream(*args, **kwargs)
⋮----
output = program(question="Why did the chicken cross the kitchen?")
⋮----
# Find answer and judgement chunks
reasoning_chunks = [c for c in all_chunks if c.signature_field_name == "reasoning"]
answer_chunks = [c for c in all_chunks if c.signature_field_name == "answer"]
⋮----
# The last chunk should be marked as last chunk for both fields.
⋮----
@pytest.mark.anyio
async def test_stream_listener_empty_last_chunk_json_adapter()
⋮----
@pytest.mark.anyio
async def test_streaming_reasoning_fallback()
⋮----
"""Test fallback behavior for non-reasoning models using dspy.Reasoning.

    This test verifies that:
    1. For non-reasoning models, reasoning is treated as a regular string field
    2. Reasoning content is streamed through regular adapter parsing (not reasoning_content)
    3. The Reasoning object is created from the parsed string content
    4. Streaming behavior is identical to regular string fields
    """
⋮----
async def non_reasoning_stream(*args, **kwargs)
⋮----
"""Simulate streaming from a non-reasoning model like GPT-4o-mini.

        The reasoning field is formatted by the adapter as a regular field,
        and content comes through delta.content (not reasoning_content).
        """
# Reasoning field marker (ChatAdapter format)
⋮----
# Reasoning content as regular text
⋮----
# Answer field marker
⋮----
# Answer content
⋮----
# Verify reasoning was streamed as regular text
⋮----
full_reasoning = "".join([chunk.chunk for chunk in reasoning_chunks])
⋮----
# Verify final prediction has Reasoning object created from string
⋮----
# Verify Reasoning object is str-like
</file>

<file path="tests/teleprompt/gepa_dummy_lm_custom_component_selector_custom_instruction_proposer.json">
{"lm": [{"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `minute`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76SqVPjCr5S9iY10Qy1IeMcYRAD', created=1757479932, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe longer minute hand points at the 2 on the dial, which corresponds to 10 minutes.\\n\\n[[ ## minute ## ]]\\n10\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe longer minute hand points at the 2 on the dial, which corresponds to 10 minutes.\n\n[[ ## minute ## ]]\n10\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.010098750000000002, "timestamp": "2025-09-09T22:45:47.013627", "uuid": "af5f7d07-378a-4000-a8e9-2a0f0aa478b4", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `hour`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76T11KnCZm6PV4gqeWiN0K0WXC9', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe hour hand is pointing at the 10 on the clock face, so the hour is 10.\\n\\n[[ ## hour ## ]]\\n10\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe hour hand is pointing at the 10 on the clock face, so the hour is 10.\n\n[[ ## hour ## ]]\n10\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.009468750000000001, "timestamp": "2025-09-09T22:45:47.014155", "uuid": "bd54d56f-c1bc-4f58-b46b-10cea63c6a2b", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `hour`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76Tzq07EuvUd6NqFPi5sDlZQ1fR', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\\n\\n[[ ## hour ## ]]\\n1\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\n\n[[ ## hour ## ]]\n1\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.01118875, "timestamp": "2025-09-09T22:45:47.014524", "uuid": "da555aaa-5dec-4ac8-86db-c747116e2286", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `minute`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76TPH60CaoQjw7UCWgM4sxc74FQ', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\\n\\n[[ ## minute ## ]]\\n15\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\n\n[[ ## minute ## ]]\n15\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.00666875, "timestamp": "2025-09-09T22:45:47.014673", "uuid": "8da47789-6ebd-45e1-a2fb-cd998d8f6e3e", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `hour`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76Tzq07EuvUd6NqFPi5sDlZQ1fR', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\\n\\n[[ ## hour ## ]]\\n1\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\n\n[[ ## hour ## ]]\n1\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.01118875, "timestamp": "2025-09-09T22:45:47.022562", "uuid": "abd91c7e-c75e-481d-a8ff-6024714dfcb2", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `minute`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76TPH60CaoQjw7UCWgM4sxc74FQ', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\\n\\n[[ ## minute ## ]]\\n15\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\n\n[[ ## minute ## ]]\n15\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.00666875, "timestamp": "2025-09-09T22:45:47.022935", "uuid": "a15980e8-9929-4229-b7e3-ef5d394a15c8", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `hour`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76Tzq07EuvUd6NqFPi5sDlZQ1fR', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\\n\\n[[ ## hour ## ]]\\n1\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\n\n[[ ## hour ## ]]\n1\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.01118875, "timestamp": "2025-09-09T22:45:47.023794", "uuid": "dc0d82aa-15cb-444a-8a92-5081a34e48af", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `minute`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76TPH60CaoQjw7UCWgM4sxc74FQ', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\\n\\n[[ ## minute ## ]]\\n15\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\n\n[[ ## minute ## ]]\n15\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.00666875, "timestamp": "2025-09-09T22:45:47.024625", "uuid": "64706167-d1ad-41c6-919d-ad049a624b2d", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `hour`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76T11KnCZm6PV4gqeWiN0K0WXC9', created=1757479933, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe hour hand is pointing at the 10 on the clock face, so the hour is 10.\\n\\n[[ ## hour ## ]]\\n10\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe hour hand is pointing at the 10 on the clock face, so the hour is 10.\n\n[[ ## hour ## ]]\n10\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.009468750000000001, "timestamp": "2025-09-09T22:45:47.025768", "uuid": "11397f7e-cfd9-43a0-a774-e34b74fc2731", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `clock_photo`, produce the fields `reasoning`, `minute`."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76SqVPjCr5S9iY10Qy1IeMcYRAD', created=1757479932, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nThe longer minute hand points at the 2 on the dial, which corresponds to 10 minutes.\\n\\n[[ ## minute ## ]]\\n10\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\nThe longer minute hand points at the 2 on the dial, which corresponds to 10 minutes.\n\n[[ ## minute ## ]]\n10\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.010098750000000002, "timestamp": "2025-09-09T22:45:47.025919", "uuid": "7554fd11-cb2e-4611-81bb-0fab23211d1e", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task\n        - Input: clock_photo (an image of an analog clock)\n        - Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\n        \n        Goal\n        - Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\n        \n        Step-by-step procedure\n        1) Find the dial and pivot\n        - Locate the clock face and the central pivot where all hands originate.\n        - Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\n        \n        2) Determine the 12 o\u2019clock direction\n        - Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\n        - If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\n        \n        3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\n        - Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\n        - Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\n        - Hour hand: shortest, usually thicker, typically ends well inside the numerals.\n        - If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\n        \n        4) Measure positions (angles)\n        - Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\n        - Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\n        \n        5) Use minute-hand position to validate the hour-hand location\n        - The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\n        - Sanity check examples:\n          - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\n          - ~30 minutes: \u2248 halfway.\n          - ~45 minutes: \u2248 3/4 of the way.\n        - If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\n        \n        6) Determine the hour\n        - Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\n        - Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\n        - If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\n        \n        7) Edge cases and robustness\n        - Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\n        - Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\n        - Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\n        - Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\n        - Return 12, not 0, when appropriate (e.g., just after 12:00).\n        \n        Output format\n        - Provide only: hour as an integer in [1,12], with no additional text.\n        \n        Common error prevention (from prior mistakes)\n        - Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\n        - Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\n        - Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7ARTkjo2Fn2hp7mDJTG0MdJjRl7', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- Oriented clock upright using the \u201c12\u201d at the top.\\n- Identified hands: shorter, thicker hour hand points slightly past 8; longer minute hand points to about 3 (15 minutes).\\n- With minute \u2248 15, the hour hand should be about 1/4 of the way from 8 toward 9, which matches.\\n- Last passed hour is 8.\\n\\n[[ ## hour ## ]]\\n8\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- Oriented clock upright using the \u201c12\u201d at the top.\n- Identified hands: shorter, thicker hour hand points slightly past 8; longer minute hand points to about 3 (15 minutes).\n- With minute \u2248 15, the hour hand should be about 1/4 of the way from 8 toward 9, which matches.\n- Last passed hour is 8.\n\n[[ ## hour ## ]]\n8\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.007957500000000001, "timestamp": "2025-09-09T22:45:47.058204", "uuid": "b7705712-111e-48dd-8540-80926cf35f1c", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\n        \n        What to analyze\n        - Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\n        - Hands identification (do not confuse them):\n          - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\n          - Hour hand: shorter and thicker, typically ends near the numerals.\n          - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\n          - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\n        - Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\n        \n        How to compute the minute\n        1. Locate the clock center and the minute hand\u2019s tip.\n        2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\n        3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\n           - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\n           - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\n        4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\n           - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\n           - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\n           - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\n        5. Edge cases:\n           - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\n           - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\n           - No minute ticks: Evenly interpolate between numerals.\n           - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\n        \n        Output format\n        - Return only the integer minute value (0\u201359) in the minute field.\n        - If the angle computes to 60, output 0.\n        \n        Error prevention reminders\n        - Do not treat the hour hand as the minute hand.\n        - Do not use the second hand to compute minutes.\n        - Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\n        - Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7ARdyH04qBQhvBityisBdtWvQ3G', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- The \u201c12\u201d is at the top; the face is upright.\\n- The longer hand reaches the outer minute ring and points to the 9 position.\\n- The shorter hour hand is near the 4, consistent with about 3:45.\\n- 9 on the dial corresponds to 45 minutes.\\n\\n[[ ## minute ## ]]\\n45\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- The \u201c12\u201d is at the top; the face is upright.\n- The longer hand reaches the outer minute ring and points to the 9 position.\n- The shorter hour hand is near the 4, consistent with about 3:45.\n- 9 on the dial corresponds to 45 minutes.\n\n[[ ## minute ## ]]\n45\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0326675, "timestamp": "2025-09-09T22:45:47.058716", "uuid": "69e71807-0b0a-40e6-8576-908dcac161a2", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task\n        - Input: clock_photo (an image of an analog clock)\n        - Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\n        \n        Goal\n        - Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\n        \n        Step-by-step procedure\n        1) Find the dial and pivot\n        - Locate the clock face and the central pivot where all hands originate.\n        - Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\n        \n        2) Determine the 12 o\u2019clock direction\n        - Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\n        - If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\n        \n        3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\n        - Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\n        - Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\n        - Hour hand: shortest, usually thicker, typically ends well inside the numerals.\n        - If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\n        \n        4) Measure positions (angles)\n        - Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\n        - Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\n        \n        5) Use minute-hand position to validate the hour-hand location\n        - The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\n        - Sanity check examples:\n          - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\n          - ~30 minutes: \u2248 halfway.\n          - ~45 minutes: \u2248 3/4 of the way.\n        - If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\n        \n        6) Determine the hour\n        - Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\n        - Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\n        - If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\n        \n        7) Edge cases and robustness\n        - Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\n        - Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\n        - Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\n        - Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\n        - Return 12, not 0, when appropriate (e.g., just after 12:00).\n        \n        Output format\n        - Provide only: hour as an integer in [1,12], with no additional text.\n        \n        Common error prevention (from prior mistakes)\n        - Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\n        - Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\n        - Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE795hHSfyueLWRnKzAItXatIVYsk', created=1757480095, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- 12 is at the top; numerals upright.\\n- The longest hand points to 3 (15 minutes).\\n- The shortest/thick hand is just past 2, about a quarter of the way toward 3, consistent with 15 minutes past.\\n- Therefore the last passed hour is 2.\\n\\n[[ ## hour ## ]]\\n2\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- 12 is at the top; numerals upright.\n- The longest hand points to 3 (15 minutes).\n- The shortest/thick hand is just past 2, about a quarter of the way toward 3, consistent with 15 minutes past.\n- Therefore the last passed hour is 2.\n\n[[ ## hour ## ]]\n2\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.009407500000000001, "timestamp": "2025-09-09T22:45:47.059488", "uuid": "abf20fa7-eac2-4df3-8c5f-943881c2e77e", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task\n        - Input: clock_photo (an image of an analog clock)\n        - Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\n        \n        Goal\n        - Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\n        \n        Step-by-step procedure\n        1) Find the dial and pivot\n        - Locate the clock face and the central pivot where all hands originate.\n        - Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\n        \n        2) Determine the 12 o\u2019clock direction\n        - Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\n        - If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\n        \n        3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\n        - Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\n        - Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\n        - Hour hand: shortest, usually thicker, typically ends well inside the numerals.\n        - If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\n        \n        4) Measure positions (angles)\n        - Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\n        - Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\n        \n        5) Use minute-hand position to validate the hour-hand location\n        - The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\n        - Sanity check examples:\n          - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\n          - ~30 minutes: \u2248 halfway.\n          - ~45 minutes: \u2248 3/4 of the way.\n        - If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\n        \n        6) Determine the hour\n        - Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\n        - Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\n        - If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\n        \n        7) Edge cases and robustness\n        - Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\n        - Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\n        - Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\n        - Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\n        - Return 12, not 0, when appropriate (e.g., just after 12:00).\n        \n        Output format\n        - Provide only: hour as an integer in [1,12], with no additional text.\n        \n        Common error prevention (from prior mistakes)\n        - Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\n        - Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\n        - Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE795hHSfyueLWRnKzAItXatIVYsk', created=1757480095, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- 12 is at the top; numerals upright.\\n- The longest hand points to 3 (15 minutes).\\n- The shortest/thick hand is just past 2, about a quarter of the way toward 3, consistent with 15 minutes past.\\n- Therefore the last passed hour is 2.\\n\\n[[ ## hour ## ]]\\n2\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- 12 is at the top; numerals upright.\n- The longest hand points to 3 (15 minutes).\n- The shortest/thick hand is just past 2, about a quarter of the way toward 3, consistent with 15 minutes past.\n- Therefore the last passed hour is 2.\n\n[[ ## hour ## ]]\n2\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.009407500000000001, "timestamp": "2025-09-09T22:45:47.059973", "uuid": "2a0e4b56-b85e-44ea-b3f5-d71c2d7e42cb", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\n        \n        What to analyze\n        - Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\n        - Hands identification (do not confuse them):\n          - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\n          - Hour hand: shorter and thicker, typically ends near the numerals.\n          - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\n          - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\n        - Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\n        \n        How to compute the minute\n        1. Locate the clock center and the minute hand\u2019s tip.\n        2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\n        3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\n           - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\n           - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\n        4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\n           - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\n           - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\n           - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\n        5. Edge cases:\n           - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\n           - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\n           - No minute ticks: Evenly interpolate between numerals.\n           - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\n        \n        Output format\n        - Return only the integer minute value (0\u201359) in the minute field.\n        - If the angle computes to 60, output 0.\n        \n        Error prevention reminders\n        - Do not treat the hour hand as the minute hand.\n        - Do not use the second hand to compute minutes.\n        - Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\n        - Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7AR1QOoncOEPhyUNbjYH1NuMHLz', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- 12 is at the top; 3 at the right.\\n- The longest hand reaching the outer ring points slightly past the 3 \u2192 this is the minute hand.\\n- The shorter, thicker hand sits between 3 and 4, a bit more than a quarter of the way toward 4, consistent with a little after 15 minutes.\\n- Angle from 12 is just over 90\u00b0, giving about 16 minutes when rounded.\\n\\n[[ ## minute ## ]]\\n16\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- 12 is at the top; 3 at the right.\n- The longest hand reaching the outer ring points slightly past the 3 \u2192 this is the minute hand.\n- The shorter, thicker hand sits between 3 and 4, a bit more than a quarter of the way toward 4, consistent with a little after 15 minutes.\n- Angle from 12 is just over 90\u00b0, giving about 16 minutes when rounded.\n\n[[ ## minute ## ]]\n16\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.026247500000000003, "timestamp": "2025-09-09T22:45:47.059850", "uuid": "ef8932cf-8c71-479b-b5fa-34a3ac529231", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\n        \n        What to analyze\n        - Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\n        - Hands identification (do not confuse them):\n          - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\n          - Hour hand: shorter and thicker, typically ends near the numerals.\n          - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\n          - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\n        - Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\n        \n        How to compute the minute\n        1. Locate the clock center and the minute hand\u2019s tip.\n        2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\n        3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\n           - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\n           - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\n        4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\n           - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\n           - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\n           - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\n        5. Edge cases:\n           - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\n           - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\n           - No minute ticks: Evenly interpolate between numerals.\n           - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\n        \n        Output format\n        - Return only the integer minute value (0\u201359) in the minute field.\n        - If the angle computes to 60, output 0.\n        \n        Error prevention reminders\n        - Do not treat the hour hand as the minute hand.\n        - Do not use the second hand to compute minutes.\n        - Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\n        - Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7AR1QOoncOEPhyUNbjYH1NuMHLz', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- 12 is at the top; 3 at the right.\\n- The longest hand reaching the outer ring points slightly past the 3 \u2192 this is the minute hand.\\n- The shorter, thicker hand sits between 3 and 4, a bit more than a quarter of the way toward 4, consistent with a little after 15 minutes.\\n- Angle from 12 is just over 90\u00b0, giving about 16 minutes when rounded.\\n\\n[[ ## minute ## ]]\\n16\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- 12 is at the top; 3 at the right.\n- The longest hand reaching the outer ring points slightly past the 3 \u2192 this is the minute hand.\n- The shorter, thicker hand sits between 3 and 4, a bit more than a quarter of the way toward 4, consistent with a little after 15 minutes.\n- Angle from 12 is just over 90\u00b0, giving about 16 minutes when rounded.\n\n[[ ## minute ## ]]\n16\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.026247500000000003, "timestamp": "2025-09-09T22:45:47.060335", "uuid": "97962b37-9d47-4fc6-a1a7-3fd9c726d496", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\n        \n        What to analyze\n        - Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\n        - Hands identification (do not confuse them):\n          - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\n          - Hour hand: shorter and thicker, typically ends near the numerals.\n          - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\n          - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\n        - Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\n        \n        How to compute the minute\n        1. Locate the clock center and the minute hand\u2019s tip.\n        2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\n        3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\n           - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\n           - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\n        4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\n           - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\n           - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\n           - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\n        5. Edge cases:\n           - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\n           - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\n           - No minute ticks: Evenly interpolate between numerals.\n           - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\n        \n        Output format\n        - Return only the integer minute value (0\u201359) in the minute field.\n        - If the angle computes to 60, output 0.\n        \n        Error prevention reminders\n        - Do not treat the hour hand as the minute hand.\n        - Do not use the second hand to compute minutes.\n        - Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\n        - Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7AR1QOoncOEPhyUNbjYH1NuMHLz', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- 12 is at the top; 3 at the right.\\n- The longest hand reaching the outer ring points slightly past the 3 \u2192 this is the minute hand.\\n- The shorter, thicker hand sits between 3 and 4, a bit more than a quarter of the way toward 4, consistent with a little after 15 minutes.\\n- Angle from 12 is just over 90\u00b0, giving about 16 minutes when rounded.\\n\\n[[ ## minute ## ]]\\n16\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- 12 is at the top; 3 at the right.\n- The longest hand reaching the outer ring points slightly past the 3 \u2192 this is the minute hand.\n- The shorter, thicker hand sits between 3 and 4, a bit more than a quarter of the way toward 4, consistent with a little after 15 minutes.\n- Angle from 12 is just over 90\u00b0, giving about 16 minutes when rounded.\n\n[[ ## minute ## ]]\n16\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.026247500000000003, "timestamp": "2025-09-09T22:45:47.068704", "uuid": "4bc35192-d5c2-44c2-9f39-6cc2d2809806", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `minute` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## minute ## ]]\n{minute}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\n        \n        What to analyze\n        - Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\n        - Hands identification (do not confuse them):\n          - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\n          - Hour hand: shorter and thicker, typically ends near the numerals.\n          - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\n          - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\n        - Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\n        \n        How to compute the minute\n        1. Locate the clock center and the minute hand\u2019s tip.\n        2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\n        3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\n           - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\n           - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\n        4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\n           - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\n           - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\n           - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\n        5. Edge cases:\n           - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\n           - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\n           - No minute ticks: Evenly interpolate between numerals.\n           - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\n        \n        Output format\n        - Return only the integer minute value (0\u201359) in the minute field.\n        - If the angle computes to 60, output 0.\n        \n        Error prevention reminders\n        - Do not treat the hour hand as the minute hand.\n        - Do not use the second hand to compute minutes.\n        - Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\n        - Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## minute ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7ARdyH04qBQhvBityisBdtWvQ3G', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- The \u201c12\u201d is at the top; the face is upright.\\n- The longer hand reaches the outer minute ring and points to the 9 position.\\n- The shorter hour hand is near the 4, consistent with about 3:45.\\n- 9 on the dial corresponds to 45 minutes.\\n\\n[[ ## minute ## ]]\\n45\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- The \u201c12\u201d is at the top; the face is upright.\n- The longer hand reaches the outer minute ring and points to the 9 position.\n- The shorter hour hand is near the 4, consistent with about 3:45.\n- 9 on the dial corresponds to 45 minutes.\n\n[[ ## minute ## ]]\n45\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0326675, "timestamp": "2025-09-09T22:45:47.070201", "uuid": "d1d68567-b930-49da-b7af-1a59b064e7c4", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task\n        - Input: clock_photo (an image of an analog clock)\n        - Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\n        \n        Goal\n        - Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\n        \n        Step-by-step procedure\n        1) Find the dial and pivot\n        - Locate the clock face and the central pivot where all hands originate.\n        - Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\n        \n        2) Determine the 12 o\u2019clock direction\n        - Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\n        - If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\n        \n        3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\n        - Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\n        - Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\n        - Hour hand: shortest, usually thicker, typically ends well inside the numerals.\n        - If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\n        \n        4) Measure positions (angles)\n        - Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\n        - Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\n        \n        5) Use minute-hand position to validate the hour-hand location\n        - The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\n        - Sanity check examples:\n          - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\n          - ~30 minutes: \u2248 halfway.\n          - ~45 minutes: \u2248 3/4 of the way.\n        - If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\n        \n        6) Determine the hour\n        - Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\n        - Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\n        - If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\n        \n        7) Edge cases and robustness\n        - Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\n        - Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\n        - Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\n        - Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\n        - Return 12, not 0, when appropriate (e.g., just after 12:00).\n        \n        Output format\n        - Provide only: hour as an integer in [1,12], with no additional text.\n        \n        Common error prevention (from prior mistakes)\n        - Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\n        - Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\n        - Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE7ARTkjo2Fn2hp7mDJTG0MdJjRl7', created=1757480179, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- Oriented clock upright using the \u201c12\u201d at the top.\\n- Identified hands: shorter, thicker hour hand points slightly past 8; longer minute hand points to about 3 (15 minutes).\\n- With minute \u2248 15, the hour hand should be about 1/4 of the way from 8 toward 9, which matches.\\n- Last passed hour is 8.\\n\\n[[ ## hour ## ]]\\n8\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- Oriented clock upright using the \u201c12\u201d at the top.\n- Identified hands: shorter, thicker hour hand points slightly past 8; longer minute hand points to about 3 (15 minutes).\n- With minute \u2248 15, the hour hand should be about 1/4 of the way from 8 toward 9, which matches.\n- Last passed hour is 8.\n\n[[ ## hour ## ]]\n8\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.007957500000000001, "timestamp": "2025-09-09T22:45:47.070882", "uuid": "b8f0fb9f-0cc8-4c57-a69e-8c33526017a4", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `clock_photo` (Image):\nYour output fields are:\n1. `reasoning` (str): \n2. `hour` (int):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## clock_photo ## ]]\n{clock_photo}\n\n[[ ## reasoning ## ]]\n{reasoning}\n\n[[ ## hour ## ]]\n{hour}        # note: the value you produce must be a single int value\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Task\n        - Input: clock_photo (an image of an analog clock)\n        - Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\n        \n        Goal\n        - Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\n        \n        Step-by-step procedure\n        1) Find the dial and pivot\n        - Locate the clock face and the central pivot where all hands originate.\n        - Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\n        \n        2) Determine the 12 o\u2019clock direction\n        - Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\n        - If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\n        \n        3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\n        - Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\n        - Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\n        - Hour hand: shortest, usually thicker, typically ends well inside the numerals.\n        - If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\n        \n        4) Measure positions (angles)\n        - Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\n        - Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\n        \n        5) Use minute-hand position to validate the hour-hand location\n        - The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\n        - Sanity check examples:\n          - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\n          - ~30 minutes: \u2248 halfway.\n          - ~45 minutes: \u2248 3/4 of the way.\n        - If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\n        \n        6) Determine the hour\n        - Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\n        - Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\n        - If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\n        \n        7) Edge cases and robustness\n        - Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\n        - Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\n        - Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\n        - Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\n        - Return 12, not 0, when appropriate (e.g., just after 12:00).\n        \n        Output format\n        - Provide only: hour as an integer in [1,12], with no additional text.\n        \n        Common error prevention (from prior mistakes)\n        - Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\n        - Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\n        - Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3)."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## clock_photo ## ]]\n"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\n\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## hour ## ]]` (must be formatted as a valid Python int), and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE795hHSfyueLWRnKzAItXatIVYsk', created=1757480095, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\n- 12 is at the top; numerals upright.\\n- The longest hand points to 3 (15 minutes).\\n- The shortest/thick hand is just past 2, about a quarter of the way toward 3, consistent with 15 minutes past.\\n- Therefore the last passed hour is 2.\\n\\n[[ ## hour ## ]]\\n2\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## reasoning ## ]]\n- 12 is at the top; numerals upright.\n- The longest hand points to 3 (15 minutes).\n- The shortest/thick hand is just past 2, about a quarter of the way toward 3, consistent with 15 minutes past.\n- Therefore the last passed hour is 2.\n\n[[ ## hour ## ]]\n2\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.009407500000000001, "timestamp": "2025-09-09T22:45:47.071958", "uuid": "1400a004-4d6b-40f2-8a76-e8a8a45d9883", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}], "reflection_lm": [{"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `current_instruction` (str): The current instruction that was provided to the assistant to perform the multimodal task\n2. `examples_with_feedback` (str): Task examples with visual content showing inputs, assistant outputs, and feedback. Pay special attention to feedback about visual analysis accuracy, visual-textual integration, and any domain-specific visual knowledge that the assistant missed.\nYour output fields are:\n1. `improved_instruction` (str): A better instruction for the assistant that addresses visual analysis issues, provides clear guidance on how to process and integrate visual and textual information, includes necessary visual domain knowledge, and prevents the visual analysis mistakes shown in the examples.\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## current_instruction ## ]]\n{current_instruction}\n\n[[ ## examples_with_feedback ## ]]\n{examples_with_feedback}\n\n[[ ## improved_instruction ## ]]\n{improved_instruction}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        I provided an assistant with instructions to perform a task involving visual content, but the assistant's performance needs improvement based on the examples and feedback below.\n        \n        Your task is to write a better instruction for the assistant that addresses the specific issues identified in the feedback, with particular attention to how visual and textual information should be analyzed and integrated.\n        \n        ## Analysis Steps:\n        1. **Read the inputs carefully** and identify both the visual and textual input formats, understanding how they work together\n        2. **Read all the assistant responses and corresponding feedback** to understand what went wrong with visual analysis, text processing, or their integration\n        3. **Identify visual analysis patterns** - what visual features, relationships, or details are important for this task\n        4. **Identify domain-specific knowledge** about both visual and textual aspects, as this information may not be available to the assistant in the future\n        5. **Look for successful visual-textual integration strategies** and include these patterns in the instruction\n        6. **Address specific visual analysis issues** mentioned in the feedback\n        \n        ## Instruction Requirements:\n        - **Clear task definition** explaining how to process both visual and textual inputs\n        - **Visual analysis guidance** specific to this task (what to look for, how to describe, what features matter)\n        - **Integration strategies** for combining visual observations with textual information\n        - **Domain-specific knowledge** about visual concepts, terminology, or relationships\n        - **Error prevention guidance** for common visual analysis mistakes shown in the feedback\n        - **Precise, actionable language** for both visual and textual processing\n        \n        Focus on creating an instruction that helps the assistant properly analyze visual content, integrate it with textual information, and avoid the specific visual analysis mistakes shown in the examples."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## current_instruction ## ]]\nGiven the fields `clock_photo`, produce the fields `reasoning`, `hour`.\n\n[[ ## examples_with_feedback ## ]]\n[1] \u00ab\u00ab\u00ab\n    The examples below include visual content (3 images total). Please analyze both the text and visual elements when suggesting improvements.\n    \n    # Example 1\n    ## Inputs\n    ### clock_photo\n    [IMAGE-1 - see visual content]\n    \n    ## Generated Outputs\n    ### reasoning\n    The hour hand is pointing at the 10 on the clock face, so the hour is 10.\n    \n    ### hour\n    10\n    \n    ## Feedback\n    Target: 8:18, Predicted: 10:10\n    \n    \n    \n    # Example 2\n    ## Inputs\n    ### clock_photo\n    [IMAGE-1 - see visual content]\n    \n    ## Generated Outputs\n    ### reasoning\n    The minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\n    \n    ### hour\n    1\n    \n    ## Feedback\n    Target: 4:16, Predicted: 1:15\n    \n    \n    \n    # Example 3\n    ## Inputs\n    ### clock_photo\n    [IMAGE-1 - see visual content]\n    \n    ## Generated Outputs\n    ### reasoning\n    The minute hand points at 3 (15 minutes), and the shorter hour hand is just past 1. Therefore, the hour is 1.\n    \n    ### hour\n    1\n    \n    ## Feedback\n    Target: 4:16, Predicted: 1:15\n    \n    \n\u00bb\u00bb\u00bb\n[2] \u00ab"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\u00bb\n[3] \u00ab"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\u00bb\n[4] \u00ab"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\u00bb\n\nRespond with the corresponding output fields, starting with the field `[[ ## improved_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE76mfnk2mA8wJysppAeTXECkyOu8', created=1757479952, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## improved_instruction ## ]]\\nTask\\n- Input: clock_photo (an image of an analog clock)\\n- Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\\n\\nGoal\\n- Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\\n\\nStep-by-step procedure\\n1) Find the dial and pivot\\n- Locate the clock face and the central pivot where all hands originate.\\n- Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\\n\\n2) Determine the 12 o\u2019clock direction\\n- Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\\n- If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\\n\\n3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\\n- Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\\n- Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\\n- Hour hand: shortest, usually thicker, typically ends well inside the numerals.\\n- If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\\n\\n4) Measure positions (angles)\\n- Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\\n- Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\\n\\n5) Use minute-hand position to validate the hour-hand location\\n- The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\\n- Sanity check examples:\\n  - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\\n  - ~30 minutes: \u2248 halfway.\\n  - ~45 minutes: \u2248 3/4 of the way.\\n- If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\\n\\n6) Determine the hour\\n- Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\\n- Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\\n- If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\\n\\n7) Edge cases and robustness\\n- Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\\n- Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\\n- Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\\n- Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\\n- Return 12, not 0, when appropriate (e.g., just after 12:00).\\n\\nOutput format\\n- Provide only: hour as an integer in [1,12], with no additional text.\\n\\nCommon error prevention (from prior mistakes)\\n- Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\\n- Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\\n- Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3).\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## improved_instruction ## ]]\nTask\n- Input: clock_photo (an image of an analog clock)\n- Output: hour (an integer 1\u201312). Output only the hour number with no extra text.\n\nGoal\n- Determine the correct hour by accurately identifying the hour hand and its position relative to the hour marks, taking into account the minute hand\u2019s position (since the hour hand moves continuously between numbers).\n\nStep-by-step procedure\n1) Find the dial and pivot\n- Locate the clock face and the central pivot where all hands originate.\n- Ignore decorative elements that do not originate at the central pivot (e.g., ornaments, shadows, reflections).\n\n2) Determine the 12 o\u2019clock direction\n- Prefer the numeral \u201c12\u201d if visible. Otherwise use the upright orientation of numerals or the topmost marker.\n- If the photo is rotated, mentally rotate so numerals read upright: 12 at top, 3 right, 6 bottom, 9 left.\n\n3) Identify the hands correctly (do not assume a default \u201c10:10\u201d)\n- Second hand: thinnest, often with a counterweight, may span very long; ignore for the hour.\n- Minute hand: longest, usually reaches or nearly reaches the outer minute tick marks.\n- Hour hand: shortest, usually thicker, typically ends well inside the numerals.\n- If ambiguous, classify by tip distance from center: minute \u2265 hour. Use the piece actually anchored at the pivot, not its shadow.\n\n4) Measure positions (angles)\n- Measure each hand\u2019s angle clockwise from 12 o\u2019clock.\n- Minute angle \u03b8m \u2248 position of the minute hand; hour angle \u03b8h \u2248 position of the hour hand.\n\n5) Use minute-hand position to validate the hour-hand location\n- The hour hand advances 0.5\u00b0 per minute (i.e., 1/12 of the distance between hour marks every 5 minutes).\n- Sanity check examples:\n  - ~15 minutes past: hour hand \u2248 1/4 of the way from the current hour toward the next.\n  - ~30 minutes: \u2248 halfway.\n  - ~45 minutes: \u2248 3/4 of the way.\n- If this relationship doesn\u2019t hold, you likely swapped hour and minute hands\u2014re-identify them.\n\n6) Determine the hour\n- Compute the \u201clast passed\u201d hour: H = floor((\u03b8h mod 360) / 30). Map 0 to 12 (i.e., if floor(...) = 0, H = 12).\n- Do not round up to the next hour. The correct hour is the number the hour hand has most recently passed, not the one it is approaching.\n- If the hour hand appears exactly on an hour mark but the minute hand is not at 12, treat it as still between hours and choose the lower (last passed) hour.\n\n7) Edge cases and robustness\n- Stylized or missing numerals: rely on the 12/3/6/9 axes and tick marks rather than numeral shapes.\n- Roman numerals: \u201c4\u201d may be IIII; positions are unchanged.\n- Ignore mirrored effects, reflections, and shadows; only consider hands anchored at the pivot.\n- Overlap times: if hands nearly overlap, use \u03b8m to ensure the hour hand offset matches 0.5\u00b0 per minute.\n- Return 12, not 0, when appropriate (e.g., just after 12:00).\n\nOutput format\n- Provide only: hour as an integer in [1,12], with no additional text.\n\nCommon error prevention (from prior mistakes)\n- Do not confuse the minute hand for the hour hand; verify by length and reach to the outer tick marks.\n- Do not infer times like \u201c10:10\u201d by default; always read from the actual hand angles.\n- Ensure the hour chosen matches the \u201clast passed\u201d number given the minute hand\u2019s position (e.g., at ~:16, the hour hand must be just past the hour, not near 1 when the minute hand is at 3).\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.04242250000000001, "timestamp": "2025-09-09T22:45:47.038613", "uuid": "91004ed4-e416-4b55-882a-75f98374c29a", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `current_instruction` (str): The current instruction that was provided to the assistant to perform the multimodal task\n2. `examples_with_feedback` (str): Task examples with visual content showing inputs, assistant outputs, and feedback. Pay special attention to feedback about visual analysis accuracy, visual-textual integration, and any domain-specific visual knowledge that the assistant missed.\nYour output fields are:\n1. `improved_instruction` (str): A better instruction for the assistant that addresses visual analysis issues, provides clear guidance on how to process and integrate visual and textual information, includes necessary visual domain knowledge, and prevents the visual analysis mistakes shown in the examples.\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## current_instruction ## ]]\n{current_instruction}\n\n[[ ## examples_with_feedback ## ]]\n{examples_with_feedback}\n\n[[ ## improved_instruction ## ]]\n{improved_instruction}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        I provided an assistant with instructions to perform a task involving visual content, but the assistant's performance needs improvement based on the examples and feedback below.\n        \n        Your task is to write a better instruction for the assistant that addresses the specific issues identified in the feedback, with particular attention to how visual and textual information should be analyzed and integrated.\n        \n        ## Analysis Steps:\n        1. **Read the inputs carefully** and identify both the visual and textual input formats, understanding how they work together\n        2. **Read all the assistant responses and corresponding feedback** to understand what went wrong with visual analysis, text processing, or their integration\n        3. **Identify visual analysis patterns** - what visual features, relationships, or details are important for this task\n        4. **Identify domain-specific knowledge** about both visual and textual aspects, as this information may not be available to the assistant in the future\n        5. **Look for successful visual-textual integration strategies** and include these patterns in the instruction\n        6. **Address specific visual analysis issues** mentioned in the feedback\n        \n        ## Instruction Requirements:\n        - **Clear task definition** explaining how to process both visual and textual inputs\n        - **Visual analysis guidance** specific to this task (what to look for, how to describe, what features matter)\n        - **Integration strategies** for combining visual observations with textual information\n        - **Domain-specific knowledge** about visual concepts, terminology, or relationships\n        - **Error prevention guidance** for common visual analysis mistakes shown in the feedback\n        - **Precise, actionable language** for both visual and textual processing\n        \n        Focus on creating an instruction that helps the assistant properly analyze visual content, integrate it with textual information, and avoid the specific visual analysis mistakes shown in the examples."}, {"role": "user", "content": [{"type": "text", "text": "[[ ## current_instruction ## ]]\nGiven the fields `clock_photo`, produce the fields `reasoning`, `minute`.\n\n[[ ## examples_with_feedback ## ]]\n[1] \u00ab\u00ab\u00ab\n    The examples below include visual content (3 images total). Please analyze both the text and visual elements when suggesting improvements.\n    \n    # Example 1\n    ## Inputs\n    ### clock_photo\n    [IMAGE-1 - see visual content]\n    \n    ## Generated Outputs\n    ### reasoning\n    The longer minute hand points at the 2 on the dial, which corresponds to 10 minutes.\n    \n    ### minute\n    10\n    \n    ## Feedback\n    Target: 8:18, Predicted: 10:10\n    \n    \n    \n    # Example 2\n    ## Inputs\n    ### clock_photo\n    [IMAGE-1 - see visual content]\n    \n    ## Generated Outputs\n    ### reasoning\n    The longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\n    \n    ### minute\n    15\n    \n    ## Feedback\n    Target: 4:16, Predicted: 1:15\n    \n    \n    \n    # Example 3\n    ## Inputs\n    ### clock_photo\n    [IMAGE-1 - see visual content]\n    \n    ## Generated Outputs\n    ### reasoning\n    The longer minute hand is pointing directly at the 3 on the clock face, which corresponds to 15 minutes past the hour.\n    \n    ### minute\n    15\n    \n    ## Feedback\n    Target: 4:16, Predicted: 1:15\n    \n    \n\u00bb\u00bb\u00bb\n[2] \u00ab"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg/500px-Pendulum_clock_by_Jacob_Kock%2C_antique_furniture_photography%2C_IMG_0931_edit.jpg"}}, {"type": "text", "text": "\u00bb\n[3] \u00ab"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\u00bb\n[4] \u00ab"}, {"type": "image_url", "image_url": {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Telechron_clock_2H07-Br_Administrator.JPG/960px-Telechron_clock_2H07-Br_Administrator.JPG"}}, {"type": "text", "text": "\u00bb\n\nRespond with the corresponding output fields, starting with the field `[[ ## improved_instruction ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}]}], "kwargs": {}, "response": "ModelResponse(id='chatcmpl-CE788elupjLcStrQ8q0wQ9hhf60qf', created=1757480036, model='gpt-5-2025-08-07', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## improved_instruction ## ]]\\nTask: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\\n\\nWhat to analyze\\n- Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\\n- Hands identification (do not confuse them):\\n  - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\\n  - Hour hand: shorter and thicker, typically ends near the numerals.\\n  - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\\n  - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\\n- Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\\n\\nHow to compute the minute\\n1. Locate the clock center and the minute hand\u2019s tip.\\n2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\\n3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\\n   - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\\n   - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\\n4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\\n   - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\\n   - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\\n   - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\\n5. Edge cases:\\n   - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\\n   - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\\n   - No minute ticks: Evenly interpolate between numerals.\\n   - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\\n\\nOutput format\\n- Return only the integer minute value (0\u201359) in the minute field.\\n- If the angle computes to 60, output 0.\\n\\nError prevention reminders\\n- Do not treat the hour hand as the minute hand.\\n- Do not use the second hand to compute minutes.\\n- Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\\n- Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0).\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage={}, service_tier='default', cache_hit=True)", "outputs": ["[[ ## improved_instruction ## ]]\nTask: From the image field clock_photo (an analog clock), output the minute value as an integer from 0\u201359 in the field minute. Output only the minute number\u2014no text or other fields.\n\nWhat to analyze\n- Clock face orientation: Identify where \u201c12\u201d is on the dial. Use the numerals (Arabic or Roman, stylized fonts) or the positions of 3, 6, 9, 12 to set the reference. If the photo is tilted, measure angles relative to the clock face, not the image frame.\n- Hands identification (do not confuse them):\n  - Minute hand: typically the longest solid hand reaching near the minute ticks/outer ring; thicker than the second hand; often has a pronounced pointer tip.\n  - Hour hand: shorter and thicker, typically ends near the numerals.\n  - Second hand (if present): the thinnest, often the longest, usually with a counterweight; ignore it for minute reading.\n  - If two non-second hands look similar, the one whose tip reaches closer to the minute tick ring is the minute hand.\n- Ticks and numerals: Each numeral-to-numeral segment equals 5 minutes. If minute tick marks exist, use them. If not, divide each numeral interval evenly into five.\n\nHow to compute the minute\n1. Locate the clock center and the minute hand\u2019s tip.\n2. Determine the angle of the minute hand from the 12 o\u2019clock direction, increasing clockwise.\n3. Convert angle to minutes: minute_estimate = (angle_from_12 / 6). Round to the nearest whole minute.\n   - Mapping: 12 \u2192 0, 1 \u2192 5, 2 \u2192 10, 3 \u2192 15, 4 \u2192 20, 5 \u2192 25, 6 \u2192 30, 7 \u2192 35, 8 \u2192 40, 9 \u2192 45, 10 \u2192 50, 11 \u2192 55.\n   - If the tip is slightly past a numeral (e.g., just past 3), do not snap to the numeral; round to the nearest minute (e.g., 16 instead of 15).\n4. Consistency check with the hour hand (useful to avoid off-by-one and hand mix-ups):\n   - The hour hand moves continuously: it advances 0.5 degrees per minute (i.e., 1/12 of the way to the next numeral every 5 minutes).\n   - If your minute_estimate is an exact multiple of 5 but the hour hand is clearly between hour markers (not aligned with an hour), re-examine: the minute hand is likely slightly past the numeral; adjust to the nearest minute accordingly.\n   - If the minute hand choice is ambiguous, infer the minute from the hour hand\u2019s fraction toward the next hour: minute \u2248 fraction_between_hour_markers \u00d7 60, then choose the hand assignment that matches this.\n5. Edge cases:\n   - Overlapping hands: Look at which tip extends farther toward the tick ring to identify the minute hand.\n   - Strong perspective or glare: Use the line from center to the visible tip; ignore reflections.\n   - No minute ticks: Evenly interpolate between numerals.\n   - Subdials or decorative elements (e.g., pendulum windows) are not the minute indicator; use the main dial only.\n\nOutput format\n- Return only the integer minute value (0\u201359) in the minute field.\n- If the angle computes to 60, output 0.\n\nError prevention reminders\n- Do not treat the hour hand as the minute hand.\n- Do not use the second hand to compute minutes.\n- Do not assume the minute hand is exactly on a numeral\u2014check for slight offsets and round to the nearest minute.\n- Ensure the final minute agrees with the hour hand\u2019s position trend (hour hand slightly past an hour implies minutes > 0).\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.03318125, "timestamp": "2025-09-09T22:45:47.041707", "uuid": "7f89fa6e-4f69-42e7-8c6a-f8600383ca38", "model": "openai/gpt-5-2025-08-07", "response_model": "gpt-5-2025-08-07", "model_type": "chat"}]}
</file>

<file path="tests/teleprompt/gepa_dummy_lm.json">
{"lm": [{"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `input`, produce the fields `output`."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat does the fox say?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0001624, "timestamp": "2025-08-13T18:20:44.059820", "uuid": "93b03c3f-7e96-43a0-bac1-2ececc179365", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `input`, produce the fields `output`."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat is the color of the sky?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nThe color of the sky is typically blue during the day due to the scattering of sunlight by the atmosphere. However, it can appear different colors at sunrise, sunset, or under various weather conditions.\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nThe color of the sky is typically blue during the day due to the scattering of sunlight by the atmosphere. However, it can appear different colors at sunrise, sunset, or under various weather conditions.\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.000136, "timestamp": "2025-08-13T18:20:44.060309", "uuid": "b3367637-304b-46cf-9147-6a976a8db439", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `input`, produce the fields `output`."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat is the color of the sky?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nThe color of the sky is typically blue during the day due to the scattering of sunlight by the atmosphere. However, it can appear different colors at sunrise, sunset, or under various weather conditions.\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nThe color of the sky is typically blue during the day due to the scattering of sunlight by the atmosphere. However, it can appear different colors at sunrise, sunset, or under various weather conditions.\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.000136, "timestamp": "2025-08-13T18:20:44.074500", "uuid": "34df9ccb-128c-4a66-8ee7-c17de080d2ad", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `input`, produce the fields `output`."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat does the fox say?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0001624, "timestamp": "2025-08-13T18:20:44.076517", "uuid": "eae79449-93ab-474c-887e-e8ecde69f8e8", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the fields `input`, produce the fields `output`."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat does the fox say?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0001624, "timestamp": "2025-08-13T18:20:44.078480", "uuid": "6768dd52-0e72-4648-891b-4d60bea205ec", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n        \n        Key details and guidelines:\n        \n        1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n        \n        2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n        \n        3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n        \n        4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n        \n        5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n        \n        6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n        \n        7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n        \n        Example:\n        \n        - Input: \"What is the color of the sky?\"\n        - Output: \"Blue.\"\n        \n        - Input: \"What does the fox say?\"\n        - Output: \"Ring-ding-ding-ding-dingeringeding!\"\n        \n        This approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat is the color of the sky?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nBlue.\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nBlue.\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.000234, "timestamp": "2025-08-13T18:20:44.093296", "uuid": "3d8eee4c-f7f0-4f74-a67a-8a43da85da15", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n        \n        Key details and guidelines:\n        \n        1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n        \n        2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n        \n        3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n        \n        4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n        \n        5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n        \n        6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n        \n        7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n        \n        Example:\n        \n        - Input: \"What is the color of the sky?\"\n        - Output: \"Blue.\"\n        \n        - Input: \"What does the fox say?\"\n        - Output: \"Ring-ding-ding-ding-dingeringeding!\"\n        \n        This approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat does the fox say?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nRing-ding-ding-ding-dingeringeding!\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nRing-ding-ding-ding-dingeringeding!\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0002492, "timestamp": "2025-08-13T18:20:44.094009", "uuid": "ef866572-bbe0-4b47-b64a-af377d18d786", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n        \n        Key details and guidelines:\n        \n        1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n        \n        2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n        \n        3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n        \n        4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n        \n        5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n        \n        6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n        \n        7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n        \n        Example:\n        \n        - Input: \"What is the color of the sky?\"\n        - Output: \"Blue.\"\n        \n        - Input: \"What does the fox say?\"\n        - Output: \"Ring-ding-ding-ding-dingeringeding!\"\n        \n        This approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat does the fox say?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nRing-ding-ding-ding-dingeringeding!\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nRing-ding-ding-ding-dingeringeding!\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0002492, "timestamp": "2025-08-13T18:20:44.094555", "uuid": "5b223989-d453-4c94-bf74-7c94d328c94d", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n        \n        Key details and guidelines:\n        \n        1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n        \n        2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n        \n        3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n        \n        4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n        \n        5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n        \n        6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n        \n        7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n        \n        Example:\n        \n        - Input: \"What is the color of the sky?\"\n        - Output: \"Blue.\"\n        \n        - Input: \"What does the fox say?\"\n        - Output: \"Ring-ding-ding-ding-dingeringeding!\"\n        \n        This approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat does the fox say?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nRing-ding-ding-ding-dingeringeding!\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nRing-ding-ding-ding-dingeringeding!\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.0002492, "timestamp": "2025-08-13T18:20:44.123406", "uuid": "64ce9bc4-5a84-4eb4-b8fa-5cceddfb8c7c", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}, {"prompt": null, "messages": [{"role": "system", "content": "Your input fields are:\n1. `input` (str):\nYour output fields are:\n1. `output` (str):\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## input ## ]]\n{input}\n\n[[ ## output ## ]]\n{output}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Given the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n        \n        Key details and guidelines:\n        \n        1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n        \n        2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n        \n        3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n        \n        4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n        \n        5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n        \n        6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n        \n        7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n        \n        Example:\n        \n        - Input: \"What is the color of the sky?\"\n        - Output: \"Blue.\"\n        \n        - Input: \"What does the fox say?\"\n        - Output: \"Ring-ding-ding-ding-dingeringeding!\"\n        \n        This approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user."}, {"role": "user", "content": "[[ ## input ## ]]\nWhat is the color of the sky?\n\nRespond with the corresponding output fields, starting with the field `[[ ## output ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`."}], "kwargs": {}, "response": "[[ ## output ## ]]\nBlue.\n\n[[ ## completed ## ]]", "outputs": ["[[ ## output ## ]]\nBlue.\n\n[[ ## completed ## ]]"], "usage": {}, "cost": 0.000234, "timestamp": "2025-08-13T18:20:44.125206", "uuid": "5b38b61a-e139-4962-b788-570b3cbb5c38", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}], "reflection_lm": [{"prompt": "I provided an assistant with the following instructions to perform a task for me:\n```\nGiven the fields `input`, produce the fields `output`.\n```\n\nThe following are examples of different task inputs provided to the assistant along with the assistant's response for each of them, and some feedback on how the assistant's response could be better:\n```\n# Example 1\n## Inputs\n### input\nWhat is the color of the sky?\n\n## Generated Outputs\n### output\nThe color of the sky is typically blue during the day due to the scattering of sunlight by the atmosphere. However, it can appear different colors at sunrise, sunset, or under various weather conditions.\n\n## Feedback\nWrong answer.\n\n\n\n# Example 2\n## Inputs\n### input\nWhat does the fox say?\n\n## Generated Outputs\n### output\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n## Feedback\nWrong answer.\n\n\n\n# Example 3\n## Inputs\n### input\nWhat does the fox say?\n\n## Generated Outputs\n### output\nThe fox says a variety of sounds including barks, yips, and howls. However, the phrase \"What does the fox say?\" is famously known from the viral song by Ylvis, where humorous and nonsensical sounds are suggested as the fox's call.\n\n## Feedback\nWrong answer.\n\n\n```\n\nYour task is to write a new instruction for the assistant.\n\nRead the inputs carefully and identify the input format and infer detailed task description about the task I wish to solve with the assistant.\n\nRead all the assistant responses and the corresponding feedback. Identify all niche and domain specific factual information about the task and include it in the instruction, as a lot of it may not be available to the assistant in the future. The assistant may have utilized a generalizable strategy to solve the task, if so, include that in the instruction as well.\n\nProvide the new instructions within ``` blocks.", "messages": null, "kwargs": {}, "response": "``` \nGiven the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n\nKey details and guidelines:\n\n1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n\n2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n\n3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n\n4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n\n5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n\n6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n\n7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n\nExample:\n\n- Input: \"What is the color of the sky?\"\n- Output: \"Blue.\"\n\n- Input: \"What does the fox say?\"\n- Output: \"Ring-ding-ding-ding-dingeringeding!\"\n\nThis approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user.\n```", "outputs": ["``` \nGiven the field `input` containing a question or phrase, produce the field `output` containing the exact, direct, and contextually appropriate answer or response that the user expects, without additional explanations, commentary, or general knowledge unless explicitly requested.\n\nKey details and guidelines:\n\n1. The `input` field contains a question or phrase that may be literal, factual, or culturally specific (e.g., references to popular culture or memes).\n\n2. The `output` must be the precise answer or response that directly addresses the `input` as intended by the user, not a general or encyclopedic explanation.\n\n3. If the `input` is a well-known phrase or question from popular culture (e.g., \"What does the fox say?\"), the `output` should reflect the expected or canonical answer associated with that phrase, rather than a factual or scientific explanation.\n\n4. Avoid providing additional background information, scientific explanations, or alternative interpretations unless explicitly requested.\n\n5. The goal is to produce the answer that the user expects or the \"correct\" answer in the context of the question, including culturally recognized or meme-based answers.\n\n6. If the `input` is a straightforward factual question (e.g., \"What is the color of the sky?\"), provide the commonly accepted direct answer (e.g., \"Blue\") rather than a detailed scientific explanation.\n\n7. The output should be concise, clear, and focused solely on answering the question or phrase in the `input`.\n\nExample:\n\n- Input: \"What is the color of the sky?\"\n- Output: \"Blue.\"\n\n- Input: \"What does the fox say?\"\n- Output: \"Ring-ding-ding-ding-dingeringeding!\"\n\nThis approach ensures that the assistant provides the expected, contextually appropriate answers rather than general or overly detailed responses that may be considered incorrect by the user.\n```"], "usage": {}, "cost": 0.000774, "timestamp": "2025-08-13T18:20:44.080463", "uuid": "c71eee51-af0b-4469-a365-343105013d66", "model": "openai/gpt-4.1-mini", "response_model": "gpt-4.1-mini-2025-04-14", "model_type": "chat"}]}
</file>

<file path="tests/teleprompt/test_bootstrap_finetune.py">
# Define a simple metric function for testing
def simple_metric(example, prediction, trace=None)
⋮----
examples = [
trainset = [examples[0]]
⋮----
def test_bootstrap_finetune_initialization()
⋮----
"""Test BootstrapFinetune initialization with various parameters."""
bootstrap = BootstrapFinetune(metric=simple_metric)
⋮----
class SimpleModule(dspy.Module)
⋮----
def __init__(self, signature)
⋮----
def forward(self, **kwargs)
⋮----
def test_compile_with_predict_instances()
⋮----
"""Test BootstrapFinetune compilation with Predict instances."""
# Create SimpleModule instances for student and teacher
student = SimpleModule("input -> output")
teacher = SimpleModule("input -> output")
⋮----
lm = DummyLM([{"output": "blue"}, {"output": "Ring-ding-ding-ding-dingeringeding!"}])
⋮----
# Set LM for both student and teacher
⋮----
# Mock the fine-tuning process since DummyLM doesn't support it
⋮----
compiled_student = bootstrap.compile(student, teacher=teacher, trainset=trainset)
⋮----
def test_error_handling_missing_lm()
⋮----
"""Test error handling when predictor doesn't have an LM assigned."""
⋮----
lm = DummyLM([{"output": "test"}])
⋮----
# Intentionally NOT setting LM for the student module
⋮----
# This should raise ValueError about missing LM and hint to use set_lm
</file>

<file path="tests/teleprompt/test_bootstrap_trace.py">
def test_bootstrap_trace_data()
⋮----
"""Test bootstrap_trace_data function with single dspy.Predict program."""
⋮----
# Define signature for string -> int conversion
class StringToIntSignature(dspy.Signature)
⋮----
"""Convert a string number to integer"""
⋮----
text: str = dspy.InputField()
number: int = dspy.OutputField()
⋮----
# Create program with single dspy.Predict
program = dspy.Predict(StringToIntSignature)
⋮----
# Create dummy dataset of size 5
dataset = [
⋮----
# Define exact match metric
def exact_match_metric(example, prediction, trace=None)
⋮----
# Configure dspy
⋮----
# Mock litellm completion responses
# 4 successful responses and 1 that will trigger AdapterParseError
successful_responses = [
⋮----
# Create a side effect that will trigger AdapterParseError on the 3rd call (index 2)
def completion_side_effect(*args, **kwargs)
⋮----
call_count = completion_side_effect.call_count
⋮----
if call_count == 5:  # Third call (0-indexed)
# Return malformed response that will cause AdapterParseError
⋮----
# Call bootstrap_trace_data
results = bootstrap_trace_data(
⋮----
# Verify results
⋮----
# Count successful and failed predictions
successful_count = 0
failed_count = 0
⋮----
# Verify failed prediction structure
⋮----
# Verify successful prediction structure
⋮----
# Verify we have the expected number of successful and failed bootstrapping
⋮----
# Verify that traces are present
⋮----
# Each trace entry should be a tuple of (predictor, inputs, prediction)
⋮----
def test_bootstrap_trace_data_passes_callback_metadata(monkeypatch)
⋮----
class DummyProgram(dspy.Module)
⋮----
def forward(self, **kwargs):  # pragma: no cover - stub forward
⋮----
captured_metadata: dict[str, Any] = {}
⋮----
class DummyEvaluate
⋮----
def __init__(self, *args, **kwargs)
⋮----
def __call__(self, *args, callback_metadata=None, **kwargs)
⋮----
class _Result
⋮----
results: list[Any] = []
</file>

<file path="tests/teleprompt/test_bootstrap.py">
# Define a simple metric function for testing
def simple_metric(example, prediction, trace=None)
⋮----
# Simplified metric for testing: true if prediction matches expected output
⋮----
examples = [
trainset = [examples[0]]
valset = [examples[1]]
⋮----
def test_bootstrap_initialization()
⋮----
# Initialize BootstrapFewShot with a dummy metric and minimal setup
bootstrap = BootstrapFewShot(metric=simple_metric, max_bootstrapped_demos=1, max_labeled_demos=1)
⋮----
class SimpleModule(dspy.Module)
⋮----
def __init__(self, signature)
⋮----
def forward(self, **kwargs)
⋮----
def test_compile_with_predict_instances()
⋮----
# Create Predict instances for student and teacher
# Note that dspy.Predict is not itself a module, so we can't use it directly here
student = SimpleModule("input -> output")
teacher = SimpleModule("input -> output")
⋮----
lm = DummyLM(["Initial thoughts", "Finish[blue]"])
⋮----
# Initialize BootstrapFewShot and compile the student
⋮----
compiled_student = bootstrap.compile(student, teacher=teacher, trainset=trainset)
⋮----
def test_bootstrap_effectiveness()
⋮----
# This test verifies if the bootstrapping process improves the student's predictions
⋮----
lm = DummyLM([{"output": "blue"}, {"output": "Ring-ding-ding-ding-dingeringeding!"}], follow_examples=True)
⋮----
# Check that the compiled student has the correct demos
⋮----
# Test the compiled student's prediction.
# We are using a DummyLM with follow_examples=True, which means that
# even though it would normally reply with "Ring-ding-ding-ding-dingeringeding!"
# on the second output, if it seems an example that perfectly matches the
# prompt, it will use that instead. That is why we expect "blue" here.
prediction = compiled_student(input=trainset[0].input)
⋮----
def test_error_handling_during_bootstrap()
⋮----
"""
    Test to verify error handling during the bootstrapping process
    """
⋮----
class BuggyModule(dspy.Module)
⋮----
teacher = BuggyModule("input -> output")
⋮----
# Setup DummyLM to simulate an error scenario
lm = DummyLM(
⋮----
{"output": "Initial thoughts"},  # Simulate initial teacher's prediction
⋮----
bootstrap = BootstrapFewShot(
⋮----
def test_validation_set_usage()
⋮----
"""
    Test to ensure the validation set is correctly used during bootstrapping
    """
⋮----
{"output": "Finish[blue]"},  # Expected output for both training and validation
⋮----
# Check that validation examples are part of student's demos after compilation
</file>

<file path="tests/teleprompt/test_copro_optimizer.py">
# Define a simple metric function for testing
def simple_metric(example, prediction)
⋮----
# Simplified metric for testing: true if prediction matches expected output
⋮----
# Example training and validation sets
trainset = [
⋮----
def test_signature_optimizer_initialization()
⋮----
optimizer = COPRO(metric=simple_metric, breadth=2, depth=1, init_temperature=1.4)
⋮----
class SimpleModule(dspy.Module)
⋮----
def __init__(self, signature)
⋮----
# COPRO doesn't work with dspy.Predict
⋮----
def forward(self, **kwargs)
⋮----
def test_signature_optimizer_optimization_process()
⋮----
student = SimpleModule("input -> output")
⋮----
# Assuming the compile method of COPRO requires a student module, a development set, and evaluation kwargs
optimized_student = optimizer.compile(
⋮----
# Check that the optimized student has been modified from the original
# This check can be more specific based on how the optimization modifies the student
⋮----
# Further tests can be added to verify the specifics of the optimization process,
# such as checking the instructions of the optimized student's predictors.
⋮----
def test_signature_optimizer_statistics_tracking()
⋮----
optimizer.track_stats = True  # Enable statistics tracking
⋮----
# Verify that statistics have been tracked and attached to the optimized student
⋮----
# Assuming the setup_signature_optimizer fixture and simple_metric function are defined as before
⋮----
def test_optimization_and_output_verification()
⋮----
lm = DummyLM(
⋮----
# Compile the student with the optimizer
⋮----
# Simulate calling the optimized student with a new input
test_input = "What is the capital of France?"
prediction = optimized_student(input=test_input)
⋮----
def test_statistics_tracking_during_optimization()
⋮----
# Verify that statistics have been tracked
⋮----
# Check if the results_best and results_latest contain valid statistics
⋮----
# Additional detailed checks can be added here to verify the contents of the tracked statistics
</file>

<file path="tests/teleprompt/test_ensemble.py">
class MockProgram(dspy.Module)
⋮----
def __init__(self, output)
⋮----
def forward(self, *args, **kwargs)
⋮----
# Simple reduction function to test with
def mock_reduce_fn(outputs)
⋮----
def test_ensemble_without_reduction()
⋮----
"""Test that Ensemble correctly combines outputs without applying a reduce_fn."""
programs = [MockProgram(i) for i in range(5)]
ensemble = Ensemble()
ensembled_program = ensemble.compile(programs)
⋮----
outputs = ensembled_program()
⋮----
def test_ensemble_with_reduction()
⋮----
"""Test that Ensemble correctly applies a reduce_fn to combine outputs."""
⋮----
ensemble = Ensemble(reduce_fn=mock_reduce_fn)
⋮----
output = ensembled_program()
expected_output = sum(range(5)) / 5
⋮----
def test_ensemble_with_size_limitation()
⋮----
"""Test that specifying a size limits the number of programs used in the ensemble."""
programs = [MockProgram(i) for i in range(10)]
ensemble_size = 3
ensemble = Ensemble(size=ensemble_size)
⋮----
def test_ensemble_deterministic_behavior()
⋮----
"""Verify that the Ensemble class raises an assertion for deterministic behavior."""
</file>

<file path="tests/teleprompt/test_finetune.py">
# TODO
</file>

<file path="tests/teleprompt/test_gepa_instruction_proposer.py">
def count_messages_with_image_url_pattern(messages)
⋮----
"""Helper to count image URLs in messages - borrowed from image adapter tests"""
pattern = {"type": "image_url", "image_url": {"url": lambda x: isinstance(x, str)}}
⋮----
def check_pattern(obj, pattern)
⋮----
def count_patterns(obj, pattern)
⋮----
count = 0
⋮----
@dataclass
class ImagesInHistory
⋮----
has_structured_images: bool
has_text_serialized_images: bool
⋮----
def check_images_in_history(history: list[Any]) -> ImagesInHistory
⋮----
def check_text_serialized(item: Any) -> bool
⋮----
has_structured_images = False
⋮----
image_count = count_messages_with_image_url_pattern(call["messages"])
⋮----
has_structured_images = True
⋮----
def test_reflection_lm_gets_structured_images()
⋮----
"""
    Verify reflection LM receives structured image messages, not serialized text.
    """
student = dspy.Predict("image: dspy.Image -> label: str")
image = dspy.Image("https://example.com/test.jpg")
example = dspy.Example(image=image, label="dog").with_inputs("image")
⋮----
reflection_lm = DummyLM(
lm = DummyLM(
⋮----
gepa = dspy.GEPA(
⋮----
images_in_history = check_images_in_history(reflection_lm.history)
⋮----
def test_custom_proposer_without_reflection_lm()
⋮----
"""Test that custom instruction proposers can work without reflection_lm when using updated GEPA core."""
⋮----
# External reflection LM managed by the custom proposer
external_reflection_lm = DummyLM(
⋮----
class ProposerWithExternalLM
⋮----
def __call__(self, candidate, reflective_dataset, components_to_update)
⋮----
# This proposer manages its own external reflection LM
⋮----
# Use external LM for reflection (optional - could be any custom logic)
⋮----
student = dspy.Predict("text -> label")
example = dspy.Example(text="test input", label="test").with_inputs("text")
⋮----
# Use a robust dummy LM with enough responses for optimization steps
⋮----
# Test the full flexibility: no reflection_lm provided to GEPA at all!
# The updated GEPA core library now allows this when using custom proposers
⋮----
metric=lambda gold, pred, trace=None, pred_name=None, pred_trace=None: 0.7,  # Score to trigger optimization
max_metric_calls=5,  # More calls to allow proper optimization
reflection_lm=None,  # No reflection_lm provided - this now works!
⋮----
result = gepa.compile(student, trainset=[example], valset=[example])
⋮----
def test_image_serialization_into_strings()
⋮----
"""
    Test that demonstrates the image serialization problem when calling lm directly with serialized image data.
    """
⋮----
class InstructionProposerCallingLMDirectly
⋮----
updated_components = {}
⋮----
current_instruction = candidate[component_name]
component_data = reflective_dataset[component_name]
⋮----
feedback_analysis = "Feedback analysis:\n"
⋮----
# Non ideal approach: extract and serialize image objects directly
inputs = example.get("Inputs", {})
⋮----
outputs = example.get("Generated Outputs", {})
feedback = example.get("Feedback", "")
⋮----
context_lm = dspy.settings.lm
messages = [
⋮----
result = context_lm(messages=messages)
⋮----
direct_lm_call_proposer = InstructionProposerCallingLMDirectly()
⋮----
student = dspy.Predict("image -> label")
⋮----
image = dspy.Image("https://picsum.photos/id/237/200/300")
⋮----
examples = [
⋮----
@pytest.mark.parametrize("reasoning", [True, False])
def test_default_proposer(reasoning: bool, caplog)
⋮----
# Let logs propagate up to root because gepa uses try-catch and logs the error
# https://github.com/gepa-ai/gepa/blob/1b5eff5133be1015210e0512953c25a4b85ad454/src/gepa/proposer/reflective_mutation/reflective_mutation.py#L128
dspy_logger = logging.getLogger("dspy")
original_propagate = dspy_logger.propagate
⋮----
# Check that no internal GEPA reflection errors occurred
</file>

<file path="tests/teleprompt/test_gepa.py">
class SimpleModule(dspy.Module)
⋮----
def __init__(self, signature)
⋮----
def forward(self, **kwargs)
⋮----
class DictDummyLM(dspy.clients.lm.LM)
⋮----
def __init__(self, history)
⋮----
def __call__(self, prompt=None, messages=None, **kwargs)
⋮----
m = self.history[hash(repr(messages))]
⋮----
def simple_metric(example, prediction, trace=None, pred_name=None, pred_trace=None)
⋮----
def bad_metric(example, prediction)
⋮----
def test_gepa_adapter_disables_logging_on_minibatch_eval(monkeypatch, reflection_minibatch_size, batch, expected_callback_metadata)
⋮----
class DummyModule(dspy.Module)
⋮----
def forward(self, **kwargs):  # pragma: no cover - stub forward
⋮----
# Exercise the adapter evaluate path directly.
adapter = gepa_utils.DspyAdapter(
⋮----
captured_kwargs: dict[str, Any] = {}
⋮----
def dummy_bootstrap_trace_data(*args, **kwargs)
⋮----
@pytest.fixture
def mock_mlflow()
⋮----
mock_mlflow = mock.MagicMock()
⋮----
@pytest.mark.parametrize("use_mlflow", [True, False])
def test_basic_workflow(use_mlflow, mock_mlflow)
⋮----
"""Test to ensure the basic compile flow runs without errors."""
student = SimpleModule("input -> output")
⋮----
data = json.load(f)
lm_history = data["lm"]
reflection_lm_history = data["reflection_lm"]
⋮----
lm_main = DictDummyLM(lm_history)
⋮----
reflection_lm = DictDummyLM(reflection_lm_history)
⋮----
optimizer = dspy.GEPA(
⋮----
trainset = [
⋮----
optimized_program = optimizer.compile(student, trainset=trainset, valset=trainset)
⋮----
def test_workflow_with_custom_instruction_proposer_and_component_selector()
⋮----
"""Test to ensure the basic compile flow runs without errors when using a custom instruction proposer and component selector."""
⋮----
class TimeReader(dspy.Module)
⋮----
def __init__(self)
⋮----
def forward(self, clock_photo: dspy.Image)
⋮----
def metric(example, prediction, trace=None, pred_name=None, pred_trace=None)
⋮----
score = -abs(target_hour * 60 + target_minute - (predicted_hour * 60 + predicted_minute))
⋮----
def all_component_selector(state, trajectories, subsample_scores, candidate_idx, candidate)
⋮----
"""Select all components."""
⋮----
student = TimeReader()
⋮----
o = optimizer.compile(student, trainset=trainset, valset=trainset)
⋮----
def test_metric_requires_feedback_signature()
⋮----
reflection_lm = DictDummyLM([])
⋮----
"""
    For this test, we only care that the program runs, not the score.
    """
return 0.0  # ← Just returns 0.0, doesn't access any attributes!
⋮----
def test_gepa_compile_with_track_usage_no_tuple_error(caplog)
⋮----
"""
    GEPA.compile should not log tuple-usage error when track_usage=True and complete without hanging.
    Before, compile would hang and/or log "'tuple' object has no attribute 'set_lm_usage'" repeatedly.
    """
student = dspy.Predict("question -> answer")
trainset = [dspy.Example(question="What is 2+2?", answer="4").with_inputs("question")]
⋮----
task_lm = DummyLM([{"answer": "mock answer 1"}])
reflection_lm = DummyLM([{"new_instruction": "Something new."}])
⋮----
compiled_container: dict[str, Any] = {}
exc_container: dict[str, BaseException] = {}
⋮----
def run_compile()
⋮----
optimizer = dspy.GEPA(metric=any_metric, reflection_lm=reflection_lm, max_metric_calls=3)
⋮----
t = threading.Thread(target=run_compile, daemon=True)
⋮----
# Assert compile did not hang (pre-fix behavior would time out here)
⋮----
# Assert no tuple-usage error is logged anymore
⋮----
# If any exception occurred, fail explicitly
⋮----
# No timeout, no exception -> so the program must exist
⋮----
class MultiComponentModule(dspy.Module)
⋮----
"""Test module with multiple predictors."""
⋮----
def forward(self, input)
⋮----
category = self.classifier(input=input).category
output = self.generator(category=category, input=input).output
⋮----
def component_selection_metric(example, prediction, trace=None, pred_name=None, pred_trace=None)
⋮----
"""Simple metric for component selection testing."""
⋮----
def test_component_selector_functionality()
⋮----
"""Test custom component selector function can select single/multiple components."""
⋮----
# Track calls for verification
selector_calls = []
⋮----
def test_selector(state, trajectories, subsample_scores, candidate_idx, candidate)
⋮----
# Test both single and multiple selection
⋮----
student = MultiComponentModule()
⋮----
# Provide enough responses for all possible LM calls during optimization
task_lm = DummyLM([{"category": "test_category", "output": "test_output"}] * 20)
reflection_lm = DummyLM(
trainset = [dspy.Example(input="test", output="expected").with_inputs("input")]
⋮----
max_metric_calls=6,  # Reduced to minimize output
⋮----
result = optimizer.compile(student, trainset=trainset, valset=trainset)
⋮----
# Verify selector was called with correct parameters
⋮----
def test_component_selector_default_behavior()
⋮----
"""Test default behavior when no custom selector provided."""
⋮----
# Provide enough responses for all possible LM calls
task_lm = DummyLM([{"category": "test_category", "output": "test_output"}] * 15)
reflection_lm = DummyLM([{"improved_instruction": "Better instruction"}] * 8)
⋮----
# No component_selector - should use round-robin default
⋮----
max_metric_calls=4,  # Minimal calls to reduce noise
⋮----
def test_component_selector_string_round_robin()
⋮----
"""Test string-based round_robin selector."""
⋮----
component_selector="round_robin",  # String-based selector
⋮----
def test_component_selector_string_all()
⋮----
"""Test string-based 'all' selector and verify it actually updates all components."""
⋮----
# Store original instructions to verify they get updated
original_classifier_instruction = student.classifier.signature.instructions
original_generator_instruction = student.generator.signature.instructions
⋮----
def optimize(component_selector)
⋮----
# Metric that progressively improves to encourage GEPA to accept new candidates
call_count = 0
⋮----
def improving_metric(example, prediction, trace=None, pred_name=None, pred_trace=None)
⋮----
# Score improves with each call to encourage acceptance of new candidates
score = min(0.3 + (call_count * 0.1), 1.0)
⋮----
track_stats=True,  # Track intermediate results to verify updates
⋮----
result_round_robin = optimize(component_selector="round_robin")
⋮----
candidates_round_robin = result_round_robin.detailed_results.candidates
⋮----
result_all = optimize(component_selector="all")
⋮----
candidates_all = result_all.detailed_results.candidates
⋮----
def test_component_selector_custom_random()
⋮----
"""Test custom component selector function that randomly samples components."""
⋮----
# Simple function-based selector
def random_component_selector(state, trajectories, subsample_scores, candidate_idx, candidate)
⋮----
"""Randomly select half of the available components."""
component_names = list(candidate.keys())
num_to_select = max(1, len(component_names) // 2)  # At least 1, half of total
⋮----
component_selector=random_component_selector,  # Function-based selector
⋮----
def test_alternating_half_component_selector()
⋮----
"""Test alternating half selector that optimizes different halves on even/odd iterations."""
⋮----
selection_history = []
⋮----
def alternating_half_selector(state, trajectories, subsample_scores, candidate_idx, candidate)
⋮----
"""Optimize half the components on even iterations, half on odd iterations."""
components = list(candidate.keys())
⋮----
# If there's only one component, always optimize it
⋮----
selected = components
⋮----
mid_point = len(components) // 2
⋮----
# Use state.i (iteration counter) to alternate between halves
⋮----
# Even iteration: optimize first half
selected = components[:mid_point]
⋮----
# Odd iteration: optimize second half
selected = components[mid_point:]
⋮----
# Track selections for verification
⋮----
student = MultiComponentModule()  # Has "classifier" and "generator" components
⋮----
# Provide enough responses for multiple iterations
⋮----
reflection_lm = DummyLM([{"improved_instruction": "Better instruction"}] * 10)
⋮----
max_metric_calls=8,  # Allow multiple iterations
⋮----
# Even iteration should select first half: ["classifier"]
⋮----
# Odd iteration should select second half: ["generator"]
</file>

<file path="tests/teleprompt/test_grpo.py">
def test_grpo_dataset_shuffler()
⋮----
dataset = [1, 2, 3]
grpo = GRPO(
⋮----
trainset_instances = []
⋮----
def test_grpo_dataset_shuffler_with_num_ex_per_step_less_dataset()
⋮----
counter = Counter()
⋮----
def test_grpo_dataset_shuffler_with_num_ex_per_step_greater_dataset()
</file>

<file path="tests/teleprompt/test_knn_fewshot.py">
def mock_example(question: str, answer: str) -> dspy.Example
⋮----
"""Creates a mock DSP example with specified question and answer."""
⋮----
@pytest.fixture
def setup_knn_few_shot() -> KNNFewShot
⋮----
"""Sets up a KNNFewShot instance for testing."""
trainset = [
⋮----
def test_knn_few_shot_initialization(setup_knn_few_shot)
⋮----
"""Tests the KNNFewShot initialization."""
knn_few_shot = setup_knn_few_shot
⋮----
class SimpleModule(dspy.Module)
⋮----
def __init__(self, signature)
⋮----
def forward(self, *args, **kwargs)
⋮----
def reset_copy(self)
⋮----
# Creates a new instance of SimpleModule with the same predictor
⋮----
# TODO: Test not working yet
def _test_knn_few_shot_compile(setup_knn_few_shot)
⋮----
"""Tests the compile method of KNNFewShot with SimpleModule as student."""
student = SimpleModule("input -> output")
teacher = SimpleModule("input -> output")  # Assuming teacher uses the same module type
⋮----
# Setup DummyLM with a response for a query similar to one of the training examples
lm = DummyLM(["Madrid", "10"])
dspy.configure(lm=lm)  # Responses for the capital of Spain and the result of 5+5)
⋮----
trainset = knn_few_shot.KNN.trainset
compiled_student = knn_few_shot.compile(student, teacher=teacher, trainset=trainset, valset=None)
⋮----
# Simulate a query that is similar to one of the training examples
output = compiled_student.forward(input="What is the capital of Spain?").output
⋮----
# Validate that the output corresponds to one of the expected DummyLM responses
# This assumes the compiled_student's forward method will execute the predictor with the given query
</file>

<file path="tests/teleprompt/test_random_search.py">
class SimpleModule(dspy.Module)
⋮----
def __init__(self, signature)
⋮----
def forward(self, **kwargs)
⋮----
def simple_metric(example, prediction, trace=None)
⋮----
def test_basic_workflow()
⋮----
"""Test to ensure the basic compile flow runs without errors."""
student = SimpleModule("input -> output")
teacher = SimpleModule("input -> output")
⋮----
lm = DummyLM(
⋮----
"Finish[blue]",  # Expected output for both training and validation
⋮----
optimizer = BootstrapFewShotWithRandomSearch(metric=simple_metric, max_bootstrapped_demos=1, max_labeled_demos=1)
trainset = [
</file>

<file path="tests/teleprompt/test_teleprompt.py">
class DummyTeleprompter(Teleprompter)
⋮----
def __init__(self, param1: int, param2: str)
⋮----
def compile(self, student, *, trainset, teacher=None, valset=None, **kwargs)
⋮----
def test_get_params()
⋮----
teleprompter = DummyTeleprompter(param1=1, param2="test")
params = teleprompter.get_params()
</file>

<file path="tests/teleprompt/test_utils.py">
class DummyModule(dspy.Module)
⋮----
def __init__(self)
⋮----
def forward(self, **kwargs)
⋮----
def test_eval_candidate_program_full_trainset()
⋮----
trainset = [1, 2, 3, 4, 5]
candidate_program = DummyModule()
evaluate = Mock(return_value=0)
batch_size = 10
⋮----
result = eval_candidate_program(batch_size, trainset, candidate_program, evaluate)
⋮----
def test_eval_candidate_program_minibatch()
⋮----
batch_size = 3
⋮----
def test_eval_candidate_program_failure()
⋮----
evaluate = Mock(side_effect=ValueError("Error"))
</file>

<file path="tests/test_utils/server/__init__.py">
@pytest.fixture()
def litellm_test_server() -> tuple[str, str]
⋮----
"""
    Start a LiteLLM test server for a DSPy integration test case, and tear down the
    server when the test case completes.
    """
⋮----
# Create a server log file used to store request logs
server_log_file_path = os.path.join(server_log_dir_path, "request_logs.jsonl")
⋮----
port = _get_random_port()
host = "127.0.0.1"
⋮----
process = subprocess.Popen(
⋮----
server_url = f"http://{host}:{port}"
⋮----
def read_litellm_test_server_request_logs(server_log_file_path: str) -> list[dict[str, Any]]
⋮----
"""
    Read request logs from a LiteLLM server used during DSPy integration tests.

    Args:
        server_log_file_path: The filesystem path to the LiteLLM server request logs jsonlines file.
    Return:
        A list of log entries, where each entry corresponds to one request handled by the server.
    """
data = []
⋮----
def _get_litellm_config_path()
⋮----
module_dir = os.path.dirname(os.path.abspath(__file__))
⋮----
def _get_random_port()
⋮----
def _wait_for_port(host, port, timeout=10)
⋮----
start_time = time.time()
⋮----
time.sleep(0.5)  # Wait briefly before trying again
</file>

<file path="tests/test_utils/server/litellm_server_config.yaml">
model_list:
  - model_name: "dspy-test-model"
    litellm_params:
      model: "dspy-test-provider/dspy-test-model"
  - model_name: "dspy-test-model-2"
    litellm_params:
      model: "dspy-test-provider/dspy-test-model"

litellm_settings:
  num_retries: 0
  custom_provider_map:
    - {
        "provider": "dspy-test-provider",
        "custom_handler": litellm_server.dspy_test_model,
      }
</file>

<file path="tests/test_utils/server/litellm_server.py">
LITELLM_TEST_SERVER_LOG_FILE_PATH_ENV_VAR = "LITELLM_TEST_SERVER_LOG_FILE_PATH"
⋮----
class DSPyTestModel(CustomLLM)
⋮----
def completion(self, *args, **kwargs) -> litellm.ModelResponse
⋮----
async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse
⋮----
def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]
⋮----
generic_streaming_chunk: GenericStreamingChunk = {
return generic_streaming_chunk  # type: ignore
⋮----
async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]
⋮----
def _get_mock_llm_response(request_kwargs)
⋮----
def _throw_exception_based_on_content_if_applicable(request_kwargs)
⋮----
"""
    Throws an exception, for testing purposes, based on the content of the request message.
    """
model = request_kwargs["model"]
content = request_kwargs["messages"][0]["content"]
⋮----
def _append_request_to_log_file(completion_kwargs)
⋮----
log_file_path = os.environ.get(LITELLM_TEST_SERVER_LOG_FILE_PATH_ENV_VAR)
⋮----
log_blob = (
⋮----
dspy_test_model = DSPyTestModel()
</file>

<file path="tests/test_utils/__init__.py">

</file>

<file path="tests/utils/resources/mcp_server.py">
mcp = FastMCP("test")
⋮----
class Profile(BaseModel)
⋮----
name: str
age: int
⋮----
class Account(BaseModel)
⋮----
profile: Profile
account_id: str
⋮----
@mcp.tool()
def add(a: int, b: int) -> int
⋮----
"""Add two numbers"""
⋮----
@mcp.tool()
def hello(names: list[str]) -> str
⋮----
"""Greet people"""
⋮----
@mcp.tool()
def wrong_tool()
⋮----
"""This tool raises an error"""
⋮----
@mcp.tool()
def get_account_name(account: Account)
⋮----
"""This extracts the name from account"""
⋮----
@mcp.tool()
def current_datetime() -> str
⋮----
"""Get the current datetime"""
</file>

<file path="tests/utils/__init__.py">

</file>

<file path="tests/utils/test_annotation.py">
def test_experimental_decorator_on_function()
⋮----
@experimental
    def test_function()
⋮----
"""A test function."""
⋮----
def test_experimental_decorator_on_function_with_version()
⋮----
@experimental(version="3.1.0")
    def test_function()
⋮----
"""A test function with version."""
⋮----
def test_experimental_decorator_on_class()
⋮----
@experimental
    class TestClass
⋮----
"""A test class."""
⋮----
def method(self)
⋮----
instance = TestClass()
⋮----
def test_experimental_decorator_on_class_with_version()
⋮----
@experimental(version="2.5.0")
    class TestClass
⋮----
"""A test class with version."""
⋮----
def test_experimental_decorator_without_docstring()
⋮----
def test_experimental_decorator_without_docstring_with_version()
⋮----
@experimental(version="1.0.0")
    def test_function()
⋮----
def test_experimental_decorator_with_callable_syntax()
⋮----
def test_function()
⋮----
decorated = experimental(test_function)
⋮----
def test_experimental_decorator_with_version_callable_syntax()
⋮----
decorated = experimental(test_function, version="4.0.0")
</file>

<file path="tests/utils/test_asyncify.py">
@pytest.mark.anyio
async def test_async_limiter()
⋮----
limiter = get_limiter()
⋮----
@pytest.mark.anyio
async def test_asyncify()
⋮----
def the_answer_to_life_the_universe_and_everything(wait: float)
⋮----
ask_the_question = dspy.asyncify(the_answer_to_life_the_universe_and_everything)
⋮----
async def run_n_tasks(n: int, wait: float)
⋮----
async def verify_asyncify(capacity: int, number_of_tasks: int, wait: float = 0.5)
⋮----
start = time()
⋮----
end = time()
total_time = end - start
⋮----
# If asyncify is working correctly, the total time should be less than the total number of loops
# `(number_of_tasks / capacity)` times wait time, plus the computational overhead. The lower bound should
# be `math.floor(number_of_tasks * 1.0 / capacity) * wait` because there are more than
# `math.floor(number_of_tasks * 1.0 / capacity)` loops.
lower_bound = math.floor(number_of_tasks * 1.0 / capacity) * wait
upper_bound = math.ceil(number_of_tasks * 1.0 / capacity) * wait + 2 * wait  # 2*wait for buffer
</file>

<file path="tests/utils/test_exceptions.py">
def test_adapter_parse_error_basic()
⋮----
adapter_name = "ChatAdapter"
signature = dspy.make_signature("question->answer1, answer2")
lm_response = "[[ ## answer1 ## ]]\nanswer1"
⋮----
error = AdapterParseError(adapter_name=adapter_name, signature=signature, lm_response=lm_response)
⋮----
error_message = str(error)
⋮----
def test_adapter_parse_error_with_message()
⋮----
message = "Critical error, please fix!"
⋮----
error = AdapterParseError(adapter_name=adapter_name, signature=signature, lm_response=lm_response, message=message)
⋮----
def test_adapter_parse_error_with_parsed_result()
⋮----
parsed_result = {"answer1": "value1"}
⋮----
error = AdapterParseError(
</file>

<file path="tests/utils/test_langchain_tool.py">
@pytest.mark.asyncio
@pytest.mark.extra
async def test_convert_custom_simple_tool()
⋮----
@tool
    def add(a: int, b: int) -> int
⋮----
"""Add two numbers."""
⋮----
tool = convert_langchain_tool(add)
⋮----
@pytest.mark.asyncio
@pytest.mark.extra
async def test_convert_custom_tool_with_custom_class()
⋮----
class Profile(BaseModel)
⋮----
name: str
age: int
⋮----
@tool
    def get_age(profile: Profile) -> int
⋮----
"""Get the age of the profile."""
⋮----
tool = convert_langchain_tool(get_age)
</file>

<file path="tests/utils/test_magicattr.py">
class Test
⋮----
l = [1, 2]
a = [0, [1, 2, [3, 4]]]
b = {"x": {"y": "y"}, "z": [1, 2]}
z = "z"
⋮----
class Person
⋮----
settings = {
⋮----
def __init__(self, name, age, friends)
⋮----
def test_magicattr_get(key, value)
⋮----
obj = Test()
⋮----
def test_person_example()
⋮----
bob = Person(name="Bob", age=31, friends=[])
jill = Person(name="Jill", age=29, friends=[bob])
jack = Person(name="Jack", age=28, friends=[bob, jill])
⋮----
def test_empty()
</file>

<file path="tests/utils/test_mcp.py">
@pytest.mark.asyncio
@pytest.mark.extra
async def test_convert_mcp_tool()
⋮----
server_params = StdioServerParameters(
⋮----
response = await session.list_tools()
⋮----
# Check add
add_tool = convert_mcp_tool(session, response.tools[0])
⋮----
# Check hello
hello_tool = convert_mcp_tool(session, response.tools[1])
⋮----
# Check error handling
error_tool = convert_mcp_tool(session, response.tools[2])
⋮----
# Check nested Pydantic arg
nested_pydantic_tool = convert_mcp_tool(session, response.tools[3])
⋮----
account_in_json = {
result = await nested_pydantic_tool.acall(account=account_in_json)
⋮----
# Check no input parameter current_datetime tool
current_datetime_tool = convert_mcp_tool(session, response.tools[4])
</file>

<file path="tests/utils/test_parallelizer.py">
def test_worker_threads_independence()
⋮----
def task(item)
⋮----
# Each thread maintains its own state by appending to a thread-local list
⋮----
data = [1, 2, 3, 4, 5]
executor = ParallelExecutor(num_threads=3)
results = executor.execute(task, data)
⋮----
def test_parallel_execution_speed()
⋮----
time.sleep(0.1)  # Simulate a time-consuming task
⋮----
executor = ParallelExecutor(num_threads=5)
⋮----
start_time = time.time()
⋮----
end_time = time.time()
⋮----
def test_max_errors_handling()
⋮----
executor = ParallelExecutor(num_threads=3, max_errors=1)
⋮----
def test_max_errors_not_met()
⋮----
executor = ParallelExecutor(num_threads=3, max_errors=2)
⋮----
# Ensure that the execution completes without crashing when max_errors is not met
⋮----
# Verify that the results exclude the failed task
⋮----
def test_parallel_executor_tracks_failed_indices_and_exceptions()
⋮----
executor = ParallelExecutor(num_threads=3, max_errors=3)
</file>

<file path="tests/utils/test_saving.py">
def test_save_predict(tmp_path)
⋮----
predict = dspy.Predict("question->answer")
⋮----
loaded_predict = dspy.load(tmp_path, allow_pickle=True)
⋮----
def test_save_custom_model(tmp_path)
⋮----
class CustomModel(dspy.Module)
⋮----
def __init__(self)
⋮----
model = CustomModel()
⋮----
loaded_model = dspy.load(tmp_path, allow_pickle=True)
⋮----
def test_save_model_with_custom_signature(tmp_path)
⋮----
class MySignature(dspy.Signature)
⋮----
"""Just a custom signature."""
⋮----
current_date: datetime.date = dspy.InputField()
target_date: datetime.date = dspy.InputField()
date_diff: int = dspy.OutputField(desc="The difference in days between the current_date and the target_date")
⋮----
predict = dspy.Predict(MySignature)
⋮----
@pytest.mark.extra
def test_save_compiled_model(tmp_path)
⋮----
trainset = [
trainset = [dspy.Example(**example).with_inputs("question") for example in trainset]
⋮----
def dummy_metric(example, pred, trace=None)
⋮----
optimizer = dspy.BootstrapFewShot(max_bootstrapped_demos=4, max_labeled_demos=4, max_rounds=5, metric=dummy_metric)
compiled_predict = optimizer.compile(predict, trainset=trainset)
⋮----
def test_load_with_version_mismatch(tmp_path)
⋮----
# Mock versions during save
save_versions = {"python": "3.9", "dspy": "2.4.0", "cloudpickle": "2.0"}
⋮----
# Mock versions during load
load_versions = {"python": "3.10", "dspy": "2.5.0", "cloudpickle": "2.1"}
⋮----
# Create a custom handler to capture log messages
class ListHandler(logging.Handler)
⋮----
def emit(self, record)
⋮----
# Add handler and set level
handler = ListHandler()
original_level = logger.level
⋮----
# Mock version during save
⋮----
# Mock version during load
⋮----
# Assert warnings were logged, and one warning for each mismatched dependency.
⋮----
# Verify the model still loads correctly despite version mismatches
⋮----
# Clean up: restore original level and remove handler
⋮----
def test_pickle_loading_requires_explicit_permission(tmp_path)
⋮----
"""Test that loading pickle files requires explicit permission."""
⋮----
# Should fail without dangerously_allow_pickle
⋮----
# Should succeed with dangerously_allow_pickle
⋮----
def test_pkl_file_loading_requires_explicit_permission(tmp_path)
⋮----
"""Test that loading .pkl files requires explicit permission."""
⋮----
pkl_path = tmp_path / "model.pkl"
⋮----
# Should fail without allow_pickle
new_predict = dspy.Predict("question->answer")
⋮----
# Should succeed with allow_pickle
⋮----
def test_json_file_loading_works_without_permission(tmp_path)
⋮----
"""Test that loading .json files works without explicit permission."""
⋮----
json_path = tmp_path / "model.json"
⋮----
# Should succeed without allow_pickle
</file>

<file path="tests/utils/test_settings.py">
def test_basic_dspy_settings()
⋮----
def test_forbid_configure_call_in_child_thread()
⋮----
def worker()
⋮----
def test_dspy_context()
⋮----
def test_dspy_context_parallel()
⋮----
def worker(i)
⋮----
def test_dspy_context_with_dspy_parallel()
⋮----
class MyModule(dspy.Module)
⋮----
def __init__(self)
⋮----
def forward(self, question: str) -> str
⋮----
lm = dspy.LM("openai/gpt-4o-mini", cache=False) if "France" in question else dspy.settings.lm
⋮----
module = MyModule()
parallelizer = dspy.Parallel()
input_pairs = [
⋮----
# Verify mock was called correctly
⋮----
# France question uses gpt-4o-mini
⋮----
# Germany question uses gpt-4o
⋮----
# The main thread is not affected by the context
⋮----
@pytest.mark.asyncio
async def test_dspy_context_with_async_task_group()
⋮----
async def aforward(self, question: str) -> str
⋮----
lm = (
⋮----
result = await self.predict.acall(question=question)
⋮----
# Define the coroutines to be run
coroutines = [
⋮----
# Run them concurrently and gather results
results = await asyncio.gather(*coroutines)
⋮----
@pytest.mark.asyncio
async def test_dspy_configure_allowance_async()
⋮----
def bar1()
⋮----
# `dspy.configure` is disallowed in different async tasks from the initial one.
# In this case, foo1 (async) calls bar1 (sync), and bar1 uses the async task from foo1.
⋮----
async def foo1()
⋮----
async def foo2()
⋮----
async def foo3()
⋮----
# `dspy.context` is allowed in different async tasks from the initial one.
⋮----
async def foo4()
⋮----
# foo4 is directly invoked by the entry task, so it has the same async task as the entry task.
⋮----
# `dspy.configure` is allowed to be called multiple times in the same async task.
⋮----
def test_dspy_settings_save_load(tmp_path)
⋮----
loaded_settings = dspy.load_settings(tmp_path / "settings.pkl")
⋮----
def test_dspy_settings_save_exclude_keys(tmp_path)
⋮----
def test_settings_save_with_extra_modules(tmp_path)
⋮----
# Create a temporary Python file with our custom module
custom_module_path = tmp_path / "custom_module.py"
⋮----
# Add the tmp_path to Python path so we can import the module
⋮----
settings_path = tmp_path / "settings.pkl"
⋮----
# Remove the custom module again to simulate it not being available at load time
⋮----
# Loading should now succeed and preserve the adapter instance
loaded_settings = dspy.load_settings(settings_path)
⋮----
# Only need to clean up sys.path
</file>

<file path="tests/utils/test_syncify.py">
def test_syncify_in_place()
⋮----
class MyProgram(dspy.Module)
⋮----
async def aforward(self, x: int) -> int
⋮----
sync_program = dspy.syncify(MyProgram())
⋮----
def test_syncify_with_wrapper()
⋮----
sync_program = dspy.syncify(MyProgram(), in_place=False)
⋮----
def test_syncify_works_with_optimizers()
⋮----
def __init__(self)
⋮----
async def aforward(self, question: str)
⋮----
async_program = MyProgram()
⋮----
def dummy_metric(gold, pred, traces=None)
⋮----
# We only test the optimizer completes without errors, so the LM response doesn't matter.
lm = dspy.utils.DummyLM([{"answer": "dummy"} for _ in range(100)])
⋮----
dataset = [dspy.Example(question="question", answer="answer").with_inputs("question") for _ in range(10)]
⋮----
optimizer = dspy.BootstrapFewShot(metric=dummy_metric, max_bootstrapped_demos=2, max_labeled_demos=0)
⋮----
# Test syncify in place
sync_program = dspy.syncify(async_program, in_place=True)
optimized_program = optimizer.compile(sync_program, trainset=dataset)
⋮----
# Test syncify with wrapper
sync_program = dspy.syncify(async_program, in_place=False)
</file>

<file path="tests/utils/test_unbatchify.py">
def simple_batch_processor(batch)
⋮----
"""A simple batch function that adds 1 to each item."""
⋮----
def submit(self, input_item: any) -> Future
⋮----
"""Submits an item for processing and returns a Future."""
future = Future()
⋮----
def test_unbatchify_batch_size_trigger()
⋮----
"""Test that the batch processes exactly when max_batch_size is reached."""
batch_fn_mock = MagicMock(wraps=simple_batch_processor)
unbatcher = Unbatchify(batch_fn=batch_fn_mock, max_batch_size=2, max_wait_time=5.0)
⋮----
futures = []
⋮----
results_1_2 = [f.result() for f in futures]
⋮----
futures_3_4 = []
⋮----
results_3_4 = [f.result() for f in futures_3_4]
⋮----
def test_unbatchify_timeout_trigger()
⋮----
"""Test that the batch processes after max_wait_time."""
⋮----
wait_time = 0.15
unbatcher = Unbatchify(batch_fn=batch_fn_mock, max_batch_size=5, max_wait_time=wait_time)
⋮----
results = [f.result() for f in futures]
</file>

<file path="tests/utils/test_usage_tracker.py">
def test_add_usage_entry()
⋮----
"""Test adding usage entries to the tracker."""
tracker = UsageTracker()
⋮----
# Test with a single usage entry
usage_entry = {
⋮----
def test_get_total_tokens()
⋮----
"""Test calculating total tokens from usage entries."""
⋮----
# Add multiple usage entries for the same model
usage_entries = [
⋮----
total_usage = tracker.get_total_tokens()
⋮----
assert total_usage["gpt-4o-mini"]["prompt_tokens"] == 2417  # 1117 + 800 + 500
assert total_usage["gpt-4o-mini"]["completion_tokens"] == 226  # 46 + 100 + 80
assert total_usage["gpt-4o-mini"]["total_tokens"] == 2643  # 1163 + 900 + 580
assert total_usage["gpt-4o-mini"]["prompt_tokens_details"]["cached_tokens"] == 600  # 200 + 300 + 100
assert total_usage["gpt-4o-mini"]["prompt_tokens_details"]["audio_tokens"] == 75  # 50 + 0 + 25
assert total_usage["gpt-4o-mini"]["completion_tokens_details"]["reasoning_tokens"] == 100  # 20 + 50 + 30
assert total_usage["gpt-4o-mini"]["completion_tokens_details"]["audio_tokens"] == 25  # 10 + 0 + 15
assert total_usage["gpt-4o-mini"]["completion_tokens_details"]["accepted_prediction_tokens"] == 81  # 16 + 40 + 25
assert total_usage["gpt-4o-mini"]["completion_tokens_details"]["rejected_prediction_tokens"] == 20  # 0 + 10 + 10
⋮----
def test_track_usage_with_multiple_models()
⋮----
"""Test tracking usage across multiple models."""
⋮----
# Add usage entries for different models
⋮----
def test_track_usage_context_manager(lm_for_test)
⋮----
lm = dspy.LM(lm_for_test, cache=False, temperature=0.0)
⋮----
predict = dspy.ChainOfThought("question -> answer")
⋮----
def test_merge_usage_entries_with_new_keys()
⋮----
"""Ensure merging usage entries preserves unseen keys."""
⋮----
def test_merge_usage_entries_with_none_values()
⋮----
def test_merge_usage_entries_with_pydantic_models()
⋮----
"""Test merging usage entries with Pydantic model objects, like `PromptTokensDetailsWrapper` from litellm."""
⋮----
# Here we define a simplified version of the Pydantic models from litellm to avoid the dependency change on litellm.
class CacheCreationTokenDetails(BaseModel)
⋮----
ephemeral_5m_input_tokens: int
ephemeral_1h_input_tokens: int
⋮----
class PromptTokensDetailsWrapper(BaseModel)
⋮----
audio_tokens: int | None
cached_tokens: int
text_tokens: int | None
image_tokens: int | None
cache_creation_tokens: int
cache_creation_token_details: CacheCreationTokenDetails
⋮----
def test_parallel_executor_with_usage_tracker()
⋮----
"""Test that usage tracking works correctly with ParallelExecutor and mocked LM calls."""
⋮----
parent_tracker = UsageTracker()
⋮----
# Mock LM with different responses
mock_lm = mock.MagicMock(spec=dspy.LM)
⋮----
def task1()
⋮----
# Simulate LM usage tracking for task 1
⋮----
def task2()
⋮----
# Simulate LM usage tracking for task 2 with different values
⋮----
# Execute tasks in parallel
⋮----
executor = dspy.Parallel()
results = executor([(task1, {}), (task2, {})])
# Verify that the two workers had different usage
usage1 = results[0]
usage2 = results[1]
⋮----
# Task 1 should have 50 prompt tokens, task 2 should have 80
⋮----
# Parent tracker should remain unchanged (workers have independent copies)
</file>

<file path="tests/__init__.py">

</file>

<file path="tests/conftest.py">
from tests.test_utils.server import litellm_test_server, read_litellm_test_server_request_logs  # noqa: F401
⋮----
SKIP_DEFAULT_FLAGS = ["reliability", "extra", "llm_call"]
⋮----
@pytest.fixture(autouse=True)
def clear_settings()
⋮----
"""Ensures that the settings are cleared after each test."""
⋮----
@pytest.fixture
def anyio_backend()
⋮----
# Taken from: https://gist.github.com/justinmklam/b2aca28cb3a6896678e2e2927c6b6a38
def pytest_addoption(parser)
⋮----
def pytest_configure(config)
⋮----
def pytest_collection_modifyitems(config, items)
⋮----
skip_mark = pytest.mark.skip(reason=f"need --{flag} option to run")
⋮----
@pytest.fixture
def lm_for_test()
⋮----
model = os.environ.get("LM_FOR_TEST", None)
</file>

<file path="tests/mock_interpreter.py">
"""
Mock interpreter for testing RLM and other code-executing modules.

This interpreter doesn't actually execute code - it returns scripted responses
or uses a custom function to generate responses. Useful for:
- Unit testing without Deno/Pyodide dependencies
- Testing specific execution paths (errors, SUBMIT, etc.)
- Recording what code was submitted for execution
"""
⋮----
__all__ = ["MockInterpreter"]
⋮----
class MockInterpreter
⋮----
"""Mock interpreter that returns scripted responses.

    Implements the Interpreter protocol for testing purposes.

    Example usage:
        ```python
        # Script specific responses
        mock = MockInterpreter(responses=[
            "data explored",
            FinalOutput("42"),
        ])
        result1 = mock.execute("print(len(context))")  # Returns "data explored"
        result2 = mock.execute("SUBMIT('42')")  # Returns FinalOutput("42")

        # Use custom execution function
        def custom_exec(code, variables):
            if "SUBMIT" in code:
                return FinalOutput("done")
            return f"executed: {code[:20]}..."

        mock = MockInterpreter(execute_fn=custom_exec)
        ```
    """
⋮----
"""Initialize the mock interpreter.

        Args:
            responses: List of responses to return in sequence. Each call to
                      execute() pops the next response. If an Exception is
                      in the list, it will be raised.
            execute_fn: Custom function that receives (code, variables) and
                       returns the result. Takes precedence over responses.
            tools: Dictionary mapping tool names to callable functions.
                   MockInterpreter doesn't use tools, but stores them for protocol compliance.
        """
⋮----
def start(self) -> None
⋮----
"""Execute code and return the next scripted response.

        Args:
            code: The code that would be executed (recorded in call_history)
            variables: Variables that would be injected (recorded in call_history)

        Returns:
            The next response from the responses list, or result from execute_fn

        Raises:
            CodeInterpreterError: If the interpreter was shutdown, or if an Exception
                             is in the responses list
        """
⋮----
variables = variables or {}
⋮----
# Custom function takes precedence
⋮----
# Return scripted responses
⋮----
response = self.responses.pop(0)
⋮----
def shutdown(self) -> None
⋮----
def reset(self) -> None
⋮----
"""Reset the interpreter state for reuse in tests."""
⋮----
# Context manager support
def __enter__(self) -> "MockInterpreter"
⋮----
def __exit__(self, *args: Any) -> None
⋮----
"""Shorthand for execute()."""
</file>

<file path="tests/README.md">
The tests in this directory are primarily concerned with code correctness and Adapter reliability.

If you're looking for testing the end-to-end quality of DSPy modules and optimizer, refer to [LangProBe](https://github.com/Shangyint/langProBe).
</file>

<file path=".gitignore">
/experiments/
/checkpoints/
# /data/
/logs/
/mlruns/
/profiler/
/logs/
/docs/downloads/
/docs/experiments/

/examples/qa/hotpot/MIPRO_notebook_cache/
/examples/nli/scone/MIPRO_notebook_cache/
/examples/nli/scone/ScoNe/
/examples/nli/scone/compiled_program.dspy
/examples/qa/hotpot/compiled_program.dspy
/ScoNe/
testing/outputs/
testing/playbook.ipynb

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Vim
*.swp

# Jupyter Notebook
.ipynb_checkpoints
# notebooks/

# mac
.DS_Store

# Other
.vscode
*.tsv
*.pt
gpt*.txt
*.env
local/
build/
*.egg-info/
# *.jsonl
# *.json
!/data/*.json
/dist/
# **/*.pkl
checklist.md
finetuning_ckpts/
# cache/
* copy*
.idea
assertion.log
*.log
*.db
/.devcontainer/.personalization.sh

.mypy_cache
CLAUDE.local.md
dummy.csv
docs/docs/**/*.json*
*.index
*.pkl
*.tar.gz

test_before_pypi/
.github/.internal_dspyai/dist/
</file>

<file path=".pre-commit-config.yaml">
default_language_version:
  python: python3.10

default_stages: [pre-commit]
default_install_hook_types: [pre-commit]

repos:
  - repo: local
    hooks:
      - id: ruff-check
        name: ruff (lint)
        entry: ruff
        language: system
        types_or: [python, pyi]
        files: ^(dspy|tests)/.*\.py$
        exclude: ^(dspy/__metadata__\.py|tests/reliability/.*\.py)$
        args: [check, --fix-only]

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      - id: check-yaml
        args: ["--allow-multiple-documents", "--unsafe"]
      - id: check-toml
      - id: check-added-large-files
        args: ["--maxkb=1024"]
      - id: check-merge-conflict
      - id: debug-statements
</file>

<file path="CONTRIBUTING.md">
# Contribution Guide

DSPy is an actively growing project and community! We welcome your contributions and involvement. Below are instructions for how to contribute to DSPy.

## Finding an Issue

The fastest way to contribute is to find open issues that need an assignee. We maintain two lists of GitHub tags for contributors:

- [good first issue](https://github.com/stanfordnlp/dspy/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22):
  a list of small, well-defined issues for newcomers to the project.
- [help wanted](https://github.com/stanfordnlp/dspy/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22help%20wanted%22):
  a list of issues that welcome community contributions. These issues have a wide range of complexity.

We also welcome new ideas! If you would like to propose a new feature, please open a feature request to
discuss. If you already have a design in mind, please include a notebook/code example to demonstrate
your idea. Keep in mind that designing a new feature or use case may take longer than contributing to
an open issue.

## Contributing Code

Follow these steps to submit your code contribution.

### Step 1. Open an Issue

Before making any changes, we recommend opening an issue (if one doesn't already exist) and discussing your
proposed changes. This way, we can give you feedback and validate the proposed changes.

If your code change involves fixing a bug, please include a code snippet or notebook
to show how to reproduce the broken behavior.

For minor changes (simple bug fixes or documentation fixes), feel free to open a PR without discussion.

### Step 2. Make Code Changes

To make code changes, fork the repository and set up your local development environment following the
instructions in the "Environment Setup" section below.

### Step 3 Commit Your Code and Run Autoformatting

We follow the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html) and use `ruff` for both linting and formatting. To ensure consistent code quality, we use pre-commit hooks that automatically check and fix common issues.


First you need to set up the pre-commit hooks (do this once after cloning the repository):

```shell
pre-commit install
```

Then stage and commit your changes. When you run `git commit`, the pre-commit hook will be
automatically run.

```shell
git add .
git commit -m "your commit message"
```

If the hooks make any changes, you'll need to stage and commit those changes as well.

You can also run the hooks manually:

- Check staged files only:

  ```shell
  pre-commit run
  ```

- Check specific files:

  ```shell
  pre-commit run --files path/to/file1.py path/to/file2.py
  ```

Please ensure all pre-commit checks pass before creating your pull request. If you're unsure about any
formatting issues, feel free to commit your changes and let the pre-commit hooks fix them automatically.

### Step 4. Create a Pull Request

Once your changes are ready, open a pull request from your branch in your fork to the main branch in the
[DSPy repo](https://github.com/stanfordnlp/dspy).

### Step 5. Code Review

Once your PR is up and passes all CI tests, we will assign reviewers to review the code. There may be
several rounds of comments and code changes before the pull request gets approved by the reviewer.

### Step 6. Merging

Once the pull request is approved, a team member will take care of merging.

## Environment Setup

Python 3.10 or later is required.

Setting up your DSPy development environment requires you to fork the DSPy repository and clone it locally.
If you are not familiar with the GitHub fork process, please refer to [Fork a repository](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo). After creating the fork, clone
it to your local development device:

```shell
git clone {url-to-your-fork}
cd dspy
```

Next, we must set up a Python environment with the correct dependencies. There are two recommended ways to set up the
dev environment.

### [Recommended] Set Up Environment Using uv

[uv](https://github.com/astral-sh/uv) is a rust-based Python package and project manager that provides a fast
way to set up the development environment. First, install uv by following the
[installation guide](https://docs.astral.sh/uv/getting-started/installation/).

After uv is installed, in your working directory (`dspy/`), create a virtual environment using Python 3.10:

```shell
uv venv --python 3.10
```
This creates a `.venv` directory. Now, sync the environment with the development dependencies:

```shell
uv sync --extra dev
```

Then you are all set!

To verify that your environment is set up successfully, run some unit tests:

```shell
uv run pytest tests/predict
```

Note: You need to use the `uv run` prefix for every Python command, as uv creates a Python virtual
environment and `uv run` points the command to that environment. For example, to execute a Python script you will need
`uv run python script.py`.

### Set Up Environment Using conda + pip

You can also set up the virtual environment via conda + pip, which takes a few extra steps but offers more flexibility. Before starting,
make sure you have conda installed. If not, please follow the instructions
[here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).

To set up the environment, run:

```shell
conda create -n dspy-dev python=3.10
conda activate dspy-dev
pip install -e ".[dev]"
```

Then verify the installation by running some unit tests:

```shell
pytest tests/predict
```
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2023 Stanford Future Data Systems

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="pyproject.toml">
[build-system]
requires = ["setuptools>=77.0.1"]
build-backend = "setuptools.build_meta"

[project]
# Do not add spaces around the '=' sign for any of the fields
# preceded by a marker comment as it affects the publish workflow.
#replace_package_name_marker
name="dspy"
#replace_package_version_marker
version="3.1.3"
description = "DSPy"
readme = "README.md"
authors = [{ name = "Omar Khattab", email = "okhattab@stanford.edu" }]
license = {file = "LICENSE"}
requires-python = ">=3.10, <3.15"
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Science/Research",
    "Operating System :: POSIX :: Linux",
    "Programming Language :: Python :: 3"
]
dependencies = [
    "openai>=0.28.1",
    "regex>=2023.10.3",
    "orjson>=3.9.0",
    "tqdm>=4.66.1",
    "requests>=2.31.0",
    "optuna>=3.4.0",
    "pydantic>=2.0",
    "litellm>=1.64.0",
    "diskcache>=5.6.0",
    "json-repair>=0.54.2",
    "tenacity>=8.2.3",
    "anyio",
    "asyncer==0.0.8",
    "cachetools>=5.5.0",
    "cloudpickle>=3.0.0",
    "numpy>=1.26.0",
    "xxhash>=3.5.0",
    "gepa[dspy]==0.0.26",
]

[project.optional-dependencies]
anthropic = ["anthropic>=0.18.0,<1.0.0"]
weaviate = ["weaviate-client~=4.5.4"]
mcp = ["mcp; python_version >= '3.10'"]
langchain = ["langchain_core"]
dev = [
    "pytest>=6.2.5",
    "pytest-mock>=3.12.0",
    "pytest-asyncio>=0.26.0",
    "ruff>=0.3.0",
    "pre-commit>=3.7.0",
    "pillow>=10.1.0",
    "datamodel_code_generator>=0.26.3",
    "build>=1.0.3",
    "litellm>=1.64.0; sys_platform == 'win32' or python_version == '3.14'",
    "litellm[proxy]>=1.64.0; sys_platform != 'win32' and python_version < '3.14'",  # Remove 3.14 condition once uvloop supports
]
test_extras = [
    "mcp; python_version >= '3.10'",
    "datasets>=2.14.6",
    "pandas>=2.1.1",
    "optuna>=3.4.0",
    "langchain_core",
]

[tool.setuptools.packages.find]
where = ["."]
include = ["dspy", "dspy.*"]
exclude = ["tests", "tests.*"]

[tool.setuptools.package-data]
dspy = ["primitives/*.js"]

[project.urls]
homepage = "https://github.com/stanfordnlp/dspy"

[tool.coverage.run]
branch = true
omit = [
    "*/__init__.py",
    "*/test_*.py",
    "*/tests/*.py",
    "*/conftest.py",
    "*/venv/*",
    "*/virtualenv/*",
    "*/.venv/*",
    "*/.virtualenv/*",
    "*/env/*",
    "*/.env/*",
    "*/setup.py",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == '__main__':",
    "logger",
    "try",
    "except",
    "^\\s*self\\.\\w+(:\\s*[^=]+)?\\s*=.*$",
    "continue",
]

[tool.pytest.ini_options]
markers = [
    "integration: tests requiring external dependencies (Deno, LM)",
]
filterwarnings = [
    # litellm uses deprecated pydantic config classes sometimes.
    # The issue has been fixed repeatedly, but still keeps showing up.
    # For examples, see litellm PRs #6903, #7300, #8096, #9372, and #12528.
    "ignore:.+class-based `config` is deprecated, use ConfigDict:DeprecationWarning",
]

[tool.ruff]
include = ["dspy/**/*.py", "tests/**/*.py"]
exclude = [
  "dspy/__metadata__.py",
  "tests/reliability/*.py",
]


line-length = 120
indent-width = 4
target-version = "py310"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C",   # flake8-comprehensions
    "B",   # flake8-bugbear
    "UP",  # pyupgrade
    "N",   # pep8-naming
    "RUF", # ruff-specific rules
    "Q",   # flake8-quotes
]

ignore = [
    "B027",  # Allow non-abstract empty methods in abstract base classes
    "FBT003",# Allow boolean positional values in function calls
    "C901",  # Ignore complexity checking
    "E501",  # Ignore line length errors (handled by formatter)
    "UP035", # Allow python typing modules
    "RUF005", # Allow using + operator to concatenate collections
    "B904", # Allow raise custom exceptions in except blocks
    "F403", # Allow wildcard imports
    "E721", # Allow using == to compare with type
    "UP031", # Allow percent format
    "RUF022", # Allow unsorted __all__ value
]

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

[tool.ruff.format]
docstring-code-format = false
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.ruff.lint.isort]
known-first-party = ["dspy"]

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all"

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = [
    "S101",    # Allow assert statements in tests
    "TID252",  # Allow relative imports in tests
    "ARG001",  # Allow unused arguments in tests (like pytest fixtures)
]
"__init__.py" = ["F401"]  # Init files can be empty
"dspy/__init__.py" = [
    "I001",  # Allow unsorted or unformatted imports (isort)
    "E402",  # Allow imports not at the top of the file (needed for certain __init__ patterns)
    "F405",  # Allow undefined names from wildcard imports (common in __init__ files)
]
</file>

<file path="README.md">
<p align="center">
  <img align="center" src="docs/docs/static/img/dspy_logo.png" width="460px" />
</p>
<p align="left">


## DSPy: _Programming_—not prompting—Foundation Models

**Documentation:** [DSPy Docs](https://dspy.ai/)

[![PyPI Downloads](https://static.pepy.tech/personalized-badge/dspy?period=monthly)](https://pepy.tech/projects/dspy)


----

DSPy is the framework for _programming—rather than prompting—language models_. It allows you to iterate fast on **building modular AI systems** and offers algorithms for **optimizing their prompts and weights**, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.

DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional _Python code_ and use DSPy to **teach your LM to deliver high-quality outputs**. Learn more via our [official documentation site](https://dspy.ai/) or meet the community, seek help, or start contributing via this GitHub repo and our [Discord server](https://discord.gg/XCGy2WDCQB).


## Documentation: [dspy.ai](https://dspy.ai)


**Please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**


## Installation


```bash
pip install dspy
```

To install the very latest from `main`:

```bash
pip install git+https://github.com/stanfordnlp/dspy.git
````




## 📜 Citation & Reading More

If you're looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai).

If you're looking to understand the underlying research, this is a set of our papers:

**[Jul'25] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)**       
**[Jun'24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct'23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul'24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun'24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb'24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan'24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec'23] [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)   
[Dec'22] [Demonstrate-Search-Predict: Composing Retrieval & Language Models for Knowledge-Intensive NLP](https://arxiv.org/abs/2212.14024.pdf)

To stay up to date or learn more, follow [@DSPyOSS](https://twitter.com/DSPyOSS) on Twitter or the DSPy page on LinkedIn.

The **DSPy** logo is designed by **Chuyi Zhang**.

If you use DSPy or DSP in a research paper, please cite our work as follows:

```
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
```

<!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) -->
</file>

</files>
